[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Laboratorio Análisis de Datos",
    "section": "",
    "text": "Prefacio\nLaboratorio de software y problema de probabilidad y estadística\nOtro manual de probabilidad y estadística.\n\n\n\n\n\n\n\n\n\nEstá escrito en markdown utilizando ideas de (Knuth 1984).\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "p01_basicos.html",
    "href": "p01_basicos.html",
    "title": "Antes de empezar",
    "section": "",
    "text": "Para hacer tratamientos de datos en general necesitamos conocimiento básicos de matemáticas. Concretamente\n\nAritmética básica\nTeoría de conjuntos y cardinalidad. Combinatoria\nCálculo básico funciones, derivadas intergrales series\nGeometría (euclídea) básica \\(\\mathbb{R}^n\\)\nÁlgebra matricial, diagonalización.\nProbabilidad y estadística básica\nEstadística descriptiva de una y variables variables.\n\nUna vez definidos estos conceptos podemos avanzar a variables aleatroias para en el tratamiento de datos.",
    "crumbs": [
      "Antes de empezar"
    ]
  },
  {
    "objectID": "p01_01_combinatoria.html",
    "href": "p01_01_combinatoria.html",
    "title": "1  Algunos conceptos básicos.",
    "section": "",
    "text": "1.1 Teoría de conjuntos\nPor experiencia sabemos que la mayoría de estudiantes tienen más conocimientos de cálculo, geometría y matrices.\nPero muchos tienen una falta de conocimientos en teoría básica de conjuntos y combinatoria (matemática discreta).\nLa teoría de conjuntos básicas es simple y natural y es la que necesitamos para este curso.\nLa teoría de conjuntos matemática es más compleja y presenta varias paradojas como la paradoja de Russell.\nLa idea o noción práctica de conjunto es la de una colección de objetos de un cierto tipo.\nEstas colecciones o conjuntos se pueden definir por:",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Algunos conceptos básicos.</span>"
    ]
  },
  {
    "objectID": "p01_01_combinatoria.html#teoría-de-conjuntos",
    "href": "p01_01_combinatoria.html#teoría-de-conjuntos",
    "title": "1  Algunos conceptos básicos.",
    "section": "",
    "text": "Definición de conjunto\n\n\nLa definición de conjunto es una idea o noción primitiva. Es decir es una concepto básico del pensamiento humano: un conjunto es una colección de objetos: números, imágenes… cualquier cosa, jugadores de fútbol, palabras, colores….\n\n\n\n\n\n\n\nCompresión: reuniendo los objetos que cumplen una propiedad \\(p\\)\nExtensión: dando una lista exhaustiva de los miembros del conjunto",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Algunos conceptos básicos.</span>"
    ]
  },
  {
    "objectID": "p01_01_combinatoria.html#conjuntos-básicos",
    "href": "p01_01_combinatoria.html#conjuntos-básicos",
    "title": "1  Algunos conceptos básicos.",
    "section": "1.2 Conjuntos básicos",
    "text": "1.2 Conjuntos básicos\nLos conjuntos suelen tener un conjunto madre como por ejemplo\n\n\\(\\mathbb{N}=\\{0,1,2,\\ldots\\}\\)\n\\(\\mathbb{Z}=\\{\\ldots,-2,-1,0,1,2,\\ldots\\}\\)\n\\(\\mathbb{Q}=\\left\\{\\frac{p}{q}\\quad\\Big|\\quad p,q\\in \\mathbb{Z} \\mbox{ y } q \\not= 0.\\right\\}\\)\n\\(\\mathbb{R}=\\{\\mbox{Todos los puntos de una recta.}\\}\\)\n\\(\\mathbb{C}= \\left\\{a+b\\cdot i\\quad \\big|\\quad a,b\\in \\mathbb{R}\\right\\}\\mbox{ los números complejos}\\quad a+b\\cdot i.\\)\nAlfabeto = \\(\\{a,b,c,\\ldots, A,B,C,\\ldots\\}.\\)\nPalabras = \\(\\{paz, guerra, amor, probabilidad,\\ldots\\}.\\)\n\nRecordemos que \\(i\\) es la unidad imaginaria que cumple que \\(i=\\sqrt{-1}\\).\n\n1.2.1 Características y propiedades básicas de los conjuntos\n\nSi a cada objeto \\(x\\) de \\(\\Omega\\) le llamaremos elemento del conjunto \\(\\Omega\\) y diremos que \\(x\\) pertenece a \\(\\Omega\\). Lo denotaremos por \\(x\\in \\Omega\\).\nAl número de elementos de un conjunto se le llama cardinal. Se denota por \\(\\#(\\Omega)\\) o por \\(|\\Omega|.\\)\nUn conjunto de un elemento, por ejemplo \\(\\{1\\}\\) recibe el nombre de conjunto elemental.\nSea \\(A\\) otro conjunto diremos que \\(A\\) es igual a \\(B\\) si todos los elementos \\(A\\) están en \\(B\\) y todos los elementos de \\(B\\) están en \\(A\\). Por ejemplo \\(A=\\{1,2,3\\}\\) es igual a \\(B=\\{3,1,2\\}\\). Se denota por \\(A=B\\).\nSi \\(B\\) es otro conjunto, tal que si \\(x\\in A\\) entonces \\(x\\in B\\) diremos que \\(A\\) es un subconjunto de o que está contenido en \\(B\\). Lo denotaremos por \\(A\\subseteq B.\\)\nEl conjunto que no tiene elementos se denomina **conjunto vacío* y se denota por el símbolo \\(\\emptyset\\).\n\n\n\nEjemplo\n\n\nDado \\(A\\) un conjunto cualquiera obviamente \\(\\emptyset\\subseteq A.\\) Tomemos como conjunto base \\(\\Omega=\\{1,2,3\\},\\) Es evidente que \\(\\Omega\\) es un conjunto de cardinal 3, se denota por \\(\\#(\\Omega)=3\\) o por \\(|\\Omega|=3.\\) El conjunto \\(\\Omega\\) tiene \\(2^3=8\\) subconjuntos:\n\nel vacio \\(\\emptyset\\) y los elementales \\(\\{1\\},\\{2\\},\\{3\\},\\)\nlos subconjuntos de dos elementos: \\(\\{1,2\\},\\{1,3\\},\\{2,3\\},\\)\nel conjunto total de tres elementos \\(\\Omega=\\{1,2,3\\}.\\)\n\n\n\nDado un conjunto \\(\\Omega\\) podemos construir el conjunto de todas sus partes (todos sus subconjuntos) al que denotamos por \\(\\mathcal{P}(\\Omega)\\). También se denomina de forma directa partes de \\(\\Omega\\).\n\n\nCardinal de las partes de un conjunto\n\n\nCardinal de las partes de un conjunto\nEl cardinal de la partes de un conjunto es \\(\\#(\\mathcal{P}(\\Omega))=2^{\\#(\\Omega)}.\\)\n\n\n\n\nEjemplo\n\n\nPor ejemplo \\(\\#\\left(\\mathcal{P}(\\{1,2,3\\})\\right)=2^{\\#(\\{1,2,3\\})}=2^3=8.\\)\nEfectivamente\n\\[\\mathcal{P}(\\{1,2,3\\})=\\{\\emptyset,\\{1\\},\\{2\\},\\{3\\},\\{1,2\\},\\{1,3\\},\\{2,3\\},\\{1,2,3\\}\\}.\\]\n\n\n\n\nFunción caracteristica de un conjunto\n\n\nDado un subconjunto \\(A\\) de \\(\\Omega\\) podemos se define la función característica de \\(A\\) \\[\\chi_A:\\Omega \\to \\{0,1\\}\\]\ndado un \\(\\omega\\in \\Omega\\)\n\\[\n\\chi_A(\\omega)=\n\\left\\{\n\\begin{array}{ll}\n1 &  \\mbox{si }\\omega \\in A\\\\\n0 &  \\mbox{si }\\omega \\not\\in A\n\\end{array}\n\\right.\n\\]\n\n\n\n\nOperaciones ente conjuntos: intersección, unión y diferencia de conjuntos\n\n\nSea \\(\\Omega\\) un conjunto y \\(A\\) y \\(B\\) dos subconjuntos de \\(\\Omega\\).\nEl conjunto intersección de \\(A\\) y \\(B\\) es el formado por todos los elementos que perteneces a \\(A\\) Y \\(B\\), se denota por \\(A\\cap B\\).\nFormalmente:\n\\[\nA\\cap B=\\left\\{x\\in\\Omega \\big| x\\in A \\mbox{ y } x\\in B\\right\\}.\n\\]\nEl conjunto unión de \\(A\\) y \\(B\\) es el formado por todos los elementos que perteneces a \\(A\\) O pertenecen a \\(B\\), se denota por \\(A\\cup B\\).\nMás formalmente:\n\\[\nA\\cup B=\\left\\{x\\in\\Omega \\big| x\\in A \\mbox{ o } x\\in B\\right\\}.\n\\]\nEl conjunto diferencia de \\(A\\) y \\(B\\) es el formado por todos los elementos que perteneces a \\(A\\) Y NO pertenecen a \\(B\\), se denota por \\(A-B=A-(A\\cap B)\\).\nDe otra forma\n\\[\nA-B=\\left\\{x\\in\\Omega \\big| x\\in A \\mbox{ y } x\\notin B\\right\\}.\n\\] El complementario de un subconjunto \\(A\\) de \\(\\Omega\\) es \\(\\Omega-A\\) y se denota por \\(A^c\\) o \\(\\overline{A}\\).\nMás formalmente\n\\[\nA^c=\\left\\{x\\in\\Omega \\big| x\\not\\in A\\right\\}.\n\\]\n\n\nMás notaciones y propiedades de conjuntos.\n\n\nPropiedades de las operaciones de conjuntos\n\n\nSea \\(\\Omega\\) un conjunto y \\(A\\), \\(B\\), \\(C\\) tres subconjuntos de \\(\\Omega\\)\n\nSe dice que dos conjuntos \\(A\\) y \\(B\\) son disjuntos si \\(A\\cap B=\\emptyset.\\)\n\\(\\Omega^c=\\emptyset\\).\n\\(\\emptyset^c=\\Omega\\).\n\\(A\\cup B=B \\cup A\\) , \\(A\\cap B=B\\cap A\\) conmutativas.\n\\((A\\cup B) \\cup C = A \\cup( B \\cup C)\\) , \\((A\\cap B) \\cap C = A \\cap( B \\cap C)\\) asociativas.\n\\(A\\cup (B\\cap C)=(A\\cup B) \\cap (A\\cup C)\\) , \\(A\\cap (B\\cup C)=(A\\cap B) \\cup (A\\cap C)\\) distributivas.\n\\(\\left(A^c\\right)^c=A\\) doble complementario.\n\\(\\left(A\\cup B\\right)^c=A^c \\cap B^c\\), \\(\\left(A\\cap B\\right)^c=A^c \\cup B^c\\) leyes de De Morgan.\n\n\n\n\n\nEjemplos con código R\n\n\nCon R los conjuntos de pueden definir como vectores\n\nOmega=c(1,2,3,4,5,6,7,8,9,10)\nA=c(1,2,3,4,5)\nB=c(1,4,5)\nC=c(4,6,7,8)\nOmega\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nA\n\n[1] 1 2 3 4 5\n\nB\n\n[1] 1 4 5\n\nC\n\n[1] 4 6 7 8\n\n\n\\(A\\cap B\\)\n\nA\n\n[1] 1 2 3 4 5\n\nB\n\n[1] 1 4 5\n\nintersect(A,B)\n\n[1] 1 4 5\n\n\n\\(A\\cup B\\)\n\nA\n\n[1] 1 2 3 4 5\n\nB\n\n[1] 1 4 5\n\nunion(A,B)\n\n[1] 1 2 3 4 5\n\n\n\\(B-C\\)\n\nB\n\n[1] 1 4 5\n\nC\n\n[1] 4 6 7 8\n\nsetdiff(B,C)\n\n[1] 1 5\n\n\n\\(A^c=\\Omega-A\\)\n\nOmega\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nA\n\n[1] 1 2 3 4 5\n\nsetdiff(Omega,A)\n\n[1]  6  7  8  9 10\n\n\n\n\n\n\nEjemplos con código python\n\n\nCon python\n\nOmega=set([1,2,3,4,5,6,7,8,9,10])\nA=set([1,2,3,4,5])\nB=set([1,4,5])\nC=set([4,6,7,8])\nOmega\n\n{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n\n\nA\n\n{1, 2, 3, 4, 5}\n\nB\n\n{1, 4, 5}\n\nC\n\n{8, 4, 6, 7}\n\n\n\nA & B   # intersección (&: and/y)\n\n{1, 4, 5}\n\nA | B   # unión (|: or/o)\n\n{1, 2, 3, 4, 5}\n\n\n\nA - C   # diferencia \n\n{1, 2, 3, 5}\n\nOmega-C # complementario.\n\n{1, 2, 3, 5, 9, 10}",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Algunos conceptos básicos.</span>"
    ]
  },
  {
    "objectID": "p01_01_combinatoria.html#combinatoria",
    "href": "p01_01_combinatoria.html#combinatoria",
    "title": "1  Algunos conceptos básicos.",
    "section": "1.3 Combinatoria",
    "text": "1.3 Combinatoria\nLa combinatoria es una rama de la matemática discreta que entre otras cosas cuenta distintas configuraciones de objetos de un conjunto.\nPor ejemplo si tenemos un equipo de baloncesto con 7 jugadores ¿cuántos equipos de 5 jugadores distintos podemos formar?\n\n1.3.1 Los números binomiales.\n\n\nNúmero combinatorio o número binomial\n\n\nNos da el número de subconjuntos de tamaño \\(k\\) de un conjunto de tamaño \\(n\\). Este número es\n\\[\nC_n^k={n\\choose k} = \\frac{n!}{k!\\cdot (n-k)!}.\n\\]\nRecordemos que \\[\nn!=1\\cdot 2\\cdot 3\\cdots n.\n\\]\nEn nuestro caso con 7 jugadores \\(n=7\\) el número de equipos distintos de \\(k=5\\) es\n\\[\n\\begin{array}{rl}\nC_7^5&={7\\choose 5} = \\frac{7!}{5!\\cdot (7-5)!}=\\frac{7!}{5!\\cdot 2!} \\\\\n&=\\frac{1\\cdot 2\\cdot 3 \\cdot 4\\cdot 5\\cdot 6\\cdot 7}{1\\cdot 2\\cdot 3 \\cdot 4\\cdot 5\\cdot 1\\cdot 2}=\\frac{6\\cdot 7}{2}=\\frac{42}{2}=21.\n\\end{array}\n\\]\nPuedo formar 21 equipos distintos.\n\n\n\n\nEjercicio\n\n\nCarga el paquete gtools de R y investiga la función combinations(n, r, v, set, repeats.allowed) para calcular todas las combinaciones anteriores.\n\n\n\n\n1.3.2 Combinaciones con repetición\n\n\nCombinaciones con repetición\n\n\nEn combinatoria, las combinaciones con repetición de un conjunto son las distintas formas en que se puede hacer una selección de elementos de un conjunto dado, permitiendo que las selecciones puedan repetirse.\nEl número \\(CR_n^k\\) de multiconjuntos con \\(k\\) elementos escogidos de un conjunto con \\(n\\) elementos satisface:\n\nEs igual al número de combinaciones con repetición de \\(k\\) elementos escogidos de un conjunto con \\(n\\) elementos.\nEs igual al número de formas de repartir \\(k\\) objetos en \\(n\\) grupos.\n\n\\[CR_n^k = \\binom{n+k-1}{k} = \\frac{(n+k-1)!}{k!(n-1)!}.\\]\n\n\nEl siguiente ejemplo es el típico problema de repartir caramelos entre varias\npersonas.\n\n\nEjemplo\n\n\nVamos a imaginar que vamos a repartir 12 caramelos entre Antonio, Beatriz, Carlos y Dionisio (que representaremos como A, B, C, D). Una posible forma de repartir los caramelos sería: dar 4 caramelos a Antonio, 3 a Beatriz, 2 a Carlos y 3 a Dionisio. Dado que no importa el orden en que se reparten, podemos representar esta selección como AAAABBBCCDDD.\nOtra forma posible de repartir los caramelos podría ser: dar 1 caramelo a Antonio, ninguno a Beatriz y Carlos, los 11 restantes se los damos a Dionisio. Esta repartición la representamos como ADDDDDDDDDDD\nRecíprocamente, cualquier serie de 12 letras A, B, C, D se corresponde a una forma de repartir los caramelos. Por ejemplo, la serie AAAABBBBBDDD corresponde a: Dar 4 caramelos a Antonio, 5 caramelos a Beatriz, ninguno a Carlos y 3 a Dionisio.\nDe esta forma, el número de formas de repartir los caramelos es:\n\\[CR_{4}^{12} = \\binom{4+12-1}{4}\\]\n\n\n\n\nEjemplo: Variaciones\n\n\nCon los número \\(\\{1,2,3\\}\\) ¿cuántos números de dos cifras distintas podemos formar sin repetir ninguna cifra?\nLa podemos escribir\n\\[12,13,21,23,31,32\\]\nLuego hay seis casos\n\n\n\n\nVariaciones con Repetición\n\n\nDenotaremos las variaciones (sin repetición) de \\(k\\) elementos (de orden \\(k\\)) de un conjunto de \\(n\\) elementos por \\(V_n^k\\) su valor es\n\\[\nV_n^k=\\frac{n!}{(n-k)!}=(n-k+1)\\cdot (n-k+2)\\cdots n.\n\\]\n\n\nEn nuestro ejemplo con \\(n=3\\) dígitos podemos escribir las siguientes variaciones de orden \\(k=2\\)\n\\[\nV^{k=2}_{n=3}=\\frac{3!}{(3-2)!}=\\frac{1\\cdot 2\\cdot 3}{1}=6.\n\\]\n\n\nEjercicio\n\n\nCarga el paquete gtools de R y investiga la función permutations(n, r, v, set, repeats.allowed) para calcular todas las variaciones anteriores ¿Y repitiendo algún dígito?\n\n\n\n\nVariaciones con repetición\n\n\n\\[VR_n^k=n^k\\]\n\n\n\n\nEjemplo\n\n\nEfectivamente en nuestro caso\n\\[11,12,13,21,22,23,31,32,33\\]\n\\[\nVR^{k=2}_{n=3}=n^k=3^2=9.\n\\]\n\n\n\n\nPermutaciones\n\n\nLas permutaciones de un conjunto de cardinal \\(n\\) son todas las variaciones de orden máximo \\(n\\). Las denotamos y valen:\n\\[\nP_n=V_n^n=n!\n\\]\n\n\n\n\nEjemplo\n\n\nPor ejemplo todos los números que se pueden escribir ordenando todos los dígitos \\(\\{1,2,3\\}\\) sin repetir ninguno\n\nlibrary(combinat)\nfor(permutacion in permn(3)) print(permutacion)\n\n[1] 1 2 3\n[1] 1 3 2\n[1] 3 1 2\n[1] 3 2 1\n[1] 2 3 1\n[1] 2 1 3\n\n\nEfectivamente \\[\nP_3=3!=1\\cdot  2\\cdot 3.\n\\]\n\n\n\n\nEjercicio\n\n\nCarga el paquete combinat de R e investiga la funcion permn para calcular todas las permutaciones anteriores.\n\n\n\n\nEjercicio\n\n\nInvestiga el paquete itertools y la función comb de scipy.misc de Python e investiga sus funciones para todas las formas de contar que hemos visto en este tema.\n\n\n\n\nEjercicio\n\n\nLa función gamma de Euler, cobrará mucha importancia en el curso de estadística. Comprueba que la función gamma(x+1) da el mismo valor que la función factorial(x) en R para todo \\(x = \\{1,2,3\\cdots,10\\}\\).\n\n\n\n\n1.3.3 Números multinomiales. Permutaciones con repetición.\n\n\nNúmeros multinomiales\n\n\nConsideremos un conjunto de elementos \\(\\{a_1, a_2, \\ldots, a_k\\}\\).\nEntoces, si cada uno de los objetos \\(a_i\\) de un conjunto, aparece repetido \\(n_i\\) veces para cada \\(i\\) desde 1 hasta \\(k\\), entonces el número de permutaciones con elementos repetidos es:\n\\[PR_n^{n_1,n_2,\\ldots,n_k} = {{n}\\choose {n_1\\quad n_2 \\quad\\ldots \\quad n_k}}=\\frac{n!}{n_1!\\cdot n_2!\\cdot \\ldots \\cdot n_k!},\\] donde \\(n=n_1+n_2+\\cdots+n_k\\).\n\n\n\n\nEjemplo\n\n\n¿Cuantas palabras diferentes se pueden formar con las letras de la palabra PROBABILIDAD?\nEl conjunto de letras de la palabra considerada es el siguiente: \\(\\{A, B, D, I, L, O, P, R\\}\\) con las repeticiones siguientes: las letras A, B, D, e I, aparecen 2 veces cada una; y las letras L, O, P, R una vez cada una de ellas.\nPor tanto, utilizando la fórmula anterior, tenemos que el número de palabras (permutaciones con elementos repetidos) que podemos formar es\n\\[PR^{2,2,2,2,1,1,1,1}_{12} = \\frac{12!}{(2!)^4(1!)^4} = 29937600.\\]",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Algunos conceptos básicos.</span>"
    ]
  },
  {
    "objectID": "p01_01_combinatoria.html#para-acabar",
    "href": "p01_01_combinatoria.html#para-acabar",
    "title": "1  Algunos conceptos básicos.",
    "section": "1.4 Para acabar",
    "text": "1.4 Para acabar\nPrincipios básicos para contar cardinales de conjuntos\n\n\nPropiedades\n\n\nEl principio de la suma\nSean \\(A_1, A_2,\\ldots, A_n\\) conjuntos disjuntos dos a dos, es decir \\(A_i\\cap A_j=\\emptyset\\) para todo \\(i\\not= j\\), \\(i,j=1,2,\\ldots n\\). Entonces\n\\[\\#(\\cup_{i=1}^n A_i)=\\sum_{i=1}^n \\#(A_i).\\]\nPrincipio de unión exclusión\nConsideremos dos conjuntos cualesquiera \\(A_1, A_2\\) entonces el cardinal de su unión es\n\\[\\#(A_1\\cup A_2)=\\#(A_1)+\\#(A_2)-\\#(A_1\\cap A_2).\\]\nEl principio del producto\nSean \\(A_1,A_2,\\ldots A_n\\)\n\\[\n\\begin{array}{ll}\n\\#(A_1\\times A_2\\times \\cdots A_n)=&\\#\\left(\\{(a_1,a_2,\\ldots a_n)| a_i\\in A_i, i=1,2,\\ldots n\\}\\right)\\\\\n&=\\prod_{i=1}^n \\#(A_i).\n\\end{array}\n\\]\n\n\n\n1.4.1 Otros aspectos a tener en cuenta\nEvidentemente nos hemos dejado muchas otras propiedades básicas de teoría de conjuntos y de combinatoria como:\n\nPropiedades de los números combinatorios.\nBinomio de Newton.\nMultinomio de Newton.\n\nSi nos son necesarias las volveremos a repetir a lo largo del curso o bien daremos enlaces para que las podáis hestudiar en paralelo.",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Algunos conceptos básicos.</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html",
    "href": "p01_02_probabilidad.html",
    "title": "2  Cálculo básico de probabilidades",
    "section": "",
    "text": "2.1 Definiciones y propiedades básicas\nComenzaremos dando unas definiciones básicas de probabilidad. Intentaremos ser rigurosos pero primará la aplicabilidad y la intuición sobre la formalidad.",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html#definiciones-y-propiedades-básicas",
    "href": "p01_02_probabilidad.html#definiciones-y-propiedades-básicas",
    "title": "2  Cálculo básico de probabilidades",
    "section": "",
    "text": "Espacio muestral\n\n\nEs el conjunto \\(\\Omega\\) formado por todos los sucesos elementales del experimento aleatorio\n\n\n\n\nEjemplo\n\n\nEl espacio muestral del ejemplo anterior del dado es \\(\\Omega=\\) las figuras de la caras anteriores, pero por comodidad pondremos\n\\[\\Omega = \\{1,2,3,4,5,6\\}\\]\n\n\n\n\nAlgunos sucesos notables\n\n\nAlgunos sucesos notables que merece la pena nombrar son:\n\nSuceso seguro o cierto: \\(\\Omega\\)\nSuceso imposible o vació: \\(\\emptyset\\)\nPartes de un conjunto: \\(\\mathcal{P}(\\Omega)\\): conjunto de todos los sucesos del experimento aleatorio (es decir, el conjunto de todos los subconjuntos de \\(\\Omega\\))\n\n\n\n\n\nEjercicio\n\n\n¿Cuantos elementos contiene el conjunto de partes de \\(\\Omega\\) del experimento anterior?\n\n\n\n\nEjemplo: Los \\(n\\)-gramas de una palabra\n\n\nSe denomina un \\(n\\)-grama de una palabra como el conjunto de \\(n\\) letras consecutivas de la misma (contando los blancos de inicio y final de palabra que marcamos como “_”)\nConsideremos el experimento aleatorio que consiste en escoger al azar un 3-grama de la palabra “_Baleares_”. Vamos a escribir el espacio muestral y algunos sucesos elementales del mismo.\nEn este caso, si consideramos la palabra “_Baleares_”, el espacio muestral del experimento sería:\n\\[\\Omega=\\{\\_Ba, Bal, ale, lea, ear, are, res, es\\_\\}\\]\nAlgunos sucesos serían:\n\n3-gramas que empiezan por \\(a\\): \\(\\{ale,are\\}.\\)\n3-gramas de inicio y final de palabra: \\(\\{\\_Ba,es\\_\\}.\\)\n3-gramas que contengan una \\(l\\): \\(\\{Bal,ale,lea\\}.\\)\n\n\n\n\n2.1.1 Operaciones con sucesos\nSi tenemos dos sucesos \\(A,B\\subseteq \\Omega\\), podemos definir:\n\n\\(\\Omega\\): suceso total o seguro.\n\\(\\emptyset\\): suceso vacío o imposible.\n\\(A\\cup B\\): suceso unión; el que ocurre si sucede \\(A\\) o \\(B\\).\n\\(A\\cap B\\): suceso intersección; el que ocurre si sucede \\(A\\) y \\(B\\).\n\\(A^c\\): suceso complementario el que sucede si NO sucede \\(A\\).\n\\(A- B=A\\cap B^c\\): suceso diferencia, que acontece si sucede \\(A\\) y NO sucede \\(B\\).\n\n\n\nSucesos incompatibles\n\n\nDiremos que dos sucesos \\(A\\) y \\(B\\) son incompatibles (o disjuntos) cuando \\(A\\cap B=\\emptyset\\).\n\n\n\n\nEjemplo\n\n\nSupongamos que el sexo se divide entre Mujeres y Hombres. Vamos a definir el espacio muestral, los sucesos elementales y a realizar algunas operaciones entre ellos.\n\nEstudiantes de esta clase: \\(\\Omega\\).\nMujeres de esta clase: \\(A\\).\nEstudiantes que son zurdos \\(B\\).\n\nAlgunas operaciones entre los conjuntos:\n\n\\(A\\cup B\\): Estudiantes que son mujeres o que son zurdos.\n\\(A\\cap B\\): Mujeres de esta clase que son zurdas.\n\\(A^c\\): Hombres de esta clase.\n\\(A-B\\): Mujeres de la clases que NO son zurdas.\n\\(B-A\\): Hombres de la clase que son zurdos.\nEstos sucesos no son incompatibles.\n\n\n\n\n\n2.1.2 Propiedades de las operaciones con sucesos\nAlgunas propiedades de las operaciones con sucesos.\nConmutativas :\n\\[A\\cup B=B\\cup A, \\quad A\\cap B=B\\cap A\\]\nAsociativas :\n\\[A\\cup(B\\cup C)=(A\\cup B)\\cup C, \\quad A\\cap(B\\cap C)=(A\\cap B)\\cap C\\]\nDistributivas :\n\\[A\\cap(B\\cup C)=(A\\cap B)\\cup (A\\cap C), \\quad A\\cup(B\\cap C)=(A\\cup B)\\cap (A\\cup C)\\]\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(B\\cap C\\)\n\\(A\\cup (B\\cap C)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\cup B\\)\n\\(A\\cup C\\)\n\\((A\\cup B)\\cap (A\\cup C)\\)\n\n\n\n\n\n\n\n\n\n\n[Complementario del complementario \\[(A^c)^c=A\\]\n\n\n\n\n\n\n\n\n\\(A\\)\n\\(A^c\\)\n\\((A^c)^c\\)\n\n\n\n\n\n\n\n\n\n\nLeyes de De Morgan\n\\[(A\\cup B)^c=A^c\\cap B^c.\\]\n\n\n\n\n\n\n\n\\(A\\cup B\\)\n\\((A\\cup B)^c\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A^c\\)\n\\(B^c\\)\n\\(A^c\\cap B^c\\)\n\n\n\n\n\n\n\n\n\n\nEl complementario de la intersección es la unión de los complementarios\n\\[(A\\cap B)^c=A^c\\cup B^c.\\]\n\n\n\n\n\n\n\n\\(A\\cap B\\)\n\\((A\\cap B)^c\\)\n\n\n\n\n\n\n\n\n\nLeyes de De Morgan\n\\[(A\\cap B)^c=A^c\\cup B^c\\]\n\n\n\n\n\n\n\n\n\\(A^c\\)\n\\(B^c\\)\n\\(A^c\\cup B^c\\)\n\n\n\n\n {height=“150px”}",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html#definición-de-probabilidad",
    "href": "p01_02_probabilidad.html#definición-de-probabilidad",
    "title": "2  Cálculo básico de probabilidades",
    "section": "2.2 Definición de probabilidad",
    "text": "2.2 Definición de probabilidad\nLa probabilidad de un suceso es una puntuación (score) numérico entre 0 y 1 que mide la verosimilitud de que este evento se produzca.\nEsta verosimilitud puede estar justificada por:\n\nEstimación personal\nEstimación de expertos\nLa frecuencia con la que se da\nCálculo formal\n\n\n\nDefinición formal de probabilidad\n\n\nSea \\(\\Omega\\) el espacio muestral de un experimento aleatorio. Supongamos que el número de posibles resultados, por el momento, es finito.\nUna probabilidad sobre \\(\\Omega\\) es una aplicación \\(P:\\mathcal{P}(\\Omega)\\to [0,1]\\) con las siguientes propiedades:\n\n\\(0\\leq P(A)\\leq 1\\), para todo suceso \\(A\\).\n\\(P(\\Omega)=1\\).\nSi \\(\\{A_1,A_2,\\ldots,A_n\\}\\) son sucesos disjuntos dos a dos, entonces\n\n\\[\nP(A_1\\cup A_2\\cup \\cdots \\cup A_n)=P(A_1)+P(A_2)+\\cdots +P(A_n)\n\\]\nSi \\(a\\in \\Omega\\) es un suceso elemental cometeremos el abuso de notación de poner \\(P(a)\\) en lugar de \\(P(\\{a\\})\\).\n\n\n\n\nEjemplo Grupo sanguíneo\n\n\nEn la página de la Fundación Banco de Sangre y Tejidos de las Islas Baleares (17-08-2023) podemos encontrar información sobre los porcentajes de tipos de sangre de los donantes de las Islas Baleares:\n\\[A: 46\\%;\\  B: 7.5\\%;\\  AB: 3.5\\%;\\  O: 43\\%.\\]\n¿Cuál es la probabilidad de que un balear donante de sangre no sea del tipo O?\nExperimento aleatorio: tipo de sangre de un paciente humano:\n\\[\\Omega=\\{\\mbox{A,B,AB,O}\\}\\]\nProbabilidad de un suceso: se asimila al porcentaje observado de individuos que no tienen sangre de tipo O.\nSuceso: \\(\\{\\mbox{O}\\}^c=\\{\\mbox{A,B,AB}\\}\\).\n\\[P(\\{\\mbox{O}\\}^c)\\!=\\!P(\\{\\mbox{A,B,AB}\\})\\!=\\!\nP(\\mbox{A})+P (\\mbox{B})+P(\\mbox{AB})\\!=\\!0.57.\\]\n\n\n\n2.2.1 Propiedades básicas de la probabilidad\n\n\nPropiedades básicas de la probabilidad\n\n\n\n\\(P(\\emptyset)=0\\).\n\\(\\scriptsize{P(A-B)=P(A)-P(A\\cap B)}\\) porque \\(\\scriptsize{P(A)=P(A-B)+P(A\\cap B)}\\).\n\n\n\n\n\n\n\n\n\n\n\nSi \\(B\\subseteq A\\), entonces \\(0\\leq P(B)\\leq P(A)\\).\n\\(P(A^c)=1-P(A)\\).\n\n\n\n\n\nProbabilidad de la unión\n\n\n\\(P(A\\cup B)=P(A)+P(B)-P(A\\cap B)\\)\n\n\n\n\n\n\n\n\n\n\n\nLa demostración se basa en la siguiente igualdad:\n\\[\\begin{eqnarray*}\nP(A)+P(B)-P(A\\cap B) &=& P(A-B)+P(A\\cap B)\\\\\n& & +P(B-A)+ P(A\\cap  B)-P(A\\cap  B)\\\\\n&=& P(A-B)+P(A\\cap B)+ P(B-A) \\\\\n&=& P(A\\cup B).\\\\\n\\end{eqnarray*}\\]\nPara la unión de tres conjuntos\n\\[\\begin{eqnarray*}\nP(A\\cup B\\cup C)&=&P(A)+P(B)+P(C)-P(A\\cap B) \\\\\n  & & - P(A\\cap C)-P(B\\cap C)+P(A\\cap B\\cap C).\\\\\n  \\end{eqnarray*}\\]\nEfectivamente tenemos que:\n\\[P(A\\cup B\\cup C)=P(1)+P(2)+P(3)+P(4)+P(5)+P(6)+P(7).\\]\n\n\n\n\n\n\n\n\n\n\n\nMás propiedades\n\n\n\nSi \\(A=\\{a_1,a_2,\\ldots,a_k\\}\\), entonces \\[\nP(A)=P(a_1)+P(a_2)+\\cdots+P(a_k).\n\\]\nSi todos los sucesos elementales tienen la misma probabilidad, \\[\nP(A)=\\frac{|A|}{|\\Omega|}\\Big(=\\frac{\\mbox{casos favorables}}{\\mbox{casos posibles}}\\Big).\n\\]\n\n\n\n\n\nEjemplo\n\n\nLos porcentajes de vocales de un determinado idioma (de alfabeto latino) según la Wikipedia son:\n\\[A: 18.7\\%;\\ E: 26.1\\%;\\ I: 25.7\\%;\\ O: 24.4\\%;\\ U: 5.1\\%.\\]\n¿Cuál es la probabilidad que una vocal escogida al azar de este idioma sea una E o una O?\nEl espacio muestral del experimento es \\(\\Omega=\\{A,E,I,O,U\\}\\).\nEl suceso que deseamos analizar es \\(\\{E,0\\}\\).\nY su probabilidad es\n\\[P(\\{E,O\\})=P(E)+P(O)=0.261+0.244=0.505.\\]\n\n\n\n\nEjemplo\n\n\nSegún un articulo de El País, en un control especial de la policía el \\(0.1\\%\\) de todos los conductores analizados en un control de tráfico dan positivo en un el test en cocaína, y el \\(1\\%\\) da positivo en cannabis. Un \\(1.05\\%\\) da positivo en alguno de los dos test.\n¿Cuál es la probabilidad que un individuo analizado en el control de drogas escogido al azar no de positivo en ninguno de lo dos test?\nLos sucesos elementales del enunciado del problema son:\n\n\\(A\\): dar positivo en cocaína; \\(P(A)=0.001.\\)\n\\(B\\): dar positivo en cannabis; \\(P(B)=0.01.\\)\n\nEn este caso nos interesa estudiar los sucesos:\n\n\\(A\\cup B\\): dar positivo en alguno de los dos test; \\(P(A\\cup B)=0.0105.\\)\n\\((A\\cup B)^c\\): no dar positivo en ninguno de los test,por tanto:\n\n\\[P((A\\cup B)^c)=1-P(A\\cup B)=1-0.0105=0.9895.\\]\n\n\n\n\nEjemplo\n\n\nEn un control especial de la policía el \\(0.1\\%\\) de todos los conductores analizados en un control de tráfico dan positivo en un el test en cocaína, y el \\(1\\%\\) da positivo en cannabis. Un \\(1.05\\%\\) da positivo en alguno de los dos test.\n¿Cuál es la probabilidad que un analizado al azar de positivo en los dos test en cocaína y cannabis? Los sucesos elementales son:\n\n\\(A\\): dar positivo en cocaína; \\(P(A)=0.001.\\)\n\\(B\\): dar positivo en cannabis; \\(P(B)=0.01.\\)\n\nEn este caso nos interesa estudiar los sucesos:\n\n\\(A\\cup B\\): dar positivo en algún de los dos test; \\(P(A\\cup B)=0.0105.\\)\n\\(A\\cap B\\): dar positivo en los dos test\n\nAhora bien, por la propiedad de la probabilidad de la unión de dos sucesos tenemos que:\n\\[\\begin{array}{rl}\n{P(A\\cap B)} &{=P(A)+P(B)-P(A\\cup B)}\\\\ &{=0.001+0.01-0.0105=0.0005}.\n\\end{array}\\]\nEn un control especial de la policía el \\(0.1\\%\\) de todos los conductores analizados en un control de tráfico dan positivo en un el test en cocaína, y el \\(1\\%\\) da positivo en cannabis. Un \\(1.05\\%\\) da positivo en alguno de los dos test.\n¿Cuál es la probabilidad de que un conductor analizado de positivo en cocaína pero no en cannabis?\nLos sucesos elementales son:\n\n\\(A\\): dar positivo en cocaína; \\(P(A)=0.001.\\)\n\\(B\\): dar positivo en cannabis; \\(P(B)=0.01.\\)\n\nEn este caso nos interesa estudiar los sucesos:\n\n\\(A\\cap B\\): dar positivo en los dos test; \\(P(A\\cap B)=0.0005.\\)\n\\(A-B\\): dar positivo en cocaína pero no en cannabis, por lo tanto tenemos que :\n\n\\[P(A-B) =P(A)-P(A\\cap B) =0.001-0.0005=0.0005.\\]\n\n\n\n\n2.2.2 Probabilidad condicionada\n\n\nProbabilidad condicionada\n\n\nDados dos sucesos \\(A\\) y \\(B\\), con \\(P(A)&gt;0\\), la probabilidad \\(P(B|A)\\) de \\(B\\) condicionado a \\(A\\) es la probabilidad\n\nde que suceda \\(B\\) suponiendo que pasa \\(A\\),\nde que si pasa \\(A\\), entonces suceda \\(B\\),\nde que un resultado de \\(A\\) también pertenezca a \\(B\\).\n\nSe calcula mediante la fórmula:\n\\[\nP(B|A)=\\frac{P(A\\cap B)}{P(A)}.\n\\]\n\n\n\n\nEjemplo\n\n\nEn una clase de 20 hombres y 30 mujeres, 15 hombres y 18 mujeres llevan gafas. Contestemos las siguientes preguntas:\n\n¿Cuál es la probabilidad de que un alumno lleve gafas?\n\n\\[\n\\frac{33}{50}\n\\]\n\n¿Cuál es la probabilidad de que un alumno sea mujer y lleve gafas?\n\n\\[\n\\frac{18}{50}\n\\]\n\n¿Cuál es la probabilidad de que un chica lleve gafas?\n\n\\[\n\\frac{18}{30}=\\frac{18/50}{30/50}=\\frac{P(\\mbox{mujer  y gafas})}{P(\\mbox{mujer})}.\n\\]\n\nSi escogemos un estudiante al azar ¿Cuál es la probabilidad que si es mujer, entonces lleve gafas?\n\n\\[\n\\frac{18}{30}.\n\\]\n\n¿Cuál es la probabilidad de que un alumno que lleve gafas sea mujer?\n\n\\[\n\\frac{18}{33}=\\frac{18/50}{33/50}=\\frac{P(\\mbox{mujer y gafas})}{P(\\mbox{gafas})}.\n\\]\n\nSi escogemos un estudiante al azar ¿Cuál es la probabilidad de que si lleva gafas, entonces sea mujer? \\[\n\\frac{18}{33}\n\\]\n\n¡Atención! Hay que distinguir bien entre\n\n\\(P(A\\cap B)\\): probabilidad de \\(A\\) \\(\\color{red}{\\text{y}}\\) \\(B\\).\n\nProbabilidad de que sea mujer y lleve gafas.\n\n\\(P(A|B)\\): probabilidad de que \\(\\color{red}{\\text{si}}\\) pasa \\(B\\), \\(\\color{red}{\\text{entonces}}\\) pase \\(A\\).\n\nProbabilidad de que, si es mujer, lleve gafas.\nCuando utilizamos probabilidad condicional \\(P(A|B)\\) estamos restringiendo el espacio muestral a \\(B\\).",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html#probabilidad-condicionada.-propiedades",
    "href": "p01_02_probabilidad.html#probabilidad-condicionada.-propiedades",
    "title": "2  Cálculo básico de probabilidades",
    "section": "2.3 Probabilidad condicionada. Propiedades",
    "text": "2.3 Probabilidad condicionada. Propiedades\nLa probabilidad condicionada es una probabilidad\n\n\nLa probabilidad condicionada es una medida de probabilidad\n\n\nSea \\(A\\subseteq \\Omega\\) un suceso tal que \\(P(A)&gt;0\\), entonces\n\\[\n\\begin{array}{rccl}\nP(-|A):& \\mathcal{P}(\\Omega) & \\to & [0,1]\\\\\n&B & \\mapsto & P(B|A).\n\\end{array}\n\\] satisface las propiedades de las probabilidades, como por ejemplo:\n\\[\n\\begin{array}{l}\nP(B^c|A)=1-P(B|A),\\\\\nP(B_1\\cup B_2|A)=P(B_1|A)+P(B_2|A)-P(B_1\\cap B_2|A).\n\\end{array}\n\\]\n\n\n\n\nEjercicio\n\n\nEscribid el resto de propiedades que cumpliría una probabilidad condicionada al evento \\(A\\).\n\n\n\n\nEjemplo\n\n\nUn 15% de los adultos son hipertensos, un 25% de los adultos creen que son hipertensos, y un 9% de los adultos son hipertensos y creen que lo son.\nSi un adulto cree que es hipertenso, ¿cuál es la probabilidad que lo sea?\nSean los sucesos\n\n\\(A\\): ser hipertenso, \\(P(A)=0.15\\) ,\n\\(B\\): creer ser hipertenso, \\(P(B)=0.25\\),\n\nentonces podemos definir el suceso:\n\n\\(A\\cap B\\): ser hipertenso y creerlo, \\(P(A\\cap B)=0.09\\).\n\nde donde, la probabilidad condicionada de ser hipertenso creyéndonos que lo somos es:\n\\[\\scriptsize P(A|B)=\\dfrac{P(A\\cap B)}{P(B)}=\\dfrac{0.09}{0.25}=0.36.\\]\nUn 15% de los adultos son hipertensos, un 25% de los adultos creen que son hipertensos, y un 9% de los adultos son hipertensos y creen que lo son.\nSi un adulto es hipertenso, ¿cuál es la probabilidad que crea que lo es?\nSi tenemos los sucesos:\n\n\\(A\\): ser hipertenso,\n\\(B\\): creer ser hipertenso\n\nentonces buscamos la probabilidad \\(P(B|A)\\):\n\\[\n\\begin{array}{rl}\nP(B|A) & =\\dfrac{P(A\\cap B)}{P(A)}=\\dfrac{0.09}{0.15}=\n0.6\n\\end{array}\n\\]\n\n\n\n\nEjemplo\n\n\nUn dígito de control de error toma el valor 0 en el 99% de los casos en que hay un error. Si la probabilidad de error en un mensaje es del \\(0.5\\%\\). ¿cuál es la probabilidad de que el mensaje sea erróneo y el código de error tenga valor 0?\n\n\\(B\\): mensaje con error; \\(P(B)=0.005\\),\n\\(A\\): código de error vale 0,\n\\(P(A|B)=0.99\\),\n\nentonces: \\[P(A\\cap B)=P(B)\\cdot P(A|B)=0.005\\cdot 0.99=0.00495.\\]\n\n\n\n\nEjemplo SPAM\n\n\nUn 50% de correos recibidos en un servidor llevan adjuntos y un 65% son publicidad no deseada (SPAM). Sólo un 15% de estos correos no llevan adjuntos y no son SPAM.\n\n¿Cuál es la probabilidad que un correo lleve adjunto si es SPAM?\n¿Cuál es la probabilidad que un correo no tenga adjuntos si no es SPAM?\n\nUn 50% de correos recibidos en un servidor llevan adjuntos y un 65% son publicidad no deseada (SPAM). Sólo un 15% de estos correos no llevan adjuntos y no son SPAM.\n\n¿Cuál es la probabilidad que un correo lleve adjunto si es SPAM?\n\\(A\\): llevar adjuntos; \\(P(A)=0.5\\),\n\\(S\\): SPAM; \\(P(S)=0.65\\),\n\\(A^c\\cap S^c=(A\\cup S)^c\\): no llevar adjunto y no ser SPAM; \\(P((A\\cup S)^c)=0.15\\),\n\n\\[P(A|S)=\\dfrac{P(A\\cap S)}{P(S)}=?\\]\nUn 50% de correos recibidos en un servidor llevan adjuntos y un 65% son publicidad no deseada (SPAM). Sólo un 15% de estos correos no llevan adjuntos y no son SPAM.\n\n¿Cuál es la probabilidad que un correo lleve adjunto si es SPAM?\n\\(P(A)=0.5, P(S)=0.65, P(A^c\\cap S^c)=P((A\\cup S)^c)=0.15\\),\n\\(P(A\\cup S)=1-P((A\\cup S)^c)=0.85\\),\n\\(P(A\\cap S)=P(A)+P(S)-P(A\\cup S)=0.3\\), \\[P(A|S)=\\dfrac{P(A\\cap S)}{P(S)}=\\dfrac{0.3}{0.65}\\approx 0.46.\\]\n\nUn 50% de correos recibidos en un servidor llevan adjuntos y un 65% son publicidad no deseada (SPAM). Sólo un 15% de estos correos no llevan adjuntos y no son SPAM.\n\n¿Cuál es la probabilidad de que un correo no lleve adjuntos si no es SPAM?\n\\(P(A)=0.5, P(S)=0.65, P(A^c\\cap S^c)=P((A\\cup S)^c)=0.15.\\)\n\n\\[P(A^c|S^c)=\\dfrac{P(A^c\\cap S^c)}{P(S^c)}=\\dfrac{P(A^c\\cap S^c)}{1-P(S)}=\\dfrac{0.15}{0.35}\\approx 0.43.\\]",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html#teorema-de-la-probabilidad-total",
    "href": "p01_02_probabilidad.html#teorema-de-la-probabilidad-total",
    "title": "2  Cálculo básico de probabilidades",
    "section": "2.4 Teorema de la probabilidad total",
    "text": "2.4 Teorema de la probabilidad total\n\n\nTeorema de la probabilidad total para dos sucesos\n\n\nDados dos sucesos \\(A\\) y \\(B\\) se tiene que\n\\[\n\\begin{array}{rl}\nP(B)&= P(B\\cap A) +P(B\\cap A^c)\\\\\n& =P(A)\\cdot P(B|A)+ P(A^c)\\cdot P(B|A^c).\n\\end{array}\n\\]\n\n\n\n\nPartición del espacio espacio muestral\n\n\nLos sucesos \\(A_1,A_2,\\ldots, A_n\\) son una partición del espacio muestral \\(\\Omega\\) de un determinado experimento aleatorio, si cumplen las condiciones siguientes:\n\n\\(A_1\\cup A_2\\cup\\ldots\\cup A_n=\\Omega\\),\n\\(A_1,A_2,\\ldots,A_n\\) son incompatibles dos a dos (\\(A_i\\cap A_j=\\emptyset\\)).\n\n\n\n\n\nTeorema de la probabilidad total \\(n\\) sucesos\n\n\nSea \\(A_1,A_2,\\ldots,A_n\\) una partición de \\(\\Omega\\). Sea \\(B\\) un suceso cualquiera. Entonces\n\\[\n\\begin{array}{rl}\nP(B)&= P(B\\cap A_1)+\\cdots +P(B\\cap A_n)\\\\\n& =P(A_1)\\cdot P(B|A_1)+\\ldots+P(A_n)\\cdot P(B|A_n).\n\\end{array}\n\\]\n\n\n\n\nEjemplo\n\n\nUn dígito de control de error toma el valor 0 en un \\(99\\%\\) de los casos en que hay un error y en un \\(5\\%\\) de los mensajes sin error. La probabilidad de error en un mensaje es del \\(0.5\\%\\).\n¿Cuál es la probabilidad de que un mensaje escogido al azar tenga el dígito de control a 0?\nSean los sucesos del enunciado:\n\n\\(B\\): mensaje con error; \\(P(B)=0.005\\),\n\\(A\\): código de error vale 0,\n\nentonces obtenemos las probabilidades a partir del enunciado:\n\n\\(P(A|B)=0.99,\\)\n\\(P(A|B^c)= 0.05\\)\n\ny por tanto,\n\\[\n\\begin{array}{rl}\nP(A)=& P(B)\\cdot P(A|B)+P(B^c)\\cdot P(A|B^c)\\\\\n& =0.005\\cdot 0.99+0.995\\cdot 0.05=0.0547.\n\\end{array}\n\\]",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html#clasificación-o-diagnósticos",
    "href": "p01_02_probabilidad.html#clasificación-o-diagnósticos",
    "title": "2  Cálculo básico de probabilidades",
    "section": "2.5 Clasificación o Diagnósticos",
    "text": "2.5 Clasificación o Diagnósticos\nConsideremos alguna de las siguientes situaciones:\n\nUn algoritmo detecta si una transacción con tarjeta de crédito es fraude o no.\nUn algoritmo detecta si tiene o no que mostrar un anuncio en una web.\nUn prueba de embarazo.\nUna prueba médica para una enfermedad concreta.\n\nNos ceñiremos a la casuística más elemental el algoritmo de clasificación o la diagnosis solo da dos resultado Positivo (sí tienes la enfermedad, sí es un fraude) o Negativo (en caso contrario).\nEn todas estas situaciones podemos calcular lo que se llama matriz de confusión que representa todas las situaciones posibles. En el caso de estudiar una condición de tipo binario,\n\n\n\n\nEl Test da Positivo\nEl Test da Negativo\n\n\n\n\nCondición Positiva\nCorrecto\nError\n\n\nCondición Negativa\nError\nCorrecto\n\n\n\nEn general los modelos y algoritmos de clasificación suelen aportar puntuaciones (scores) que determinan el grado de pertenencia a una clase, o que miden si dos objetos están en la misma clase.\nAsí el resultado del clasificador o del diagnóstico puede ser:\n\nun número real, en cuyo caso debe clasificador entre cada clase debe determinarse por un valor umbral (threshold) por ejemplo para determinar si una persona está estresado podemos dar un scores entre 0 y 1 (1 máximo estrés 0 estrés nulo),\nun resultado discreto que indica directamente una de las clases (esto es necesario si es un algoritmo que debe decidir qué hacer con el objeto.\n\nPositivos y Negativos Consideremos un problema de predicción de clases binario, en la que los resultados se etiquetan positivos (P) o negativos (N). Hay cuatro posibles resultados a partir de un clasificador binario como el propuesto.\n\nSi el resultado de una exploración es P y el valor dado es también P, entonces se conoce como un Verdadero Positivo (VP).\nSin embargo si el valor real es N entonces se conoce como un Falso Positivo (FP).\nDe igual modo, tenemos un Verdadero Negativo (VN) cuando tanto la exploración como el valor dado son N.\nUn Falso Negativo (FN) cuando el resultado de la predicción es N pero el valor real es P.\n\nUn ejemplo aproximado de un problema real es el siguiente: consideremos una prueba diagnóstica que persiga determinar si una persona tiene una cierta enfermedad.\n\nUn falso positivo en este caso ocurre cuando la prueba predice que el resultado es positivo, cuando la persona no tiene realmente la enfermedad.\nUn falso negativo, por el contrario, ocurre cuando el resultado de la prueba es negativo, sugiriendo que no tiene la enfermedad cuando realmente sí la tiene.\n\nEn un diagnósticos de una cierta condición (por ejemplo, test embarazo, test de enfermedad), tenemos dos tipos de sucesos:\n\n\\(T\\): el test da positivo,\n\\(M\\): el sujeto satisface la condición.\nFalsos positivos \\(T\\cap M^c\\): El test da positivo, pero la condición no se da,\nCoeficiente de falsos positivos \\(P(T|M^c)\\),\nFalsos negativos \\(T^c\\cap M\\): El test da negativo, pero la condición sí que se da,\nCoeficiente de falsos negativos: \\(P(T^c|M)\\).\n\n\n\nEjemplo\n\n\nUn test diseñado para diagnosticar una determinada enfermedad tiene un coeficiente de falsos negativos de 0.06, y un coeficiente de falsos positivos de 0.04. En un estudio masivo se observa que un 15% de la población da positivo al test.\n¿Cuál es la probabilidad que una persona escogida aleatoriamente tenga esta enfermedad? Los datos del problema son:\n\n\\(T\\): dar positivo al test; \\(P(T)=0.15\\),\n\\(M\\): tener la enfermedad,\n\\(P(T)=0.15\\), \\(P(T^c|M)=0.06\\), \\(P(T|M^c)=0.04\\),\n¿\\(P(M)\\)?\n\\(P(T)=0.15\\), \\(P(T^c|M)=0.06\\), \\(P(T|M^c)=0.04.\\)\n\n\\[\nP(T) =P(M)\\cdot P(T|M)+P(M^c)\\cdot P(T|M^c).\n\\]\ndonde\n\\[\n\\begin{array}{l}\nP(T|M)=1-P(T^c|M)=0.94 \\\\\nP(M^c)=1-P(M).\n\\end{array}\n\\]\nPor lo tanto\n\\[\n\\begin{array}{rl}\n0.15 & = P(M)\\cdot 0.94+(1-P(M))\\cdot 0.04\\\\\n& =0.04+0.9\\cdot P(M)\\\\\nP(M) & =\\dfrac{0.11}{0.9}\\approx 0.1222.\n\\end{array}\n\\]",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html#fórmula-de-bayes",
    "href": "p01_02_probabilidad.html#fórmula-de-bayes",
    "title": "2  Cálculo básico de probabilidades",
    "section": "2.6 Fórmula de Bayes",
    "text": "2.6 Fórmula de Bayes\n\n\nFórmula de Bayes\n\n\nSean \\(A\\) y \\(B\\) dos sucesos. Si \\(P(B)&gt;0\\), entonces\n\\[\nP(A|B) =\\dfrac{P(A)\\cdot P(B|A)}{P(B)}=\\dfrac{P(A)\\cdot P(B|A)}{P(A)\\cdot P(B|A)+P(A^c)\\cdot P(B|A^c)}.\n\\]\n\n\n\n\nEjercicio\n\n\nDemostrar el teorema de Bayes utilizando que\n\\[P(A|B) =\\dfrac{P(A\\cap B)}{P(B)}=\\cdots\\]\n\n\n\n\nTeorema de Bayes\n\n\nSea \\(A_1,A_2,\\ldots,A_n\\) una partición de \\(\\Omega\\). Sea \\(B\\) un suceso tal que \\(P(B)&gt;0\\). entonces(para cualquier \\(i=1,2,\\ldots,n\\)):\n\\[\n\\begin{array}{rl}\nP(A_i|B) & =\\dfrac{P(A_i)\\cdot P(B|A_i)}{P(B)}\\\\\n& =\\dfrac{P(A_i)\\cdot P(B|A_i)}{P(A_1)\\cdot P(B|A_1)+\\cdots+P(A_n)\\cdot P(B|A_n)},\n\\end{array}\n\\]\n\n\n\n\nEjercicio\n\n\nDemostrar el teorema de Bayes utilizando que\n\\[P(A_i|B) =\\dfrac{P(A_i\\cap B)}{P(B)}=\\cdots\\]\n\n\n\n\nEjemplo\n\n\nUn test para detección de VIH da positivo un 99% de los casos en los que está presente y en un 5% de los casos en los que el virus está ausente. En una población con un \\(0.5\\%\\) de infectados por VIH, ¿cuál es la probabilidad que un individuo que haya dado positivo en el test esté infectado?\nLos sucesos del ejemplo son:\n\n\\(A\\): individuo infectado,\n\\(B\\): el test da positivo,\n\nde donde podemos calcular:\n\\[\\scriptsize{\nP(A|B) =\\dfrac{P(B|A)\\cdot P(A)}{P(B|A)\\cdot P(A)+P(B|A^c)\\cdot P(A^c)}=\\dfrac{0.99\\cdot 0.005}{0.005\\cdot 0.99+0.995\\cdot 0.05}=0.09.}\n\\]\nUn test para detección de VIH da positivo un 99% de los casos en los que está presente y en un 5% de los casos en los que el virus está ausente. En una población con un \\(0.5\\%\\) de infectados por VIH, ¿cuál es la probabilidad de que un individuo que haya dado negativo en el test no esté infectado?\nLos sucesos del ejemplo son:\n\n\\(A\\): individuo infectado,\n\\(B\\): el test da positivo,\n\nde donde podemos calcular:\n\\[\n\\scriptsize{P(A^c|B^c) =\\dfrac{P(B^c|A^c)\\cdot P(A^c)}{P(B^c|A)\\cdot P(A)+P(B^c|A^c)\\cdot P(A^c)}=\\dfrac{0.95\\cdot 0.995}{0.01\\cdot 0.005+0.95\\cdot 0.995}=0.999947.}\n\\]\n\n\n\n\nEjercicio\n\n\nSe ha observado que los clientes de una empresa de ventas por internet son de tres tipos, A, B y C, disjuntos dos a dos. La probabilidad que ser de cualquiera de cada uno de los tipos es \\(1/3\\), pero la probabilidad de compra de cada tipo es diferente: si es de tipo A compra un 50% de las veces, si de tipo B, un 75% de las veces, y de tipo C, un 60%.\nSupongamos que llega un cliente ¿cuál es la probabilidad de que si ha comprado sea del tipo B?\n\nLos sucesos del ejercicio son \\(A\\): el cliente es de tipo A, \\(B\\): el cliente es de tipo B, \\(C\\): el cliente es de tipo C y\n\n\\[P(A)=P(B)=P(C)=1/3.\\]\nBuscamos estudiar el suceso \\(E\\): el cliente compra, se tiene que:\n\\[P(E|A)=0.5, P(E|B)=0.75, P(E|C)=0.6.\\]\n\\[P(B|E)\\!=\\!\\dfrac{P(E|B)\\cdot P(B)}{P(E|A)\\!\\cdot\\! P(A)\\!+\\!P(E|B)\\!\\cdot\\! P(B)\\!+\\!P(E|C)\\!\\cdot\\! P(C)}\\!=\\!\\ldots\\]\n\n\n\n\nEjemplo\n\n\nUn test de detección precoz de abandono de clientes de una empresa de telefonía da positivo el 97.5% de las ocasiones en las que, posteriormente, el cliente se da de baja, y un 12% de las veces en que no se dio de baja. La probabilidad que un cliente escogido al azar se dé de baja es de un 2%.\n\n¿Cuál es la probabilidad que un individuo escogido al azar de positivo en el test?\n¿Cuál es la probabilidad que un individuo escogido al azar se de de baja y dé positivo en el test?\n¿Cuál es la probabilidad que un individuo que dé negativo en el test se dé de baja?\n\nDefinimos los sucesos y datos del ejercicio:\n\n\\(T\\): Dar positivo al test,\n\\(B\\): darse de baja; \\(P(B)=0.02\\),\n\\(P(T|B)=0.975, P(T|B^c)=0.12\\).\n\n\\[P(B)=0.02, P(T|B)=0.975, P(T|B^c)=0.12.\\]\n\n¿Cuál es la probabilidad que un individuo escogido al azar de positivo en el test?\n\n\\[\n\\begin{array}{rl}\nP(T) = & P(B)\\cdot P(T|B)+P(B^c)\\cdot P(T|B^c)\\\\[1ex]\n& =0.02\\cdot 0.975+0.98\\cdot 0.12=0.1371.\n\\end{array}\n\\]\n\n¿Cuál es la probabilidad que un individuo escogido al azar se de de baja y dé positivo en el test?\n\n\\[P(B)=0.02, P(T|B)=0.975, P(T|B^c)=0.12.\\]\n\n¿Cuál es la probabilidad que un individuo que dé negativo?\n\\[\\begin{array}{rl}\nP(B|T^c)= &\\displaystyle \\frac{P(B\\cap T^c)}{P(T^c)}=\n\\frac{P(B)-P(B\\cap T)}{1-P(T)}\\\\[2ex] & \\displaystyle =\n\\frac{0.02-0.0195}{1-0.1371}\\approx 0.00058.\n\\end{array}\n\\]\nO también se obtiene así \\[P(B|T^c)=\\frac{P(T^c|B)\\cdot P(B)}{P(T^c|B)\\cdot P(B)+P(T^c|B^c)\\cdot P(B^c)},\n\\]\n\ndonde \\(P(T^c|B)=1-P(T|B)=0.025\\) y \\(P(T^c|B^c)=1-P(T|B^c)=0.88.\\)",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_02_probabilidad.html#independencia-de-sucesos",
    "href": "p01_02_probabilidad.html#independencia-de-sucesos",
    "title": "2  Cálculo básico de probabilidades",
    "section": "2.7 Independencia de sucesos",
    "text": "2.7 Independencia de sucesos\n\n\nSucesos Independientes\n\n\nDiremos que los sucesos \\(A\\) y \\(B\\) son independientes si \\(P(A\\cap B)=P(A)\\cdot P(B)\\).\n\\(A_1,\\ldots, A_n\\) son sucesos independientes cuando, para toda subfamilia \\(A_{i_1},\\ldots,A_{i_k}\\), \\[\nP(A_{i_1}\\cap \\cdots\\cap A_{i_k})=P(A_{i_1})\\cdots P(A_{i_k}).\n\\]\n\n\n\n\nProposición\n\n\nDados dos sucesos \\(A\\) y \\(B\\) con \\(P(A),P(B)&gt;0\\), las siguientes afirmaciones son equivalentes:\n\n\\(A\\) y \\(B\\) son independientes.\n\\(P(A|B)=P(A)\\).\n\\(P(B|A)=P(B)\\).\n\\(A\\) y \\(B\\) son independientes\n\\(A^c\\) y \\(B\\) son independientes.\n\\(A\\) y \\(B^c\\) son independientes.\n\\(A^c\\) y \\(B^c\\) son independientes.\n\n\n\n\n\nEjemplo ventas de billetes de avión y alojamiento en hotel.\n\n\nEn la web de viajes WEBTravel, el 55% de los clientes compra billete de avión, el \\(20\\%\\) alojamiento en hotel, y el \\(60\\%\\) billete de avión o alojamiento en hotel. ¿Son los sucesos comprar billete de avión y comprar alojamiento en hotel independientes?\nLos sucesos y datos del ejemplo son:\n\n\\(A\\): comprar billete de avión; \\(P(A)=0.55\\),\n\\(B\\): comprar alojamiento; \\(P(B)=0.2\\),\n\npor tanto, podemos calcular las probabilidades siguientes\n\\(P(A\\cap B)=P(A)+P(B)-P(A\\cup B)=0.55+0.2-0.6=0.15\\) y \\(P(A)\\cdot P(B) = 0.55\\cdot 0.2=0.11.\\)\nConcluimos que son dependientes, ya que \\(P(A\\cap B)\\neq P(A)\\cdot P(B)\\).\n\n\n\n2.7.1 Sucesos independientes versus disjuntos\n\n\nEjercicios\n\n\n\nDos sucesos \\(A\\) y \\(B\\) disjuntos, ¿son necesariamente independientes?\nDos sucesos \\(A\\) y \\(B\\) independientes, ¿son necesariamente disjuntos?\n\\(\\emptyset\\) y un suceso cualquiera \\(A\\), ¿son necesariamente independientes?\n\\(\\Omega\\) y un suceso cualquiera \\(A\\), ¿son necesariamente independientes?\n¿Qué condiciones se tienen que dar para que un suceso \\(A\\) sea independiente de si mismo?",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Cálculo básico de probabilidades</span>"
    ]
  },
  {
    "objectID": "p01_03_variables_aleatorias.html",
    "href": "p01_03_variables_aleatorias.html",
    "title": "3  Introducción a las variables aleatorias",
    "section": "",
    "text": "3.1 Introducción",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducción a las variables aleatorias</span>"
    ]
  },
  {
    "objectID": "p01_03_variables_aleatorias.html#introducción",
    "href": "p01_03_variables_aleatorias.html#introducción",
    "title": "3  Introducción a las variables aleatorias",
    "section": "",
    "text": "Hasta ahora nuestros sucesos han sido de varios tipos: \\(\\{C,+\\}\\) en la moneda, nombres de periódicos, ángulos en una ruleta, número de veces que sale cara en el lanzamiento de una moneda etc.\nNecesitamos estandarizar de alguna manera todos estos sucesos. Una solución es asignar a cada suceso un cierto conjunto de números reales, es decir, convertir todos los sucesos en sucesos de números reales para trabajar con ellos de forma unificada.\nPara conseguirlo utilizaremos unas funciones que transformen los elementos del espacio muestral en números; esta funciones son las variables aleatorias.",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducción a las variables aleatorias</span>"
    ]
  },
  {
    "objectID": "p01_03_variables_aleatorias.html#definición-de-variable-aleatoria",
    "href": "p01_03_variables_aleatorias.html#definición-de-variable-aleatoria",
    "title": "3  Introducción a las variables aleatorias",
    "section": "3.2 Definición de variable aleatoria",
    "text": "3.2 Definición de variable aleatoria\nComenzaremos dando una definición poco rigurosa, pero suficiente, de variable aleatoria.\n\n\nVariable Aleatoria (definición práctica)\n\n\nUna variable aleatoria (v.a.) es una aplicación que toma valores numéricos determinados por el resultado de un experimento aleatorio\n\n\nNotación y observaciones:\n\nNo es una variable en el sentido matemático, sino una función.\nNormalmente representaremos las v.a. por letras mayúsculas \\(X,Y,Z\\ldots\\)\nLos valores que “toman” las v.a. los representaremos por letras minúsculas (las mismas en principio) \\(x,y,z\\ldots\\)\n\n\n\nEjemplo\n\n\nLanzamos un dado convencional de parchís el espacio muestral del experimento es\n\\[\\Omega=\\{1,2, 3, 4,  5, 6\\}.\\]\nUna v.a \\(X:\\Omega\\to\\mathbb{R}\\) sobre este espacio queda definida por\n\\[X(1)=1, X(2)=2, X(3)=3, (4)=4, X(5)=5, X(6)=6.\\]\n\nAhora el suceso \\(A=\\{2, 4, 6\\}\\), es decir “salir número par”, es equivalente a \\(\\{X=2,X=4,X=6\\}\\).\nEl suceso \\(B=\\{1,2,3\\}\\), es decir “salir un número inferior o igual a \\(3\\)” es en términos de la v.a. \\(\\{X=1,X=2,X=3\\}\\) o también \\(\\{X\\leq\n3\\}\\).\n\n\n\n\n\nEjemplo\n\n\nConsideremos el experimento lanzar una anilla al cuello de una botella. Si acertamos a ensartar la anilla en la botella el resultado del experimento es éxito y fracaso en caso contrario.\nEl espacio muestral asociado a este experimento será \\(\\Omega=\\{\\mbox{éxito, fracaso}\\}\\). Construyamos la siguiente variable aleatoria:\n\\[X:\\{\\mbox{éxito, fracaso}\\}\\to\\mathbb{R}\\]\ndefinida por\n\\[X(\\mbox{éxito})=1 \\mbox{ y } X(\\mbox{fracaso})=0.\\]",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducción a las variables aleatorias</span>"
    ]
  },
  {
    "objectID": "p01_03_variables_aleatorias.html#tipos-de-variables-aleatorias",
    "href": "p01_03_variables_aleatorias.html#tipos-de-variables-aleatorias",
    "title": "3  Introducción a las variables aleatorias",
    "section": "3.3 Tipos de variables aleatorias",
    "text": "3.3 Tipos de variables aleatorias\nHay dos tipos fundamentales de variables aleatorias, las discretas y las continuas.\nDamos a continuación una definición informal.\n\n\nVariables Aleatorias Discretas y Continuas\n\n\n\nUna variable aleatoria es discreta si sólo puede tomar una cantidad numerable de valores con probabilidad positiva.\nLas variables aleatorias continuas toman valores en intervalos.\nTambién existen las variables aleatorias mixtas; con una parte discreta y otra continua.\n\n\n\n\n\nEjemplo\n\n\nSon variables aleatorias discretas:\n\nNúmero de artículos defectuosos en un cargamento.\nNúmero de clientes que llegan a una ventanilla de un banco en una hora.\nNúmero de errores detectados en las cuentas de una compañía.\nNúmero de reclamaciones de una póliza de un seguro médico.\n\nSon variables aleatorias continuas:\n\nRenta anual de una familia.\nCantidad de petróleo importado por un país.\nVariación del precio de las acciones de una compañía de telecomunicaciones.\nPorcentaje de impurezas en un lote de productos químicos.",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducción a las variables aleatorias</span>"
    ]
  },
  {
    "objectID": "p01_03_variables_aleatorias.html#variables-aleatorias-discretas",
    "href": "p01_03_variables_aleatorias.html#variables-aleatorias-discretas",
    "title": "3  Introducción a las variables aleatorias",
    "section": "3.4 Variables aleatorias discretas",
    "text": "3.4 Variables aleatorias discretas\nVeremos en esta sección los elemento y funciones básicas de las variables aleatorias discretas.\n\n3.4.1 Distribuciones de probabilidad para v.a. discretas.\n\nPasamos ahora a describir el comportamiento de la v.a. Para ello utilizaremos distintas funciones que nos darán algunas probabilidades de la variable aleatoria.\nEn el caso discreto estas funciones son la de probabilidad, y la función de distribución o de probabilidad acumulada.\nEn el caso discreto la función de probabilidad es la que nos da las probabilidades de los sucesos elementales de la v.a. que definimos a continuación.\n\n\n\n3.4.2 Función de probabilidad para variables discretas\n\n\nFunción de Probabilidad\n\n\nLa función de probabilidad (probability mass function o incluso abusando de notación probability density function) de una variable aleatoria discreta \\(X\\) a la que denotaremos por \\(P_{X}(x)\\) está definida por\n\\[P_{X}(x)=P(X=x),\\]\nes decir la probabilidad de que \\(X\\) tome el valor \\(x\\).\nSi \\(X\\) no asume ese valor \\(x\\), entonces \\(P_{X}(x)=0\\).\n\n\n\n\n3.4.3 Función de probabilidad discreta\n\n\nDominio de una variable aleatoria discreta\n\n\nEl conjunto \\[D_X=\\{ x\\in\\mathbb{R} \\mid P_X(x)&gt;0\\}\\] recibe el nombre de dominio de la v.a. y son los valores posibles de esta variable.\n\n\nEn el caso discreto lo más habitual es que \\(X(\\Omega)=D_X\\).\n\n\nEjemplo: parchís\n\n\nLanzamos un dado de parchís una vez, en esta ocasión representaremos los sucesos elementales por el número de puntos de la cara obtenida, tenemos que \\[\\Omega=\\{\\mbox{1-puntos,2-puntos,3-puntos,4-puntos,5-puntos,6-puntos}\\}\\] y la variable aleatoria \\(X:\\Omega\\to \\mathbb{R}\\) viene definida por\n\\[X(\\mbox{i-puntos})=i\\mbox{ para } i=1,2,3,4,5,6.\\]\nSupongamos que el dado está bien balanceado. Entonces \\[\\scriptsize{P_{X}(1)=P_{X}(2)=P_{X}(3)=P_{X}(4)=P_{X}(5)=P_{X}(6)=\\frac16; \\mbox{  concretamente}.}\\]\n\\[\\scriptsize{\nP_{X}(x)=\n  \\left\\{\n  \\begin{array}{ll}\n   \\frac16 ,& \\mbox{si } x=1,2,3,4,5,6.\\\\\n  0 & \\mbox{en otro caso.}\n  \\end{array}\n  \\right.}\n\\]\nSu dominio es \\[D_X=\\{1,2,3,4,5,6\\}.\\]\n\n\n\n\nEjemplo: lanzamiento moneda\n\n\nSea \\(X\\) la v.a. asociada al lanzamiento de una moneda. Su espacio muestral es \\(\\Omega=\\{c,+\\}\\), la v.a. queda definida por:\n\\[X(\\omega)=\\left\\{\\begin{array}{ll} 1 & \\mbox{si } \\omega=c \\\\\n0 & \\mbox{si }\\omega=+\\end{array}\\right.\\]\nSu función de probabilidad es:\n\\[P_{X}(x)=P(X=x)=\\left\\{\\begin{array}{ll} \\frac12, & \\mbox{si } x=0,1,\\\\\n0, & \\mbox{en otro caso}.\\end{array}\\right.\\]\nFinalmente su dominio es \\(D_X=\\{0,1\\}.\\)\n\n\n\n\nEjemplo urna con bolas\n\n\nTenemos una urna con tres bolas rojas, una negra y dos blancas. Realizamos una extracción y observamos el color de la bola entonces un espacio muestral es \\[\\Omega=\\{roja, blanca, negra\\}.\\]\nUna variable aleatoria asociada al experimento es:\n\\[X(\\omega)=\\left\\{\\begin{array}{ll} 1, & \\mbox{si } \\omega=roja,  \\\\\n2, & \\mbox{si }\\omega=negra ,\\\\ 3, & \\mbox{si } \\omega=blanca.\\end{array}\\right.\\]\nLa función de probabilidad es\n\\[P_{X}(x)=\\left\\{\\begin{array}{ll} \\frac36, & \\mbox{si } x=1,\\\\[0.5ex]\n\\frac16, & \\mbox{si } x=2,\\\\ \\frac26, & \\mbox{si } x=3,\\\\ 0 & \\mbox{en otro\ncaso.}\\end{array}\\right.\\]\nEl dominio de la v.a. \\(X\\) es \\(D_X=\\{1,2,3\\}.\\)\n\n\n\n\n3.4.4 Propiedades de la función de probabilidad.\n\n\nPropiedades básicas de la función de probabilidad\n\n\nSea \\(X\\) una v.a. discreta \\(X:\\Omega:\\to\\mathbb{R}\\) con dominio \\(D_X\\). Su función de probabilidad \\(P_{X}\\) verifica las siguientes propiedades:\n\n\\(0\\leq P_{X}(x)\\leq 1\\) para todo \\(x\\in\\mathbb{R},\\)\n\\(\\sum\\limits_{x\\in D_X} P_{X}(x)=1.\\)\n\n\n\n\n\nEjemplo: lanzamiento moneda\n\n\nLanzamos al aire tres veces, de forma independiente, una moneda perfecta. El espacio muestral de este experimento es \\[\\Omega=\\{ccc,cc+,c+c,+cc,c++,+c+,++c,+++\\};\\] donde cada cadena contiene \\(c\\) o \\(+\\) en orden de aparición.\nEste espacio tiene todos los sucesos elementales equiprobables.\nConsideremos la variable aleatoria asociada a este experimento:\n\\[X=\\mbox{ número de caras en los tres lanzamientos}.\\]\nSu función de probabilidad es:\n\\[\n\\begin{array}{l}\nP(X=0)=P(\\{+++\\})=\\frac18,\\\\ P(X=1)=P(\\{c++,+c+,++c\\})=\\frac38,\\\\\n    P(X=2)=P(\\{cc+,c+c,+cc\\})=\\frac38,\\\\\n    P(X=3)=P(\\{ccc\\})=\\frac18.\n\\end{array}\n\\]\nPodemos reescribir la función de probabilidad de \\(X\\) de forma simplificada:\n\\[P_{X}(x)=\\left\\{\\begin{array}{ll} \\frac18, & \\mbox{si } x=0, 3,\\\\[0.5ex]\n\\frac38, & \\mbox{si } x=1,2,\\\\ 0, & \\mbox{en otro caso}.\\end{array}\\right.\\]\nEfectivamente los valores de la función de distribución suman 1:\n\\[\\sum_{x=0}^3 P_X(x)= \\frac18+\\frac38+\\frac38+\\frac18=1.\\]\n\n\n\n\n3.4.5 Función de distribución de variables aleatorias\n\n\nDistribución de Probabilidad\n\n\nLa función de distribución de probabilidad (acumulada) de la v.a. \\(X\\) (de cualquier tipo; discreta o continua) \\(F_{X}(x)\\) representa la probabilidad de que \\(X\\) tome un menor o igual que \\(x\\), es decir,\n\\[F_{X}(x)=P(X\\leq x).\\]\nEsta función también se denomina función de distribución de probabilidad o simplemente función de distribución de una v.a., y en inglés cumulative distribution function por lo que se abrevia con el acrónimo cdf.\n\n\n\n\nPropiedades de la Función de Distribución\n\n\nSea \\(X\\) una v.a. y \\(F_{X}\\) su función de distribución:\n\n\\(P(X&gt;x)=1-P(X\\leq x)=1-F_{X}(x).\\)\nSea a y b tales que \\(a&lt;b\\), \\(P(a&lt;X\\leq b)=P(X\\leq b)-P(X\\leq a)=F_{X}(b)-F_{X}(a).\\)\n\n\n\nDemostración:\nTenemos que el complementario de \\(X\\) mayor que \\(x\\) es: \\(\\overline{\\left\\{X&gt;x\\right\\}}=\\left\\{X&gt;x\\right\\}^c=\\left\\{X\\leq x\\right\\}\\). Además,\n\\[P(X&gt;x)=1-P(\\overline{\\left\\{X&gt;x\\right\\}})=1-P(X\\leq x)=1-F_{X}(x),\\]\nlo que demuestra la primera propiedad.\nPor otro lado, si \\(X\\) se encuentra entre dos valores \\(a\\) y \\(b\\) \\(\\left\\{a&lt; X \\leq b\\right\\}= \\left\\{X\\leq b\\right\\}-\\left\\{X\\leq  a\\right\\}\\). Ahora podemos hacer\n\\[\\begin{eqnarray*}\nP(a&lt;X\\leq b)&=&P(\\left\\{X\\leq b\\right\\}-\\left\\{X\\leq a\\right\\})\\\\\n&=& P(\\left\\{X\\leq b\\right\\})-P(\\left\\{X\\leq a\\right\\})\\\\\n&=& F_{X}(b)-F_{X}(a).\n\\end{eqnarray*}\\]\nLo que demuestra la segunda propiedad.\n\n\nPropiedades de la Función de Distribución\n\n\nSea \\(F_{X}\\) la función de distribución de una v.a. \\(X\\) entonces:\n\n\\(0\\leq F_{X}(x)\\leq 1\\).\nLa función \\(F_{X}\\) es no decreciente.\nLa función \\(F_{X}\\) es continua por la derecha.\nSi denotamos por \\(F_X(x_0^{-})=\\displaystyle \\lim_{x\\to x_0^{-}} F(x)\\), entonces se cumple que \\(P(X&lt; x_0)=F_X(x_0^{-})\\) y que \\(P(X=x_0)=F_X(x_0)-F_X(x_0^{-})\\).\nSe cumple que \\(\\displaystyle \\lim_{x\\to\\infty} F_{X}(x)=1\\); \\(\\displaystyle \\lim_{x\\to-\\infty}F_{X}(x)=0\\).\nToda función \\(F\\) verificando las propiedades anteriores es función de distribución de alguna v.a. \\(X\\).\n\n\n\nAdvertencia\nEn las propiedades anteriores no se pueden cambiar en general las desigualdades de estrictas o no estrictas.\nVeamos que propiedades tenemos cuando se cambian estas desigualdades.\n\n\nPropiedades de la Función de Distribución\n\n\nDada una \\(F_{X}\\) una función de distribución de la v.a. \\(X\\) y denotamos por \\[F_{X}(x_0^{-})=\\displaystyle \\lim_{x\\to x_0^{-}} F_{X}(x),\\], entonces:\n\n\\(P(X=x)=F_{X}(x)-F_{X}(x^{-})\\).\n\\(P(a&lt; X&lt; b)=F_{X}(b^{-})-F_{X}(a)\\).\n\\(P(a\\leq X&lt; b)=F_{X}(b^{-})-F_{X}(a^{-})\\),\n\\(P(X&lt;a)=F_{X}(a^{-})\\),\n\\(P(a\\leq X\\leq b)=F_{X}(b)-F_{X}(a^{-})\\),\n\\(P(X\\geq a)=1-F_{X}(a^{-})\\).\n\n\n\nObtenemos más propiedades combinando las anteriores.\n\n\nPropiedades de la Función de Distribución\n\n\nSea \\(F_{X}\\) la función de distribución de una v.a. \\(X\\) entonces:\n\nSi \\(F_X\\) es continua en \\(x\\) se tiene que \\(P(X=x)=0\\). Así que si la v.a. es continua \\(P(X\\leq a)=P(X&lt; a)+P(X=a)=P(X&lt;a)\\) y propiedades similares.\nSea \\(X\\) una variable aleatoria discreta que con dominio \\(D_X\\) y que tiene por función de probabilidad \\(P_{X}(x)\\) entonces su función de distribución \\(F_{X}(x_0)\\) es \\[F_{X}(x_0)=\\sum_{x\\leq x_0} P_{X}(x),\\] donde \\(\\sum\\limits_{x\\leq x_0}\\) indica que sumamos todos los \\(x \\in D_X\\) tales que \\(x\\leq\nx_0.\\)\n\n\n\nDemostración:\nSi \\(X\\) es continua, \\[P(X=a)=F(a)-F(a^{-})=F(a)-F(a)=0\\] por lo tanto\n\\[P(X\\leq a)=P(X&lt;a)+P(X=a)= P(X&lt;a)+0= P(X&lt;a),\\]\nlo que demuestra la primera propiedad.\nPara demostrar la segunda basta hacer\n\\[\nF_{X}(x_0)= P(X\\leq x_0)=P\\left(\\bigcup_{x\\leq\nx_0; x\\in D_X} \\{x\\}\\right)= \\sum_{x\\leq x_0}P(X=x)= \\sum_{x\\leq x_0}P_{X}(x).\n\\]\nEl resto se deja al lector como ejercicio.\n\n\nEjemplo: dado (continuación)\n\n\nEn el experimento del dado se tiene que:\n\\[P_{X}(x)=\\left\\{\\begin{array}{ll} \\frac16, & \\mbox{si } x=1,2,3,4,5,6\\\\ 0, & \\mbox{en el resto de casos.}\\end{array}\\right.\\]\npor lo tanto\n\\[\\scriptsize{F_{X}(x)=P(X\\leq x)=\\left\\{\\begin{array}{ll}\n   0, & \\mbox{si } x&lt;1,\\\\\n   \\frac16, &\\mbox{si } 1\\leq x&lt;2,\\\\[1ex]\n   \\frac26, &\\mbox{si } 2\\leq x&lt;3,\\\\\n   \\frac36, &\\mbox{si } 3\\leq x&lt;4,\\\\\n   \\frac46, &\\mbox{si } 4\\leq x&lt;5,\\\\\n   \\frac56, &\\mbox{si } 5\\leq x&lt;6,\\\\\n   1, &\\mbox{si } 6\\leq x.\\end{array}\\right.}\\]\nCalculemos más detalladamente algún valor de \\(F_{X}\\), por ejemplo:\n\\[\\begin{eqnarray*}\nF_{X}(3.5) & = & P(X\\leq 3.5)=  P(\\{X=1\\}\\cup\\{X=2\\}\\cup \\{X=3\\})\\\\\n&=& P(\\{X=1\\})+P(\\{X=2\\})+P(\\{X=3\\})\\\\\n&=& \\frac16+\\frac16+\\frac16=\\frac36 =\\frac12,\n\\end{eqnarray*}\\]\no de otra forma\n\\[F_{X}(3.5)=\\sum_{x\\leq 3.5} P_X(x)=\\sum_{x=1}^3 P(X=x)=\\sum_{x=1}^3 \\frac16= 3 \\cdot\n   \\frac16=\\frac12.\n\\]\n\n\n\n\nPropiedad\n\n\nSea \\(X\\) una variable con función de distribución \\(F_{X}\\) entonces:\n\n\\(0\\leq F_{X}(x)\\leq 1\\) para todo \\(x\\),\nSi \\(x&lt;x'\\), entonces \\[F_{X}(x)\\leq F_{X}(x').\\] Es una función creciente,es decir, no necesariamente estrictamente creciente.\n\\(\\displaystyle \\lim_{x\\to -\\infty}F_{X}(x)=0\\) y \\(\\displaystyle \\lim_{x\\to +\\infty}F_{X}(x)=1.\\)\nEs continua por la derecha \\(\\displaystyle \\lim_{x\\to x_0^{+}}F_{X}(x)=F_{X}(x_0)\\).",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducción a las variables aleatorias</span>"
    ]
  },
  {
    "objectID": "p01_03_variables_aleatorias.html#valores-esperados-o-esperanza",
    "href": "p01_03_variables_aleatorias.html#valores-esperados-o-esperanza",
    "title": "3  Introducción a las variables aleatorias",
    "section": "3.5 Valores esperados o esperanza",
    "text": "3.5 Valores esperados o esperanza\nLos valores notables que vamos a definir a quí son la esperanza y la varianza de una variable aleatoria. Estro s valores resumen la variable aleatoria en un único valor. La esperanza es una medida de la tendencia central de la variable aleatoria y la varianza es una medida de la dispersión de la variable aleatoria; lo alejados que están los valores entre sí.\n\n3.5.1 Momentos de variables aleatorias discretas\n\nAl igual que en la estadística descriptiva se utilizan distintas medidas para resumir los valores centrales y para medir la dispersión de una muestra, podemos definir las correspondiente medidas para variables aleatorias.\nA estas medidas se les suele añadir el adjetivo poblacionales mientras que a las que provienen de la muestra se las adjetiva como muestrales.\n\nPor ejemplo podemos buscar un valor que resuma toda la variable. Este valor es el que “esperamos” que se resuma la v.a. o esperamos que las realizaciones de la v.a. queden cerca de él. Demos su definición formal.\n\n\nEsperanza de una variable aleatoria discreta\n\n\nEl valor esperado o esperanza (expected value en inglés) \\(E(X)\\) de una v.a. discreta \\(X\\), se define como\n\\[\nE(X)=\\sum_{x\\in X(\\Omega)} x\\cdot P_{X}(x).\n\\]\nEn ocasiones se denomina media (mean en inglés, mitjana en catalán) poblacional o simplemente media y muy frecuentemente se la denota \\(\\mu_{X}=E(X)\\) o simplemente \\(\\mu=E(X)\\).\n\n\n\n\n3.5.2 Interpretación de la media aritmética para v.a. discretas\n\n\nEjemplo: lanzamiento de un dado \\(n\\) veces\n\n\nSupongamos que lanzamos un dado \\(n\\) veces y obtenemos unas frecuencias absolutas \\(n_{i}\\) para el resultado \\(i\\) con \\(i=1,\\ldots,6\\). Sea \\(X\\) la v.a. que nos representa el valor de una tirada del dado.\nCalculemos la media aritmética (o media muestral) de los datos\n\\[\n\\overline{x}=\\frac{1\\cdot n_1+2\\cdot  n_2+3\\cdot  n_3+4\\cdot  n_4+5\\cdot  n_5+6 \\cdot\nn_6}{n}=\\sum_{x=1}^6 x \\cdot \\frac{n_{x}}{n}.\n\\]\nSi \\(n\\to \\infty\\) se tiene que \\(\\displaystyle\\lim_{n\\to \\infty} \\frac{n_{x}}{n}=P_{X}(x).\\) Por lo tanto \\(E(X)=\\displaystyle \\lim_{n\\to\\infty}\\sum_{x=1}^6x \\cdot \\frac{n_{x}}{n}.\\)\nLuego el valor esperado en una v.a. discreta puede entenderse como el valor promedio que tomaría una v.a. en un número grande de repeticiones.\n\n\n\n\nEjemplo: Erratas en un texto\n\n\nSea \\(X\\)= número de erratas en una página de un texto con dominio \\(D_X=\\{0,1,2\\}\\).\nResulta que\n\\[\\begin{align*}\nP(X=0)&= 0.42,\\ P(X=1)=0.4,\\ P(X=2)=0.18, \\mbox{ por lo tanto, }\\\\\nE(X)&= 0\\cdot 0.42+ 1\\cdot 0.4 + 2 \\cdot 0.18=0.76.\n\\end{align*}\\]\nElegida una página del texto al azar esperamos encontrar \\(0.76\\) errores por página.\nSupongamos que el editor nos paga \\(2\\) euros por cada página que encontremos con \\(1\\) error y \\(3\\) euros por cada página con dos errores (y nada por las páginas correctas) ¿Cuánto esperamos cobrar si analizamos una página?\n\n\n\n\nPropiedad esperanza de una función de una v.a. discreta\n\n\nSea \\(X\\) una v.a. discreta con función de probabilidad \\(P_{X}\\) y de distribución \\(F_{X}\\). Entonces el valor esperado de una función \\(g(x)\\) es:\n\\[E(g(X))=\\sum_{x}g(x) \\cdot  P_{X}(x).\\]\n\n\n\n\nPropiedades\n\n\nLa esperanza de una v.a. tiene las siguientes propiedades:\n\n\\(E(k)=k\\) para cualquier constante \\(k\\).\nSi \\(a\\leq X\\leq b\\) entonces \\(a\\leq E(X)\\leq b\\).\nSi \\(X\\) es una v.a. discreta que toma valores enteros no negativos entonces \\(E(X)=\\sum_{x=0}^{+\\infty}(1- F_X(x)).\\)\n\n\n\n\n\nEjercicio\n\n\nDemostrar las propiedades anteriores.\n\n\n\n\nEjemplo: paleta de colores aleatoria\n\n\nSupongamos que estamos sentados delante de nuestro ordenador con un amigo y le decimos que en dos minutos podemos programar una paleta para poner colores a unos gráficos.\nQueremos que la paleta tenga dos botones con las opciones color rojo y color azul. Como hemos programado a gran velocidad resulta que el programa tiene un error; cada vez que se abre la paleta los colores se colocan al azar (con igual probabilidad) en cada botón, así que no sabemos en qué color hemos de pinchar.\nAdemás, como nos sobraron \\(15\\) segundos para hacer el programa y pensando en la comodidad del usuario, la paleta se cierra después de haber seleccionado un color y hay que volverla a abrir de nuevo.\nLa pregunta es ¿cuál es el valor esperado del número de veces que hemos pinchar el botón de color azul antes de obtener este color?\nLlamemos \\(X\\) al número de veces que pinchamos en el botón azul (y nos sale rojo) hasta obtener el primer azul. La variable \\(X\\) toma valores en los enteros no negativos. Su función de probabilidad queda determinada por\n\\[\nP_X(x)=P(X=x)=P(\\stackrel{x \\mbox{ veces}}{\\overbrace{rojo, rojo,\\ldots,rojo},azul})\n=\\left(\\frac12\\right)^{x+1}.\n\\]\n\n\n\n\n3.5.3 Series geométricas\n\n\nSeries geométricas\n\n\n\nUna progresión geométrica de razón \\(r\\) es una sucesión de la forma\n\\[\nr^0, r^1,\\ldots,r^n,\\ldots.\n\\]\nLa serie geométrica es la suma de todos los valores de la progresión geométrica \\(\\displaystyle\\sum_{k=0}^{+\\infty} r^k\\).\nLas sumas parciales desde el término \\(n_0\\) al \\(n\\) de una progresión geométrica valen \\[\n\\sum_{k=n_0}^n r^k=\\frac{r^{n_0}- r^n r}{1-r}.\n\\]\nSi \\(|r|&lt;1\\) la serie geométrica es convergente y \\[\\sum_{k=0}^{+\\infty }\nr^k=\\frac1{1-r}\\].\nEn el caso en que se comience en \\(n_0\\) se tiene que \\[\\sum_{k=n_0}^{+\\infty} r^k=\\frac{r^{n_0}}{1-r}.\\]\n\n\n\n\n\nDerivadas series geométricas\n\n\nSi \\(|r|&lt;1\\) también son convergentes las derivadas, respecto de \\(r\\), de la serie geométrica y convergen a la derivada correspondiente. Así tenemos que\n\\[\\begin{eqnarray*}\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)'= & \\sum_{k=1}^{+\\infty}k\nr^{k-1}; \\qquad  \\left(\\frac1{1-r}\\right)'=\\frac1{(1-r)^2}\\\\\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)^{''}=&\\sum_{k=2}^{+\\infty}k (k-1)\nr^{k-2}  ;\\qquad  \\left(\\frac1{1-r}\\right)^{''}=\\frac2{(1-r)^3}\n\\end{eqnarray*}\\].\n\n\n\n\nEjemplo: paleta de colores (continuación)\n\n\nSi seguimos con el ejemplo de la paleta de colores, su esperanza es:\n\\[\\begin{align*}\nE(X)=&\\sum_{x=0}^{+\\infty} x\\cdot  P(X=x)=\\sum_{x=0}^{+\\infty} x\\cdot\n\\left(\\frac12\\right)^{x+1}\\\\ & =  \\left(\\frac12\\right)^2\\sum_{x=1}^{+\\infty} x\\cdot\n\\left(\\frac12\\right)^{x-1}=\\left(\\frac12\\right)^2\\cdot\n\\frac1{\\left(1-\\frac12\\right)^2}=1.\n\\end {align*}\\]\nAhora calculemos su función de distribución\n\\[\\begin{align*}\nF_X(x)=& P(X\\leq x)=\\sum_{k=0}^x P(X=k)=\\sum_{k=0}^x\n\\left(\\frac12\\right)^{k+1}  \\\\\n=& \\frac{\\frac12-\\frac12^{x+1}\\cdot\n\\frac12}{1-\\frac12}=1-\\left(\\frac12\\right)^{x+1}.\n\\end {align*}\\]\nComo la variable toma valores enteros positivos, podemos calcular su valor esperado de esta otra manera\n\\[E(X)=\\sum_{x=0}^{+\\infty} (1-F_X(x))=\\sum_{x=0}^{+\\infty}\\left(\\frac12\\right)^{x+1}=\\frac12\\cdot\n\\frac1{1-\\frac12}=1.\\]\nSe deja como ejercicio el calculo del valor esperado de la variable\n\\[\nY=\\mbox{número de intentos para conseguir el color azul.}\n\\]\n\n\n\n\nMomentos de orden \\(m\\)\n\n\nLlamaremos momento de orden \\(m\\) respecto al punto \\(C\\) a \\[E\\left((X-C)^m\\right).\\]\n\nCuando \\(C=0\\) los momentos reciben el nombre de momentos respecto al origen.\nCuando \\(C=E(X)\\) reciben el nombre de momentos centrales o respecto de la media. Luego la esperanza es el momento de orden \\(1\\) respecto al origen. Estos momentos son la versión poblacional de los momentos que vimos en el curso de estadística descriptiva, recibiendo estos último el nombre de momentos muestrales.\n\n\n\n\n\n3.5.4 Resumen de conceptos\n\nHemos descrito el comportamiento aleatorio de una v.a. discreta mediante sus funciones de probabilidad \\(P_{X}\\) y de distribución \\(F_{X}\\).\nTambién tenemos un valor central; el valor esperado \\(E(X)\\).\nComo medida básica nos queda definir una medida de lo lejos que están los datos del valor central \\(E(X)\\) una de estas medidas es la varianza de \\(X\\).\n\n\n\n3.5.5 Medidas de la variabilidad\nLas mediadas de la variabilidad son medidas que nos indican lo lejos que están entre sí.\nEn el caso de las v.a. se utilizan los momentos centrales para medir la variabilidad de los datos.\nExisten muchas otras medidasd de la variabilidad, pero en este tema nos centraremos en la varianza y la desviación típica.\n\n\nDefinición: Varianza\n\n\nSea \\(X\\) una v.a. Llamaremos varianza de \\(X\\) a\n\\[Var(X)=E((X-E(X))^2).\\]\nPor lo tanto, la varianza es el momento central de orden \\(2\\).\nDe forma frecuente se utiliza la notación \\[\\sigma_{X}^2=Var(X).\\]\nA la raíz cuadrada positiva de la varianza \\[\\sigma_{X}=+\\sqrt{Var(X)}.\\]\nse la denomina desviación típica o estándar de \\(X\\).\n\n\n\n\nPropiedades\n\n\n\nSi \\(X\\) es una v.a. discreta con función de probabilidad \\(P_X\\) su varianza es \\[\\sigma_{X}^2=Var(X)=E((X-E(X))^2)=\\sum_{x}(x-E(X))^2\\cdot  P_{X}(x).\\]\nSea \\(X\\) una v.a. \\[Var(X)=E(X^2)-(E(X))^2=\\sum_{x} x^2\\cdot  P_{X}(X)-(E(X))^2\\]\n\n\n\nDemostración El primer paratdo se deja como ejercicio.\nPara el segundo apartado, tenemos que:\n\\[\\begin{eqnarray*}\nVar(X)&= & \\sum_{x}(x-E(X))^2 P_{X}(x) = \\sum_{x}(x^2 -2\\cdot x\\cdot E(X)+(E(X)^2)\\cdot P_{X}(x)\\\\\n&=& \\sum_{x}x^2\\cdot P_{X}(x) -  E(X)\\sum_{x}2\\cdot x \\cdot P_{X}(x) + (E(X)^2)\\cdot\\sum_{x} P_{X}(x)\\\\\n&=& E(X^2)- 2 E(X)\\cdot E(X) + (E(X))^2=E(X^2)-(E(X))^2.\n\\end{eqnarray*}\\]\n\n\nEjemplo: número de errores (continuación)\n\n\nCalculemos en el ejemplo anterior la varianza del número de errores.\nRecordemos que:\n\\[\nP(X=0)=0.42,\\quad P(X=1)=0.4, \\quad P(X=2)=0.18,\n\\]\ny que\n\\[\nE(X)=0.76.\n\\]\nEntonces:\n\\[\nVar(X)=E(X^2)-(E(X))^2 = E(X^2)-(0.76)^2.\n\\]\nAhora necesitamos calcular\n\\[E(X^2)= 0^2 (0.41)+ 1^2 (0.4)+ 2^2 (0.18)=0.4+0.72=1.12\\] y por lo tanto\n\\[Var(X)= E(X^2)-(0.76)^2=1.12-0.5776=0.542\\] y \\[\\sqrt{Var(X)}=\\sqrt{0.542}\\]\nEn resumen \\(\\sigma_{X}^2=0.542\\) y \\(\\sigma_{X}=\\sqrt{0.542}\\)\n\n\n\n\nPropiedades de la varianza\n\n\n\n\\(Var(X)\\geq 0\\).\n\\(Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0\\).\nEl mínimo de \\(E((X-C)^2)\\) se alcanza cuando \\(C=E(X)\\) y es \\(Var(X)\\). Esta propiedad es una de las que hace útil a la varianza como medida de dispersión.\n\n\n\nEjercicio\nSe deja como ejercicio la demostración de estas propiedades.\n\n\n3.5.6 Transformaciones lineales.\n\n\nTransformación lineales\n\n\nUn cambio de variable lineal o transformación lineal de una v.a. \\(X\\) es otra v.a. \\(Y= a+ b\\cdot  X\\) donde \\(a,b\\in\\mathbb{R}\\).\n\n\n\n\nEsperanza de una transformación lineal\n\n\nSea \\(X\\) una v.a. con \\(E(X)=\\mu_{X}\\) y \\(Var(X)=\\sigma_{X}^2\\) y \\(a,b\\in\\mathbb{R}\\). Entonces si \\(Y=a+b\\cdot  X\\):\n\n\\(E(Y)=E(a + b X)=a+ b E(X)= a + b \\cdot \\mu_{X}\\).\n\\(Var(Y)=Var(a+bX)=b^2 Var(X)= b^2\\cdot  \\sigma_{X}^2\\)-\n\\(\\sigma_{Y}=\\sqrt{Var(Y)}=\\sqrt{b^2 Var(X)}=|b| \\cdot \\sigma_{X}\\)-\n\n\n\nDemostración\n\\[\\begin{eqnarray*}\nE(Y)&=& E(a+bX)=\\sum_{x}(a+b\\cdot x)\\cdot P_{X}(x)\\\\\n&=& a \\sum_{x} P_{X}(x) + b \\sum_{x} x\\cdot P_{X}(x)\\\\\n&=& a + b\\cdot E(X)=a + b\\cdot\\mu_{X}.\n\\end{eqnarray*}\\]\nLas demostración de las demás propiedades se dejan como ejercicio.\n\n\n3.5.7 Variables aleatorias continuas\nVeremos en esta secciçon las propiedades específicas d de las variables aleatorias continuas.\n\n\nDefinición Variable aleatoria Continua\n\n\n\nComo ya hemos dicho las variables aleatorias continuas toman valores en intervalos o áreas.\nLo más habitual es que estas variables tengan función de distribución continua y derivable (salvo a los más en una cantidad finita o numerable de puntos:-)).\nEn lo que sigue supondremos que la función de distribución de variables aleatorias continuas cumplen estas propiedades.\nNotemos que si \\(X\\) es una v.a. con función de distribución continua se tiene que \\(P(X=x_0)=F_X(x_0)-F(x_0^{-})=0\\). Por lo que no tiene sentido definir función de probabilidad.\n\n\n\n\n\nPropiedades de las variables aleatorias continuas\n\n\n\nEn general se tiene que \\(P(X&lt;x_0)=P(X\\leq x_0)\\).\nPor otra parte podemos utilizar una regla parecida del cociente entre casos favorables y casos posibles de Laplace pero en este caso el conteo se hace por la medida de los casos posibles partida por la medida de los casos favorables.\nVeamos un ejemplo de v.a. continua, que ampliaremos en el tema siguiente, en el que se utilizan todos estos conceptos.\n\n\n\n\n\nEjemplo: Distribución uniforme en \\([0,1]\\).\n\n\nSupongamos que lanzamos un dardo a una diana de radio \\(1\\), de forma que sea equiprobable cualquier distancia al centro.\n¡Cuidado! esto último no es equivalente que cualquier punto de la diana sea equiprobable.\nConsideremos la v.a. continua \\(X=\\) distancia al centro de la diana.\nSu función de distribución es\n\\[\nF_{X}(x)=\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\\nx, & \\mbox{si } 0&lt;x&lt;1,\\\\\n1, & \\mbox{si } x\\geq 1.\n\\end{array}\n\\right.\n\\]\nconsideremos\n\nC.F. longitud favorable que es \\(x-0\\),\nC.P. longitud posible que es \\(1-0\\),\n\nluego\n\\[P(X\\leq x)=\\frac{C.F.}{C.P.}=\\frac{x-0}{1-0}=x.\\]\nGráfica de la función de distribución uniforme\n\ncurve(punif(x,0,1),xlim=c(-1,2),col=\"blue\",\n      main=\"Función de distribución de una v.a. \\n\n      uniforme en el intervalo unidad.\")\n\n\n\n\n\n\n\n\n\n\n\n\nPropiedades de las variables aleatorias continuas\n\n\nEn las variables continuas los sucesos del tipo \\(\\{X\\leq x \\}\\) y \\(\\{X&lt; x \\}\\) tendrán la misma probabilidad, y otros tipos de sucesos similares también, algunas de estas propiedades se explicitan en la siguiente proposición.\nDada una v.a. continua \\(X\\) se tiene que:\n\n\\(P(X\\leq b)=P(X&lt;b)\\).\n\\(P(X&lt;b)=P(X&lt;a)+P(a&lt;X&lt;b)\\).\n\\(P(a&lt;X&lt;b)=P(X&lt;b)-P(X&lt;a)\\).\n\n\n\nDemostración:\nLa primera es evidente \\(P(X\\leq b)=P(X&lt;b)+P(X=b)=P(X&lt;b).\\)\nPara demostrar la segunda, tenemos\n\\[\\{X\\leq a\\}\\cap \\{a&lt;X&lt;b\\}=\\emptyset\\] \\[\\{X\\leq a\\}\\cup \\{a&lt;X&lt;b\\}=\\{X&lt;b\\},\\]\nentonces\n\\(P(X&lt; b)= P(\\{X\\leq a\\}\\cup \\{a&lt;X&lt;b\\}) = P(X\\leq a)+P(a&lt;X&lt;b)= P(X&lt; a)+P(a&lt;X&lt;b).\\)\nLa demostración de la tercera propiedad es similar a la segunda pero aplicando la primera. La dejamos como ejercicio.\nLas propiedades anteriores y combinaciones de ellas se pueden escribir utilizando la función de distribución de \\(X\\):\n\n\nPropiedades de la Función de Distribución\n\n\nDada una variable aleatoria continua se tiene que:\n\n\\(F_{X}(b)=F_{X}(a)+P(a&lt;X&lt;b)\\).\n\\(P(a&lt;X&lt;b)=F_{X}(b)-F_{X}(a)\\).\n\\(P(a\\leq X\\leq b)=F_{X}(b)-F_{X}(a)\\).\n\n\n\nDemostración\nSe deja la demostración como ejercicio; utilizar propiedades básicas del cáculo integral.\n\n\nEjemplo: diana (continuación)\n\n\nEn el ejemplo de la diana:\n\\[P(0.25&lt;X&lt;0.3)=F_{X}(0.3)-F_{X}(0.25)=0.3-0.25=0.05.\\]\n\n\n\n\nDefinición: Función de densidad\n\n\nUna función \\(f:\\mathbb{R}\\to\\mathbb{R}\\) es una función de densidad sobre \\(\\mathbb{R}\\) si cumple que\n\n\\(f_{X}(x)\\geq 0\\) para todo \\(x \\in\\mathbb{R}.\\)\n\\(f\\) es continua salvo a lo más en una cantidad finita de puntos sobre cada intervalo acotado de \\(\\mathbb{R}\\).\n\\(\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} f_{X}(x) dx=1.\\)\n\n\n\n\n\nDefinición: Función de distribución (acumulada) de una variable aleatoria\n\n\nSea \\(X\\) una v.a. con función de distribución \\(F_X\\). Sea \\(f:\\mathbb{R}\\to\\mathbb{R}\\) una función de densidad tal que\n\\[F_X(x)=\\displaystyle\\int_{-\\infty}^{x} f_X(t) dt.\\mbox{ para todo } x\\in\\mathbb{R},\\]\nEntonces \\(X\\) es una variable aleatoria continua y \\(f_X\\) es la densidad de la v.a. \\(X\\).\nEl conjunto \\(D_X=\\{x\\in\\mathbb{R}| f_x(x)&gt;0\\}\\) recibe el nombre de soporte o dominio de la variable aleatoria continua y se interpreta como su conjunto de resultados posibles.\n\n\n\n\nEjemplo: Diana continuación\n\n\nEn el ejemplo de la diana, la función \\(f\\) es una densidad\n\\[\nf_{X}(x)=\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\\n1, & \\mbox{si } 0 &lt; x &lt; 1,\\\\\n0, & \\mbox{si } 1\\leq x.\n\\end{array}\\right.\n\\]\nque es la densidad de \\(X\\), en efecto:\n\\[\nf_{X}(x)=\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\\n1, & \\mbox{si } 0 &lt; x &lt; 1,\\\\\n0, & \\mbox{si } 1\\leq x.\n\\end{array}\\right.\n\\]\n\nSi \\(x \\leq 0\\) entonces \\(\\displaystyle\\int_{-\\infty}^x f_X(t) dt = 0.\\)\nSi \\(0\\leq x\\leq 1\\) entonces \\(\\displaystyle\\int_{-\\infty}^x f_X(t) dt = \\int_0^x 1 dt = x.\\)\nSi \\(x\\geq 1\\) entonces \\(\\displaystyle\\int_{-\\infty}^x f_X(t) dt = \\int_0^1 1 dt = 1.\\)\n\nPor lo tanto, \\(F_X(x)=\\displaystyle\\int_{-\\infty}^x f_X(t) dt\\) para todo \\(x\\in\\mathbb{R}.\\)\n\ncurve(dunif(x,0,1),xlim=c(-0.5,1.5),col=\"blue\",\n      main=\"Densidad de la distribución uniforme en [0,1]\")\n\n\n\n\n\n\n\n\n\n\nLa utilidad de la función de densidad es que nos permite calcular diversas probabilidades.\n\n\nPropiedades de la función de densidad\n\n\n\nSea \\(X\\) una v.a. continua con función de distribución \\(F_X\\) y de densidad \\(f_X\\), entonces \\[\\begin{eqnarray*}\nP(a&lt; X&lt; b) &=&  P(a&lt;X\\leq b)= P(a\\leq X&lt; b)=\\\\\n& & P(a\\leq X\\leq b)= \\displaystyle\\int_{a}^b f_X(x) dx.\n\\end{eqnarray*}\\]\nSi \\(A\\) es un subconjunto adecuado de \\(\\mathbb{R}\\) entonces \\[P(X\\in A)=\\displaystyle\\int_{A} f(x) dx=\\displaystyle\\int_{A\\cap D_X} f(x) dx.\n\\]\n\n\n\n\n\nPropiedades de la función de distribución\n\n\n\\(X\\) una v.a. continua con función de distribución \\(F_X\\) y de densidad \\(f_X\\), entonces:\n\nSi \\(f_x\\) es continua en un punto \\(x\\), \\(F_X\\) es derivable en ese punto y \\(F_X'(x)=f_X(x).\\)\n\\(P(X=x)=0\\) para todo \\(x\\in\\mathbb{R}.\\)\n\n\n\n\n\nEjercicio\n\n\nComprobar estas propiedades en el ejemplo de la diana.\n\n\n\n\nEjemplo: tiempo ejecución de un proceso.\n\n\nSea \\(X=\\) tiempo de ejecución de un proceso. Se supone que \\(X\\) sigue una distribución uniforme en dos unidades de tiempo, si tarda más el proceso se cancela.\nCalculemos la función de densidad y de distribución de la v.a \\(X\\).\nEntonces\n\\[\nF_{X}(x)=P(X\\leq x)=\\frac{CF}{CP}=\\frac{x}2.\n\\]\nLuego su función de distribución es:\n\\[\nF_{X}(x)=\\left\\{\\begin{array}{ll}\n0, & \\mbox{si } x\\leq 0,\\\\\n\\frac{x}2 & \\mbox{si } 0&lt;x&lt;2,\\\\\n1, & \\mbox{si } 2\\leq x.\n\\end{array}\\right.\n\\]\nSu función de densidad por su lado es: \\[\nf_{X}(x)=F_{X}'(x)=\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x\\leq 0\\\\\n\\frac12 & \\mbox{si } 0&lt;x\\leq 2\\\\\n0 & \\mbox{si } 2\\leq x\n\\end{array}\\right.\n\\]\nEfectivamente\n\n\\(f_{X}(x)\\geq 0,\\) y tiene un conjunto finito de discontinuidades: en \\(0\\) y en \\(2\\)\n\\(F_X(x)=\\displaystyle\\int_{-\\infty}^x f_X(t) dt,\\) para todo \\(x\\in \\mathbb{R}\\) (Ejercicio: resolverlo gráficamente.)\n\\(\\displaystyle\\int_{-\\infty}^{+\\infty}f_{X}(x)dx=\n\\int_0^2\\frac12dx=\\left[\\frac{x}2\\right]_0^2\n=\\frac22-\\frac02=1.\\)\n\nCalcular la probabilidad de que uno de nuestros procesos tarde más de una unidad de tiempo en ser procesado. Calcular también la probabilidad de que dure entre \\(0.5\\) y \\(1.5\\) unidades de tiempo.\n\n\n\n\n3.5.8 Esperanza y varianza para variables aleatorias continuas\nLos mismos comentarios y definiciones que se dieron en la sección correspondiente del tema de estadística descriptiva son aplicables aquí.\nAsí que sólo daremos las definiciones, la forma de cálculo y algunos ejemplos.\nEn lo que sigue, salvo que diagamos lo contrario, \\(X\\) es una v.a. continua con función de densidad \\(f_{X}(x)\\)\n\n\nDefinición\n\n\nSea \\(X\\) unav.a. continua \\(X\\) con función de densidad \\(f_X\\) y de distribución \\(F_X\\). Su esperanza es: \\[E(X)=\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} x\\cdot f_{X}(x)dx.\\]\n\n\n\n\nPropiedad\n\n\nCon las mismas notaciones que el apartado anterior, se cumple que si \\(g(x)\\) es una función de la variable \\(X\\) entonces:\n\\[E(g(X))=\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} g(x)\\cdot f_{X}(x)dx.\\]\nSu varianza es:\n\\[\nVar(X)=\\sigma_{X}^2=E((X-\\mu_{X})^2)=\n\\displaystyle\\int\\limits_{-\\infty}^{+\\infty} (x-\\mu_{X})^2 \\cdot f_{X}(x)dx.\n\\]\nSu desviación típica es: \\[\\sigma_{X}=+\\sqrt{\\sigma_{X}^2}.\\]\n\n\n\n\nPropiedades\n\n\n\n\\(\\sigma_{X}^2\\geq 0\\).\n\\(Var(cte)=E(cte^2)-(E(cte))^2= cte^2 - cte^2=0\\).\n\\(\\displaystyle Var(x)=E(X^2)-\\mu_{X}^2=\\int_{-\\infty}^{+\\infty}x^2\\cdot  f_{X}(x)dx - \\mu_{X}^2.\\)\nEl mínimo de \\(E((X-C)^2)\\) se alcanza cuando \\(C=E(X)\\) y es \\(Var(X)\\).\n\n\n\n\n\nEjemplo: diana (continuación)\n\n\nCalcular \\(\\mu_{X}\\) y \\(\\sigma_{X}^2\\) en el ejemplo de la diana\nResultado \\[\\mu_{X}=\\frac12,\\] \\[E(X^2)=\\frac13,\\] \\[Var(X)=\\frac1{12}.\\]\n\n\n\n\n3.5.9 Esperanza de trans. lineales de v.a. continuas\n\n\nPropiedad\n\n\nSea \\(X\\) una v.a. continua con \\(E(X)=\\mu_{X}\\) y \\(Var(X)=\\sigma_{X}^2\\) sea \\(Y=a+b X\\), donde \\(a,b\\in\\mathbb{R}\\), es una nueva v.a. continua obtenida mediante una transformación lineal de \\(X\\). Se verifican las mismas propiedades que en el caso discreto:\n\n\\(E(Y)=E(a+b\\cdot  X)=a+b\\cdot  E(X)\\).\n\\(Var(Y)=Var(a+b\\cdot  X)=b^2 \\cdot  Var(X)\\).\n\\(\\sigma_{Y}=|b|\\cdot  \\sigma_{X}\\).\n\\(Z=\\frac{X-\\mu_{X}}{\\sigma_{X}}\\) es una transformación lineal de \\(X\\) de forma que \\[E(Z)=0 \\mbox{ y } Var(Z)=1\\]\n\n\n\n\n\nEjemplo\n\n\nEn una empresa de venta de vinos por internet, sea \\(X=\\) número de litros de vino del país vendidos en un año. Supongamos que sabemos que \\(E(X)=10000\\) y que \\(Var(X)=100.\\) Supongamos que los gastos fijos de distribución son 50.000 € y el beneficio por litro es de 10 € por botella. Definimos \\(T=10\\cdot X-50000,\\) que será el beneficio después de gastos.\nEntonces la esperanza del beneficio es \\[E(T)=10 E(X)-50000 = 50000,\\] y \\[Var(T)=10^2 Var(X)= 10000.\\]\n\n\n\n\n3.5.10 Transformaciones de variables aleatorias\nMuchas variables aleatorias son funciones de otras v.a. En lo que sigue resumiremos diversas técnicas para dada una v.a. \\(X\\) y una transformación \\(Y=h(X)\\) encontrar \\(F_{Y}\\) a partir de \\(F_{X}\\).\n\n\nTransformaciones de v.a. discretas\n\n\nSea \\(X\\) una v.a. discreta con \\(X(\\Omega)=\\{x_1,x_2,\\ldots,x_{n},..\\}\\) y sea \\(h:\\mathbb{R}\\to\\mathbb{R}\\) una aplicación. Entonces \\(Y=h(X)\\) es también una v.a. discreta. Además si \\(P_X\\) y \\(F_{X}\\) son las funciones de probabilidad y de distribución de \\(X\\) entonces\n\n\\(\\displaystyle P_{Y}(y)=\\sum_{x_{i}|h(x_{i})=y}P_X(x_{i}).\\)\n\\(\\displaystyle F_{Y}(y)=\\sum_{x_{i}|h(x_{i})\\leq y} P_X(x_{i}).\\)\n\n\n\nDesafortunadamente para variables no discretas el resultado no es tan sencillo como el anterior, pues la transformación de, por ejemplo, una v.a. continua puede ser continua, discreta, mixta,\\(\\ldots\\)\n\n\nTransformaciones de v.a. discretas\n\n\nSea \\(X\\) una v.a. continua cuya función de densidad es \\(f_{X}\\). Sea \\(h:\\mathbb{R}\\to\\mathbb{R}\\), una aplicación estrictamente monótona y derivable, por lo tanto \\(h'(x)\\not=0\\) para todo \\(x\\in\\mathbb{R}\\). Sea \\(Y=h(X)\\) la transformación de \\(X\\) por \\(h\\). Entonces \\(Y\\) es una v.a. continua con función de densidad\n\\[f_{Y}(y)=\\left.\\frac{f_{X}(x)}\n{\\left|h'(x)\\right|}\\right|_{x=h^{-1}(y)}\\] donde \\(h^{-1}\\) es la función inversa de \\(h\\).\n\n\nTenemos algunas cpropiedades relacionadas con el cálculo difrencial e integral que nos permiten calcular la densidad de una transformación de una v.a. continua.\n\n\nDensidad de una transformación de una v.a. continua\n\n\nSea \\(X\\) una v.a. continua cuya función de densidad es \\(f_{X}\\). Sea \\[h:\\mathbb{R}\\to\\mathbb{R}\\] una aplicación, no necesariamente monótona tal que :\n\nsea derivable con derivada no nula\nla ecuación \\(h(x)=y\\) tiene un número finito de soluciones \\(x_1,x_2,..,x_{n}\\)\n\nentonces:\n\\[\n\\displaystyle f_{Y}(y)=\\left.\\sum_{k=1}^{n} \\frac{f_{X}(x)}\n{\\left|h'(x)\\right|}\\right|_{x=x_{k}}.\n\\]\n\n\nCuando no podamos aplicar las propiedades anteriores intentaremos calcular primero la función de distribución de la transformación y luego su densidad.\nNotemos que en general si \\(Y=g(X)\\) es una v.a. transformación de la v.a. \\(X\\) entonces\n\\[\nF_{Y}(y)=P(Y\\leq y)=P(g(X)\\leq y).\n\\]\n\n\nTransformaciones monótonas\n\n\nPor ejemplo, si \\(g\\) es estrictamente creciente y continua,\n\\[\nF_{Y}(y)=P(g(X)\\leq y)=P(X\\leq g^{-1}(y))=F_{X}(g^{-1}(y)),\n\\]\ny si \\(g\\) es estrictamente decreciente y continua, \\[\nF_{Y}(y)=P(g(X)\\leq y)=P(X\\geq g^{-1}(y))=1-F_{X}(g^{-1}(y)).\n\\]\n\n\n\n\n3.5.11 Desigualdades de Markov y de Chebychev\n\nEn esta sección distintas desigualdades que acotan determinadas probabilidades de una variable aleatoria.\nEstas desigualdades sirven en algunos casos para acotar probabilidades de determinados sucesos.\nTambién son útiles desde el punto de vista teórico, por ejemplo para justificar que la varianza es una medida de la dispersión de los datos.\n\n\n\nDesigualdad de Markov\n\n\nSea \\(X\\) una v.a. positiva con \\(E(X)\\) finita. Entonces\n\\[P(X\\geq a)\\leq \\frac{E(X)}{a}\\mbox{ para todo }a&gt;0.\\]\n\n\nDemostración:\nSi \\(X\\) es continua y solo toma valores positivos\n\\[\\begin{eqnarray*}\nE(X) &=& \\int_{-\\infty}^{+\\infty} x\\cdot f_{X}(x) dx=  \\int_0^{+\\infty} x\\cdot f_{X}(x) dx\\\\\n&=&\n\\int_0^{a} x\\cdot f_{X}(x) dx +\\int_{a}^{+\\infty} x\\cdot f_{X}(x) dx \\\\ &\\geq&   \\int_{a}^{+\\infty} x\\cdot\nf_{X}(x) dx \\geq a \\int_{a}^{+\\infty}\nf_{X}(x) dx \\\\ &=& a \\cdot  P(X\\geq a),\n\\end{eqnarray*}\\]\nde donde se sigue que\n\\[P(X\\geq a)\\leq \\frac{E(X)}{a}.\\]\nLo que demuestra la propiedad.\n\n\nCorolario: Desigualdad de Markov\n\n\nSea \\(X\\) una v.a. con \\(E(X)\\) finita entonces para todo \\(a&gt;0\\)\n\\[P(|X|\\geq a )\\leq \\frac{E(|X|)}{a}.\\]\n\n\n\n\nEjercicio\n\n\nDemuestra el corolario anterior a partir de la desigualdad de Markov.\n\n\nLa desigualdad de Chebychev también se denomina de Chebyshov y en inglés Chebyshev.\n\n\nDesigualdad de Chebychev\n\n\nSea \\(X\\) una v.a.con \\(E(X)=\\mu\\) y \\(Var(X)=\\sigma^2\\) entonces para todo \\(a&gt;0\\),\n\\[P(|X-\\mu|\\geq a)\\leq \\frac{\\sigma^2}{a^2}.\\]\n\n\nDemostración\nApliquemos la consecuencia de la desigualdad de Markov a la v.a. no negativa\n\\[Y^2=(X-\\mu)^2\\]\nentonces\n\\[\nP(Y^2\\geq a^2) \\leq\n\\frac{E(Y^2)}{a^2}=\\frac{E((X-\\mu)^2)}{a^2}\n= \\frac{Var(X)}{a^2}=\\frac{\\sigma^2}{a^2}\n.\n\\]\nPor otra parte\n\\[\nP(Y^2\\geq a^2)=P(|Y|\\geq a)= P(|X-\\mu|\\geq a),\n\\]\nhecho que, junto con la desigualdad anterior, demuestra el resultado.\n\n3.5.11.1 Utilidad básica de la desigualdad de Chebychev\nSupongamos que \\(X\\) es una v.a. con \\(Var(X)=0\\), entonces, aplicando la desigualdad anterior\n\\[P(|X-E(X)|\\geq a )=0\\mbox{ para todo }a&gt;0,\\]\nlo que implica que\n\\[P(X=E(X))=1,\\]\nPor lo que la probabilidad de que \\(X\\) sea constantemente \\(E(X)\\) es 1, hecho que nos confirma la utilidad de la varianza como una medida de la dispersión de los datos.\n\n\nEjemplo: tiempo de respuesta\n\n\nSe sabe que el tiempo de respuesta medio y la desviación típica de un sistema multiusuario son 15 y 3 unidades de tiempo respectivamente. Entonces:\n\\[\nP(|X-15|\\geq 5)\\leq \\frac9{25}=0.36.\n\\]\nSi substituimos \\(a\\) por \\(a\\cdot \\sigma\\) en la desigualdad de Chebychev, nos queda:\n\\[\nP(|X-\\mu|\\geq a\\cdot \\sigma)\\leq\n\\frac{\\sigma^2}{(a\\cdot \\sigma)^2}=\\frac1{a^2},\n\\]\nque es otra manera de expresar la desigualdad de Chebychev.\n\n\nLa desigualdad de Chebychev también se puede escribir de al menos dos maneras más:\n\\[\nP(\\mu-a\\leq X\\leq \\mu+a)\\geq 1-\\frac{\\sigma^2}{a^2},\n\\]\ny tomado como \\(a=k\\cdot \\sigma\\),\n\\[\nP(\\mu-k\\cdot \\sigma\\leq X\\leq \\mu+ k \\cdot \\sigma)\\geq 1-\\frac1{k^2}.\n\\]\nTomando la segunda expresión que hemos visto para la desigualdad de Chebychev para distintos valores de \\(k&gt;0\\) obtenemos la siguiente tabla:\nPor ejemplo para \\(k=2\\), esta desigualdad se puede interpretar como que, dada una v.a. \\(X\\) con cualquier distribución que tenga \\(E(X)\\) y \\(Var(X)\\) finitos, la probabilidad de que un valor se aleje de la media \\(\\mu\\) más de \\(a=2\\) desviaciones típicas es menor o igual que \\(0.25\\).\n\nEs decir sólo el 25% de los valores estarán alejados de la media más de \\(2\\cdot \\sigma\\)\n\n¡Sea cual sea la distribución de la v.a.!",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introducción a las variables aleatorias</span>"
    ]
  },
  {
    "objectID": "p01_04_1_Notables_discretas.html",
    "href": "p01_04_1_Notables_discretas.html",
    "title": "4  Distribuciones Notables I",
    "section": "",
    "text": "4.1 Introducción\nEn este tema estudiaremos diversos tipos de experimentos que son muy frecuentes y algunas de las variables aleatorias asociadas a ellos.\nEstas variables reciben distintos nombres que aplicaremos sin distinción al tipo de población del experimento a la variable o a su función de probabilidad, densidad o distribución.\nEmpezaremos con las variables aleatorias discretas que se presentan con frecuencia ya que están relacionadas con situaciones muy comunes como el número de caras en varios lanzamiento de una moneda, el número de veces que una maquina funciona hasta que se estropea, el numero de clientes en una cola,…",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuciones Notables I</span>"
    ]
  },
  {
    "objectID": "p01_04_1_Notables_discretas.html#distribución-bernoulli",
    "href": "p01_04_1_Notables_discretas.html#distribución-bernoulli",
    "title": "4  Distribuciones Notables I",
    "section": "4.2 Distribución Bernoulli",
    "text": "4.2 Distribución Bernoulli\n\n\nDefinición\n\n\nDistribución Bernoulli * Consideremos un experimento con dos resultados posibles éxito (E) y fracaso (F). El espacio de sucesos será \\(\\Omega=\\{E,F\\}\\). * Supongamos que la probabilidad de éxito es \\(P(E)=p\\), y naturalmente \\(P(F)=1-p=q\\) con \\(0&lt;p&lt;1\\). * Consideremos la aplicación\n\\[\nX:\\Omega=\\{E,F\\}\\to \\mathbb{R}\n\\]\ndefinida por\n\\[\nX(E)=1\\mbox{, }X(F)=0.\n\\]\n\n\nSu función de probabilidad es\n\\[\nP_{X}(x)=\n\\left\\{\n\\begin{array}{ll} 1-p=q & \\mbox{si } x=0\\\\\np & \\mbox{si } x=1\\\\\n0 & \\mbox{en cualquier otro caso}\n\\end{array}\n\\right..\n\\]\nSu función de distribución es\n\\[\nF_{X}(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n1-p=q & \\mbox{si } 0\\leq x &lt;1\\\\\n1 & \\mbox{si } 1\\leq x \\\\\n\\end{array}\n\\right..\n\\]\nBajo estas condiciones diremos que \\(X\\) es una v.a. Bernoulli o que sigue una ley de distribución de probabilidad Bernoulli de parámetro \\(p\\).\nLo denotaremos por \\[X\\sim Ber(p)\\mbox{ o también } X\\sim B(1,p).\\] A este tipo de experimentos (éxito/fracaso)se les denomina experimentos Bernoulli.\nSu descubridor fue un científico suizo Jacob Bernoulli, uno más de la conocida familia de científicos suizos Bernoulli.\nSu valor esperado es\n\\[E(X)=\\displaystyle\\sum_{x=0}^1 x\\cdot P(X=x)= 0\\cdot(1-p)+1\\cdot p=p.\\]\nCalculemos también \\(E(X^2)\\)\n\\[E(X^2)=\\displaystyle\\sum_{x=0}^1 x^2\\cdot P(X=x)= 0^2\\cdot(1-p)+1^2\\cdot p=p.\\]\nSu varianza es\n\\[Var(X)=E(X^2)-\\left(E(X)\\right)^2=p-p^2=p\\cdot (1-p)=p\\cdot q.\\]\nSu desviación típica es\n\\[\n\\sqrt{Var(X)}=\\sqrt{p \\cdot (1-p)}.\n\\]\n\n4.2.1 Resumen v.a con distribución Bernoulli\n\n\nEjemplo\n\n\nVeamos los cálculos básicos \\(Ber(p=0.25)\\) en R.\n\ndbinom(0,size=1,prob=0.25)\n\n[1] 0.75\n\ndbinom(1,size=1,prob=0.25)\n\n[1] 0.25\n\nrbinom(n=20,size = 1,prob=0.25)\n\n [1] 1 1 0 0 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0\n\n\nEl siguiente código dibuja las función de probabilidad y la de distribución de una \\(Ber(p=0.25)\\)\n\npar(mfrow=c(1,2))\nplot(x=c(0,1),y=dbinom(c(0,1),size=1,prob=0.25),\n     ylim=c(0,1),xlim=c(-1,2),xlab=\"x\",\n     main=\"Función de probabilidad\\n Ber(p=0.25)\")\nlines(x=c(0,0,1,1),y=c(0,0.75,0,0.25), type = \"h\", lty = 2,col=\"blue\")\ncurve(pbinom(x,size=1,prob=0.25),\n      xlim=c(-1,2),col=\"blue\",\n      main=\"Función de distribución\\n Ber(p=0.25)\")\npar(mfrow=c(1,1))",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuciones Notables I</span>"
    ]
  },
  {
    "objectID": "p01_04_1_Notables_discretas.html#distribución-binomial",
    "href": "p01_04_1_Notables_discretas.html#distribución-binomial",
    "title": "4  Distribuciones Notables I",
    "section": "4.3 Distribución binomial",
    "text": "4.3 Distribución binomial\n\n\nDistribución binomial\n\n\nSi repetimos \\(n\\) veces de forma independiente un experimento Bernoulli de parámetro \\(p\\).\nEl espacio muestral \\(\\Omega\\) estará formado por cadenas de \\(E\\)’s y \\(F\\)’s de longitud \\(n\\) Consideremos la v.a.\n\\[X(\\overbrace{EFFF\\ldots EEF}^{n})=\\mbox{número de éxitos en la cadena}.\\] A la variable aleatoria anterior se le conoce como distribución binomial de parámetros \\(n\\) y \\(p\\), y lo denotaremos por \\(X\\sim B(n,p).\\)\nCalculemos su función de probabilidad:\n\\[\nP_{X}(x)=\\left\\{\n\\begin{array}{ll}\n{n\\choose x}\\cdot  p^x \\cdot(1-p)^{n-x} &\\mbox{ si } x=0,1,\\ldots,n\\\\\n0  & \\mbox{ en otro caso}\n\\end{array}\\right..\n\\]\nSu función de distribución no tiene una fórmula cerrada. Hay que acumular la función de probabilidad:\n\\[\\begin{eqnarray*}\nF_{X}(x)=P(X\\leq x) & = & \\sum_{i=0}^x P_X(i)\\\\\n& = &\n\\left\\{\n\\begin{array}{ll}\n0 & \\mbox{ si } x&lt;0\\\\\\displaystyle\n\\sum_{i=0}^k {n\\choose i}\\cdot  p^i \\cdot (1-p)^{n-i} & \\mbox{ si }\n\\left\\{\n  \\begin{array}{l}\n  k\\leq x&lt; k+1\\\\\n  k=0,1,\\ldots,n.\n  \\end{array}\n\\right.\\\\\n1 & \\mbox{ si } n\\leq x\n\\end{array}\n\\right.\n\\end{eqnarray*}\\]\n\n\nLos números binomiales calculan el número de equipos de baloncesto distintos que (\\(k=5\\) jugadores) se pueden hacer con 6 jugadores (\\(n=6\\)).\nEs decir cuántas maneras distintas hay para elegir (choose) 5 jugadores en un conjunto de 6 jugadores. Todo el mundo diría ¡¡¡6!!!. Efectivamente con R es\n\nchoose(6,5)\n\n[1] 6\n\n\nCon 10 jugadores el número de equipos de 5 distintos es bastante más grande\n\nchoose(10,5)\n\n[1] 252\n\n\nY, por ejemplo, con un equipo de fútbol profesional que tiene en plantilla 22 jugadores (quitando los guardametas) se pueden formar ¡¡nada menos que!!\n\nchoose(22,10)\n\n[1] 646646\n\n\nun bonito número capicúa que nos da el número de equipos distintos que se pueden formar.\nObviamente se tiene que una v.a. Bernoulli es una binomial con \\(n=1\\)\n\\[B(1,p)=Ber(p).\\]\n\n\nEjercicio\n\n\nCalculad las funciones de distribución de una binomial \\(B(n=1,p=0.3)\\) y comprobar que coinciden con las distribuciones de una \\(Ber(p=0.3)\\).\n\n\n\n4.3.1 Observaciones sobre la distribución binomial\n\nLa probabilidad de fracaso se suele denotar con \\(q=1-p\\), sin ningún aviso adicional, con el fin de acortar y agilizar la escritura de las fórmulas.\nSu función de distribución no tienen una formula general, hay que calcularla con una función de R o python… En el siglo pasado se tabulaban en los libros de papel :-).\nEn el material adicional os pondremos unas tablas de esta distribución para distintos valores de \\(n\\) y \\(p\\) para que disfrutéis de tan ancestral método de cálculo.\nCualquier paquete estadístico, hoja de cálculo dispone de funciones para el cálculo de estas probabilidades, así que el uso de las tablas queda totalmente anticuado.\n\nLa esperanza de una \\(B(n,p)\\) es\n\\[E(X)=\\displaystyle\\sum_{k=0}^n k \\cdot  {n \\choose k }\\cdot p^k\\cdot q^{n-k} = n\\cdot p.\\]\nLa esperanza de \\(X^2\\) es\n\\[\\begin{eqnarray*}\nE(X^2)&=& \\displaystyle\\sum_{k=0}^n k^2 \\cdot  {n \\choose k }\\cdot p^k\\cdot q^{n-k}\\\\\n&=& n\\cdot p\\cdot q+(n\\cdot p)^2.\n\\end{eqnarray*}\\]\nLa varianza de una \\(B(n,p)\\) es\n\\[Var(X)=E(X^2)-\\left(E(X)\\right)^2=n\\cdot p \\cdot q=n\\cdot p\\cdot (1-p).\\]\nSu desviación típica es\n\\[\\sqrt{n\\cdot p\\cdot q}=\\sqrt{n\\cdot p\\cdot (1-p)}.\\]\nEn temas posteriores veremos una forma sencilla del cálculo de la esperanza y varianza de una \\(B(n,p)\\) como las suma de \\(n\\) v.a. \\(Ber(p)\\) independientes.\n\n\nEjemplo\n\n\nJustificar de forma intuitiva que si \\(X_i\\) con \\(i=1,2,\\ldots, n\\) son v.a. \\(Ber(p)\\) independientes entonces \\(X=\\displaystyle\\sum_{i=1}^n X_i\\) sigue una distribución \\(B(n,p).\\)\n\n\n\n\n4.3.2 Resumen v.a con distribución binomial \\(B(n,p)\\)\n\n\n\n4.3.3 La distribución binomial con R\nVeamos los cálculos básicos con funciones de R para una v.a \\(X\\) con distribución binomial \\(B(n=10,p=0.25)\\).\nSi queremos calcular con R algún valor de la función de distribución como por ejemplo \\(F_X(0)=P(X\\leq 0)\\), tenemos que hacer:\n\npbinom(0,size=10,prob=0.25)\n\n[1] 0.05631351\n\n\ny si queremos por ejemplo \\(F_X(4)=P(X\\leq 4)\\), tenemos que hacer:\n\npbinom(4,size=10,prob=0.25)\n\n[1] 0.9218731\n\n\nSin embargo, si queremos calcular algún valor de la función de probabilidad como por ejemplo \\(P(X=0)\\), tenemos que hacer:\n\ndbinom(0,size=10,prob=0.25)\n\n[1] 0.05631351\n\n\no por ejemplo para \\(P(X=4)\\):\n\ndbinom(4,size=10,prob=0.25)\n\n[1] 0.145998\n\n\n\n\n4.3.4 Generación de muestras aleatorias con R\nGeneraremos una muestra aleatoria de 100 valores de una población con distribución \\(B(20,0.5)\\)\n\nset.seed(2019)\nrbinom(100,size = 20,prob=0.5)\n\n  [1] 12 11  9 11  6  6 12  5  7 11 12 11  8  8 11 11  7 11  9 10  9 10 14\n [24]  8  8  5 11 14 11 10 11  5 12  8  6  7  9 10  5 12 11  9 12 11 12 10\n [47] 13 13  8  8  9  7  6  9 10  9 16 13  6  6  8  8 11  9 12 15  9  7 12\n [70] 11  9  8  9  8 11 15  7 10  9 12  6 13 14  8 10  8 10 11 11  9 10 11\n [93] 12  8 10 12  9 13  9 13\n\n\nQue corresponde a los resultados de repetir 100 veces el experimento de lanzar una moneda 20 veces y contar el número de caras.\n\n\n4.3.5 Cálculos distribución binomial con python\nVeamos los cálculos básicos con funciones de python para una v.a \\(X\\) con distribución binomial \\(B(n=10,p=0.25)\\).\nPrimero importamos la función binom de la librería scipy.stat\n\nfrom scipy.stats import binom\n\nEn general en el paquete scipy, la función de probabilidad se invocará con el método pmf, la de distribución con el método cdf mientras que una muestra aleatoria que siga esta distribución con el método rvs. En todos ellos aparecerá siempre el parámetro loc que se utiliza para desplazar el dominio de la variable aleatoria. Por ejemplo, en este caso\n\nbinom.pmf(k, n, p, loc) =  binom.pmf(k - loc, n, p)\n\n\n\n4.3.6 Cálculos distribución binomial con python\nPara calcular los valores de la función de distribución como por ejemplo \\(F_X(0)=P(X\\leq 0)\\) y \\(F_X(4)=P(X\\leq 4)\\) utilizamos la función cdf\n\nbinom.cdf(0,n=10,p=0.25)\n\n0.056313514709472684\n\nbinom.cdf(4,n=10,p=0.25)\n\n0.9218730926513672\n\n\nNotemos que al no indicar el valor de loc, se le asume que toma el valor 0.\nPara calcular los valores de la función de probabilidad \\(P(X=0)\\) y \\(P(X=4)\\) utilizamos la función pmf:\n\nbinom.pmf(0,n=10,p=0.25)\n\n0.056313514709472656\n\nbinom.pmf(4,n=10,p=0.25)\n\n0.14599800109863284\n\n\nNotemos que al no indicar el valor de loc, se le asume que toma el valor 0.\nSi queremos generar una muestras aleatorias que siga una distribución binomial, podemos usar la función rvs. En este caso, generaremos una muestra aleatoria de 100 valores de una población \\(B(20,0.5)\\)\n\nbinom.rvs(n=20,p=0.25,size = 100)\n\narray([ 5,  7,  8,  4,  3,  6,  4,  4,  4,  4,  7,  3,  6,  6,  6,  5,  6,\n        6,  6,  4,  6,  5,  5,  6,  3,  8,  9,  6,  7,  4,  4,  8,  8,  3,\n        7,  3,  2,  6,  7, 10,  5,  7,  7,  6,  6,  7,  4,  9,  5,  9,  3,\n        6,  6,  6,  5,  5,  6,  5,  7,  4,  6,  4,  4,  4,  7,  4,  4,  5,\n        5,  8,  6,  4,  3,  4,  7,  4,  6,  5, 10,  5,  3,  5,  5,  6,  7,\n        3,  6,  4,  5,  7,  5,  3,  7,  5,  6,  3,  0,  6,  5,  6],\n      dtype=int64)\n\n\nAlerta Notemos que la secuencia aleatoria generada no es la misma que con R. De hecho, si volvemos a ejecutar esta función obtendremos una muestra aleatoria distinta.\n\nbinom.rvs(n=20,p=0.25,size = 100)\n\narray([ 6,  5,  8,  5,  5,  6,  9,  6,  7,  7,  6,  6,  6,  3, 10,  3,  5,\n        2,  2,  5, 10,  5,  3,  5,  7,  2,  7,  5,  5,  1,  3,  5,  6,  2,\n        4,  7,  5,  3,  6,  5,  6,  4,  6,  4,  3,  5,  3,  6, 12,  6,  3,\n        3,  4,  3,  4,  6,  8,  7,  8,  5,  6,  9,  6,  5,  5,  6,  3,  7,\n        5,  5,  6,  6,  2,  1,  6,  6, 10,  8,  4,  2,  6,  6,  5,  4,  4,\n        8,  8,  6,  3,  5,  5,  7,  5,  4,  8,  5,  4,  5,  7,  3],\n      dtype=int64)\n\n\nVeamos algunos cálculos básicos con funciones de python para la binomial \\(B(n=10,p=0.25)\\).\n\nbinom.cdf(5,n=10,p=0.25)\n\n0.9802722930908203\n\nbinom.pmf(1,n=10,p=0.25)\n\n0.1877117156982421\n\nbinom.rvs(n=20,p=0.25,size=10)\n\narray([6, 3, 3, 4, 5, 6, 4, 8, 5, 8], dtype=int64)\n\n\n\n\n4.3.7 Gráficas de la distribución binomial con R\nEl siguiente código de R dibuja las función de probabilidad y la de distribución de una \\(B(n=10,p=0.25)\\)\n\npar(mfrow=c(1,2))\naux=rep(0,22)\naux[seq(2,22,2)]=dbinom(c(0:10),size=10,prob=0.25)\nplot(x=c(0:10),y=dbinom(c(0:10),size=10,prob=0.25),\n  ylim=c(0,1),xlim=c(-1,11),xlab=\"x\",\n  main=\"Función de probabilidad\\n B(n=10,p=0.25)\")\nlines(x=rep(0:10,each=2),y=aux, type = \"h\", lty = 2,col=\"blue\")\ncurve(pbinom(x,size=10,prob=0.25),\n  xlim=c(-1,11),col=\"blue\",\n  main=\"Función de distribución\\n B(n=10,p=0.25)\")\npar(mfrow=c(1,1))\n\nEl siguiente código de R dibuja las función de probabilidad y la de distribución de una \\(B(n=10,p=0.25)\\)\n\n\n\n\n\n\n\n\n\n\n\nEjercicio\n\n\nBuscad en la documentación de python cómo se dibuja la función de probabilidad y de distribución de una binomial y recread los gráficos anteriores.\nPista: Necesitaremos investigar más librerías:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\n\nn, p = 10, 0.25\nx = np.arange(binom.ppf(0.01, n, p),binom.ppf(0.99, n, p))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\nax.vlines(x, 0, binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\nax.plot(x, binom.cdf(x, n, p), 'bo', ms=8, label='binom pmf')\nax.vlines(x, 0, binom.cdf(x, n, p), colors='b', lw=5, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion Binomial')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo: Número de bolas rojas extraídas de una urna con reposición\n\n\nTenemos una urna con \\(100\\) bolas de las cuales 40 son rojas y 60 son blancas. Extraemos al azar una bola, anotamos su color y la devolvemos a (reponemos en) la urna.\nSupongamos que repetimos este proceso \\(n=10\\) reponiendo en cada ocasión la bola extraída.\nConsideremos la variable aleatoria \\(X\\) como el número de bolas rojas extraídas (con reposición) en \\(n=10\\) repeticiones del mismo experimento de Bernoulli.\nBajo estas condiciones repetimos \\(n=10\\) veces el mismo experimento de Bernoulli con probabilidad de éxito (sacar bola roja) \\[P(Roja)=P(Éxito)=p=\\frac{40}{100}=0.4.\\]\nAsí que la variable \\(X\\) que es el número de bolas rojas extraídas de la urna (con reposición) en \\(n=10\\) ocasiones sigue una ley binomial \\(B(n=10,p=0.4).\\)\nNos preguntamos:\n\n¿Cuál es la probabilidad de que saquemos exactamente \\(4\\) bolas rojas?\n¿Cuál es la probabilidad de que saquemos al menos \\(4\\) bolas rojas?\n¿Cuál es la probabilidad de que saquemos menos de \\(3\\) bolas rojas?\n¿Cuál es el valor esperado del número de bolas rojas?\n¿Cuál es la desviación típica del número de bolas rojas?\n\nSolución 1. ¿Cuál es la probabilidad de que saquemos exactamente \\(4\\) rojas?\nUtilizando la función de probabilidad, tenemos que: \\[\\begin{eqnarray*}\nP(X=4)&=&{10\\choose 4}\\cdot 0.4^4\\cdot (1-0.4)^{10-4}\n= \\frac{10!}{(10-4)!\\cdot 4!}\\cdot 0.4^4\\cdot 0.6^6\\\\\n&=& \\frac{7\\cdot 8\\cdot 9\\cdot 10}{1\\cdot 2\\cdot 3\\cdot 4}\\cdot 0.4^4\\cdot 0.6^6=0.2508227.\n\\end{eqnarray*}\\]\nCon R\n\ndbinom(4,size=10,prob = 0.4)\n\n[1] 0.2508227\n\n\nSolución 2. ¿Cuál es la probabilidad de que saquemos al menos \\(4\\) bolas rojas?\nLa probabilidad de sacar al menos 4 rojas se expresa como\n\\(P(X \\geq  4)=1-P(X&lt;4)=1-P(X\\leq 3):\\)\n\\[\\begin{eqnarray*}\nP(x\\leq 3)&=& P(X=0)+P(X=1)+P(X=2)+P(X=3)\\\\\n&=&\n{10\\choose 0}\\cdot 0.4^0\\cdot (1-0.4)^{10-0}+ {10\\choose 1}\\cdot 0.4^1\\cdot (1-0.4)^{10-1}\\\\\n&+&{10\\choose 2}\\cdot 0.4^2\\cdot (1-0.4)^{10-2}+ {10\\choose 3}\\cdot 0.4^3\\cdot (1-0.4)^{10-3}\\\\\n&=&0.3822806.\n\\end{eqnarray*}\\]\nCon R\n\npbinom(3,10,0.4)\n\n[1] 0.3822806\n\n\nAsí que\n\\[P(X \\geq 4 )=1-P(X&lt; 4)=1-P(X\\leq 3)=1-0.3822806=0.6177194.\\]\nOtra manera usando R sería:\n\n1-pbinom(3,10,0.4)\n\n[1] 0.6177194\n\n\nAunque en estos casos el parámetro lower.tail = FALSE es sin duda nuestra mejor opción:\n\npbinom(3,10,0.4,lower.tail = FALSE)\n\n[1] 0.6177194\n\n\nSolución 3. ¿Cuál es la probabilidad de que saquemos menos de \\(3\\) bolas rojas?\n\\[\\begin{eqnarray*}\nP(X&lt; 3)&=& P(X\\leq 2)=  P(X=0)+P(X=1)+P(X=2)\\\\\n&=&\n{10\\choose 0}\\cdot 0.4^0\\cdot (1-0.4)^{10-0}+ {10\\choose 1}\\cdot 0.4^1\\cdot (1-0.4)^{10-1}\\\\\n&&+\n{10\\choose 2}\\cdot 0.4^2\\cdot (1-0.4)^{10-2}\\\\\n&=&0.1672898.\n\\end{eqnarray*}\\]\nCon código R:\n\ndbinom(0,10,0.4)+dbinom(1,10,0.4)+dbinom(2,10,0.4)\n\n[1] 0.1672898\n\npbinom(2,10,0.4)\n\n[1] 0.1672898\n\n\nSolución 4. ¿Cuál es el valor esperado del número de bolas rojas?\nComo \\(X\\) es una \\(B(n=10,p=0.4)\\) sabemos que\n\\[E(X)=n\\cdot p = 10\\cdot 0.4=4.\\]\nAunque en python tenemos la función stats que nos lo calcula directamente:\n\nprint(\"E(X) = {m}\".format(m=binom.stats(n = 10, p = 0.4, moments='m')))\n\nSolución 5. ¿Cuál es la desviación típica del número de bolas rojas?\nLa varianza es: \\[\nVar(X)=n\\cdot p \\cdot(1-p)=10\\cdot 0.4\\cdot 0.6=2.4.\n\\]\nPor lo tanto la desviación típica es\n\\[\\sqrt{Var(X)}=\\sqrt{2.4}= 1.5491933.\\]\nAunque en python tenemos la función stats que nos lo calcula directamente:\n\nprint(\"Var(X) = {v}\".format(v=binom.stats(n = 10, p = 0.4, moments='v')))\n\n\n4.4 Distribución geométrica\nTodos hemos jugado a, por ejemplo, tirar una moneda hasta que obtengamos la primera cara.\nO también tirar una pelota a una canasta de baloncesto hasta obtener la primera canasta.\nDesde otro punto de vista también podemos intentar modelar el número de veces que accionamos una interruptor y la bombilla se ilumina hasta que falla.\nO también el número de veces que un cajero automático nos da dinero hasta que falla.\nLa modelización de este tipo de problemas se consigue con la llamada distribución geométrica.\n\n\nDefinición: Distribución geométrica\n\n\n\nRepitamos un experimento Bernoulli, de parámetro \\(p\\), de forma independiente hasta obtener el primer éxito.\nSea \\(X\\) la v.a. que cuenta el número de fracasos antes del primer éxito. Por ejemplo que hayamos tenido \\(x\\) fracasos será una cadena de \\(x\\) fracasos culminada con un éxito. Más concretamente\n\n\\[P(\\overbrace{FFF\\ldots F}^{x}E)=P(F)^{x}\\cdot P(E)=(1-p)^{x}\\cdot p=q^{x}\\cdot p.\\]\n\n\nAsí su función de probabilidad es\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}\n(1-p)^{x}\\cdot p & \\mbox{ si } x=0,1,2,\\ldots\\\\\n0 &\\mbox{ en otro caso}\n\\end{array}\\right..\n\\]\nLa v.a. definida anteriormente diremos que sigue una distribución geométrica de parámetro \\(p\\) La denotaremos por \\(Ge(p)\\). Su dominio es \\(D_X=\\{0,1,2,\\ldots\\}\\).\n\n\nEjemplo\n\n\nCalculemos P(\\(X\\leq 3\\)).\nPor la propiedad de la probabilidad del suceso complementario tenemos que\n\\[\nP(X\\leq 3 )=1-P(X&gt; 3)=1-P(X\\geq 4)\n\\]\nEfectivamente, el complementario del evento \\(X\\leq 3\\) nos dice que hemos fracasado más de tres veces hasta conseguir el primer éxito, es decir, hemos fracasado 4 o más veces. Podemos simbolizar dicho evento de la forma siguiente: \\[\n\\{X&gt;3\\}=\\{X\\geq 4\\}= \\{FFFF\\}\n\\]\nAhora, al ser los intentos independientes, tenemos que:\n\\[\\begin{eqnarray*}\nP(X&gt;3) & = & P(\\{FFFF\\})= P(F)\\cdot P(F)\\cdot P(F)\\cdot P(F)\\\\\n&=& (1-p)\\cdot (1-p)\\cdot (1-p)\\cdot (1-p)= (1-p)^{3+1}\\\\\n&=&(1-p)^{4}.\n\\end{eqnarray*}\\]\nEl valor de la función de distribución de \\(X\\) en \\(x=3\\) será, pues: \\[F_X(3)=P(X\\leq 3)=1-P(X&gt;3)=1-(1-p)^{3+1}.\\] Generalizando el resultado anterior a cualquier entero positivo \\(k=0,1,2,\\ldots\\), tenemos: \\[F_X(k)=P(X\\leq k)=1-(1-p)^{k+1},\\mbox{ si } k=0,1,2,\\ldots\\]\n\n\nEn general, tendremos que la función de distribución de una v.a.a \\(Ge(p)\\) es :\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\\begin{array}{ll}\n0, & \\mbox{ si } x&lt;0,\\\\\n1- (1-p),  & \\mbox{ si } k=0\\leq x &lt;1,\\\\\n1- (1-p)^2, & \\mbox{ si } k=1\\leq x &lt;2,\\\\\n1- (1-p)^3, & \\mbox{ si } k=2\\leq x &lt;3,\\\\\n1- (1-p)^{k+1}, & \\mbox{ si } \\left\\{ \\begin{array}{l}k\\leq x&lt; k+1,\\\\\\mbox{para } k=0,1,2,\\ldots\\end{array}\n    \\right.\n\\end{array}\n\\right..\n\\]\n\n4.4.1 Función de distribución geométrica\nDe forma más compacta, tendremos que \\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\\begin{array}{ll}\n0, & \\mbox{ si } x&lt;0,\\\\\n1- (1-p)^{k+1}, & \\mbox{ si } \\left\\{ \\begin{array}{l}k\\leq x&lt; k+1,\\\\\\mbox{para } k=0,1,2,\\ldots\\end{array}\n\\right.\n\\end{array}\n\\right..\n\\]\nNotemos que el límite de la función de distribución es: \\[\n\\displaystyle\\lim_{k\\to +\\infty } F_X(k)=\\lim_{k\\to +\\infty } 1-(1-p)^{k+1}=\n1,\n\\] ya que \\(0&lt;1-p&lt;1\\).\nRecordemos del tema de variables aleatorias que\n\n\nPropiedades\n\n\n\nSi \\(|r|&lt;1\\) también son convergentes las derivadas, respecto de \\(r\\), de la serie geométrica y convergen a la derivada correspondiente. Así tenemos que\n\n\\[\n\\begin{array}{rlrl}\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)'&= \\sum_{k=1}^{+\\infty}k\\cdot\nr^{k-1} &= \\left(\\frac1{1-r}\\right)'=\\frac1{(1-r)^2}\\\\\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)^{''}&= \\sum_{k=2}^{+\\infty}k \\cdot(k-1)\\cdot\nr^{k-2}&=\\left(\\frac1{1-r}\\right)^{''}=\\frac2{(1-r)^3}\n\\end{array}\n\\]\n\n\nVamos a calcular el valor esperado de una \\(Geo(p)\\) Recordemos que \\(P(X=x)=(1-p)^x\\cdot p\\) si \\(x=0,1,2,\\ldots\\) y aplicado la fórmula anterior con \\(r=1-p\\)\n\\[\\begin{eqnarray*}\nE(X)&=&\\sum_{x=0}^{+\\infty} x\\cdot P_x(x)=\\sum_{x=0}^{+\\infty} x\\cdot (1-p)^x\\cdot p\\\\\n&=& p\\cdot (1-p) \\cdot \\sum_{x=1}^{+\\infty} x\\cdot (1-p)^{x-1}\\\\\n&=& p\\cdot (1-p)\\cdot \\frac{1}{(1-(1-p))^2}=p\\cdot (1-p)\\cdot \\frac{1}{p^2}=\\frac{1-p}{p}\n\\end{eqnarray*}\\]\nAhora caclulamos \\(E(X^2)\\)\n\\[\\begin{eqnarray*}\nE(X^2)&=&\\sum_{x=0}^{+\\infty} x^2\\cdot P_X(x)=\\sum_{x=1}^{+\\infty} x^2\\cdot (1-p)^x\\cdot p\\\\\n&=&\n\\sum_{x=1}^{+\\infty} (x\\cdot (x-1)+x)\\cdot (1-p)^{x}\\cdot p\\\\\n&=&\n\\sum_{x=1}^{+\\infty} x\\cdot (x-1)\\cdot (1-p)^{x}\\cdot p+\\sum_{x=1}^{+\\infty} x \\cdot (1-p)^{x}\\cdot p\\\\\n&=&\n(1-p)^{2}\\cdot p\\cdot \\sum_{x=2}^{+\\infty} x\\cdot (x-1)\\cdot (1-p)^{x-2}\\\\\n&  +&   (1-p)\\cdot p\\sum_{x=1}^{+\\infty} x \\cdot (1-p)^{x-1} = \\ldots\n\\end{eqnarray*}\\].\n\\[\\begin{eqnarray*}\nE(X^2)&=&\\ldots\\\\\n&=&\n(1-p)^{2}\\cdot p\\cdot \\sum_{x=2}^{+\\infty} x\\cdot (x-1)\\cdot (1-p)^{x-2}\\\\\n&  +&   (1-p)\\cdot p\\sum_{x=1}^{+\\infty} x \\cdot (1-p)^{x-1}\\\\\n&=&\np\\cdot (1-p)^2 \\frac{2}{(1-(1-p))^3}+  (1-p)\\cdot p \\frac{1}{(1-(1-p))^2}\\\\\n&=&\np\\cdot (1-p)^2 \\frac{2}{p^3}+  (1-p)\\cdot p \\frac{1}{p^2}\\\\\n&=&\\frac{2\\cdot (1-p)^2}{p^2}+\\frac{1-p}{p}.\n\\end{eqnarray*}\\]\nAhora varianza de una v.a. \\(Ge(p)\\) es\n\\[\\begin{eqnarray*}\nVar(X)&=&E(X^2)-E(X)^2=\\frac{2\\cdot (1-p)^2}{p^2}+\\frac{1-p}{p}-\\left(\\frac{1-p}{p}\\right)^2\\\\\n&=&\n\\frac{2\\cdot (1-p)^2+p\\cdot(1-p)-(1-p)^2}{p^2}=\\frac{(1-p)^2+p\\cdot(1-p)}{p^2}\\\\\n&=&\n\\frac{1-2\\cdot p + p^2+p-p^2}{p^2}\\\\\n&=& \\frac{1-p}{p^2}.\n\\end{eqnarray*}\\]\nY su desviación típica será\n\\[\\sqrt{Var(X)}=\\sqrt{\\frac{1-p}{p^2}}.\\]\n\n\n4.4.2 Resumen distribución geométrica \\(Ge(p)\\) empezando en 0\n\n\n\n4.4.3 La variable geométrica que cuenta los intentos para obtener el primer éxito.\n\nSupongamos que sólo estamos interesados en el número de intentos para obtener el primer éxito.\nSi definimos \\(Y\\)= número de intentos para obtener el primer éxito. Entonces \\(Y=X+1\\) donde \\(X\\sim Ge(p)\\).\nSu dominio es \\(D_Y=\\{1,2,\\ldots\\}\\)\nLa media se incrementa en un intento debido al éxito \\(E(Y)=E(X+1)=E(X)+1=\\frac{1-p}{p}+1=\\frac1{p}\\).\nLa varianza es la misma \\(Var(Y)=Var(X+1)=Var(X)=\\frac{1-p}{p^2}\\).\n\n\n\n4.4.4 Resumen distribución geométrica \\(Ge(p)\\) empezando en \\(1\\).\n\n\n\n4.4.5 Propiedad de la falta de memoria\n\n\nPropiedad de la falta de memoria\n\n\nSea \\(X\\) una v.a. discreta con dominio \\(D_X=\\{0,1,2,\\ldots\\}\\), con \\(P(X=0)=p\\).\nEntonces \\(X\\) sigue una ley \\(Ge(p)\\) si, y sólo si,\n\\[\nP\\left(X&gt; k+j\\big| X\\geq j\\right)=P(X&gt; k)\n\\] para todo \\(k,j=0,1,2,3\\ldots\\).\n\n\nDemostración\nSi \\(X\\) es geométrica entonces el lado derecho de la igualdad es\n\\[\nP(X&gt;k)=1-P(X\\leq k)=1-\\left(1-(1-p)^{k+1}\\right)=(1-p)^{k+1},\n\\] y el lado de izquierdo es\n{ \\[\\begin{eqnarray*}\nP\\left(X&gt; k+j\\big| X\\geq j\\right)&=&\\frac{P\\left(\\{X&gt; k+j\\}\\cap \\{X\\geq j\\} \\right)}{P\\left(X\\geq j\\right)}=\n\\frac{P\\left(X&gt;k+j \\right)}{P\\left(X\\geq j \\right)} = \\frac{1-P(X\\leq k+j)}{1-P(X\\leq j-1)}\\\\\n&=&  \\frac{1-(1-(1-p)^{k+j+1})}{1-(1-(1-p)^{j-1+1})} =\\frac{(1-p)^{k+j+1}}{(1-p)^{j}} = (1-p)^{k+1},\n\\end{eqnarray*}\\] } \nlo que demuestra la igualdad.\nPara demostrar el recíproco, tomemos \\(j=1\\) y \\(k\\geq 0\\). Entonces, por la propiedad de la pérdida de memoria: \\[\nP\\left(X&gt; k+1\\big| X\\geq 1\\right)=P(X&gt; k)\n\\] Como \\(P(X=0)=p\\), tenemos que \\(P(X \\geq 1 )=1-P(X&lt;1)=1-P(X=0)=1-p\\).\nCombinado las igualdades, tenemos que:\n\\[\nP\\left(X&gt; k+1\\big| X\\geq 1\\right)=\\frac{P(X&gt;k+1, X\\geq 1)}{P(X\\geq 1)}=\\frac{P(X&gt;k+1)}{P(X\\geq 1)}=P(X&gt;k).\n\\] Así podemos poner que\n\\[\\begin{eqnarray*}\nP(X&gt;k+1)&=&P(X\\geq 1)\\cdot P(X&gt;k)=\\left(1-P(X&lt;1)\\right)\\cdot P(X&gt;k)\\\\\n&=&\\left(1-P(X=0)\\right)\\cdot P(X&gt;k)=(1-p)\\cdot P(X&gt;k).\n\\end{eqnarray*}\\]\nEs decir en general tenemos que\n\\[\nP(X&gt;k+1)=(1-p)\\cdot P(X&gt;k)\n\\] Del mismo modo para \\(j=2\\)\n\\[\n\\scriptsize{P(X&gt;k+2)=(1-p)\\cdot P(X&gt;k+1)}\n\\]\nRestando la primera igualdad de la última obtenemos.\n\\[\n\\scriptsize{P(X&gt;k+1)-P(X&gt;k+2)=(1-p)\\cdot P(X&gt;k)-(1-p)\\cdot P(X&gt;k+1)}\n\\]\nde donde operando en cada lado de la igualdad obtenemos la recurrencia\n\\[\n\\scriptsize{[1-P(X\\leq k+1)]-[1-P(X\\leq k+2)]=(1-p)\\cdot [P(X&gt;k)-P(X&gt;k+1)]}\n\\]\nAhora operando \\[\nP(X\\leq k+2)-P(X\\leq k+1)=(1-p)\\cdot[1-P(X\\leq k)-\\left(1-P(X\\leq k+1)\\right)]\n\\] \\[\nP(X=k+2)=(1-p)\\cdot[P(X\\leq k+1)-P(X\\leq k)]\n\\] \\[\nP(X=k+2)=(1-p)\\cdot P(X=k+1)\n\\]\nDe forma similar obtenemos\n\\[\nP(X=k+1)=(1-p)\\cdot P(X=k)\n\\] Utilizando la recurrencia anterior, podemos calcular todas las probabilidades \\(P(X=k)\\) a partir de la \\(P(X=0)=p\\): \\[\n\\scriptsize{\n\\begin{array}{rl}\nP(X=0)&= p,\\\\\nP(X=1)&=P(X=0+1)= (1-p)\\cdot P(X=0) =(1-p)\\cdot  p,\\\\\nP(X=2)&=P(X=1+1)= (1-p)\\cdot P(X=1)=(1-p)\\cdot (1-p)\\cdot p=(1-p)^2\\cdot p,\\\\\n\\vdots &    \\vdots \\\\\nP(X=k)&=P(X=(k-1)+1)= (1-p)\\cdot P(X=k-1)=(1-p)\\cdot (1-p)^{k-1}\\cdot p=(1-p)^{k}\\cdot p,\n\\end{array}\n}\n\\] lo que demuestra el recíproco, es decir, que \\(X\\) es \\(Geom(p)\\).\n\n\nObservación: Interpretación de la propiedad\n\n\nLa propiedad de la falta de memoria \\[\nP(X&gt; k+j\\big|X \\geq j)=P(X &gt; k),\n\\]\nsignifica que, aunque ya llevemos al menos \\(j\\) fracasos, la probabilidad de que fracasemos \\(k\\) veces más no disminuye, es la misma que era cuando empezamos el experimento.\nA este efecto se le suele etiquetar con la frase el experimento carece de memoria o es un experimento sin memoria (Memoryless Property).\n\n\nUn ejemplo muy sencillo nos aclarará el alcance de esta propiedad\n\n\nEjemplo:La llave que abre la puerta\n\n\nTenemos un llavero con 10 llaves, solo una de ellas abre una puerta. Cada vez que probamos una llave y falla olvidamos que llave hemos probado. ¿Cuál es la probabilidad de que si ya lo hemos intentado 5 veces necesitemos más de 4 intentos adicionales para abrir la puerta?\nTomemos \\(k=4,j=5\\), aplicando la propiedad de la falta de memoria\n\\[\nP(X&gt; 4+5/X \\geq 5)=P(X &gt; 4)\n\\]\nDespués de 5 fracasos no estamos “más cerca” de abrir la puerta. La propiedad de la falta de memoria nos dice que en después de cada intento es como si empezásemos de nuevo a abrir la puerta. Tras 5 fracasos la probabilidad de que fallemos más de 4 veces más es la misma que cuando lo intentamos la primera vez.\n¿Cuál es el número esperado de fracasos hasta abrir la puerta?\n\\[\nE(X)=\\frac{1-p}{p}=\\frac{1-\\frac{1}{10}}{\\frac{1}{10}}=\\frac{\\frac{9}{10}}{\\frac{1}{10}}=9.\n\\]\nLa varianza es\n\\[\nVar(X)=\\frac{1-p}{p^2}=\\frac{1-\\frac{1}{10}}{\\left(\\frac{1}{10}\\right)^2}=\\frac{\\frac{9}{10}}{\\frac{1}{100}}=\n90.\n\\]\nLa desviación típica es \\(\\sqrt{90}=9.486833.\\)\n\n\n\n\nEjemplo: partidos hasta que el Barça gana al Madrid\n\n\nEjemplo: partidos hasta que el Barça gana al Madrid\nLos partidos Real Madrid vs FC Barcelona de la liga española se suelen denominar El Clásico, sean en el Bernabeu (estadio del Real Madrid) o en el Camp Nou (estadio del Barça)\nSea \\(X\\) la variable que cuenta el número de veces consecutivas que en un partido de fútbol de la liga el Barça no gana al Madrid sea en el Camp Nou o el Bernabeu.\nNuestra amiga Aina es muy culé (hincha del Barça) y quiere averiguar cuántos partidos consecutivos de El Clásico tiene que ver hasta ver ganar al Barça por primera vez.\nLe interesa estimar cuánto le va a costar este capricho. Tendrá que comprar las entradas y pagar los viajes de Barcelona a Madrid.\nEn datos historicos de El clásico en la wikipedia están los datos hasta el 3 de marzo de 2019: se han jugado en total 178 Clásicos donde el Real Madrid ganó en 72 ocasiones, el Barça, en 72 y empataron 34 veces.\nNos hacemos las siguientes preguntas:\n\nSi Aina solo tiene dinero para ir a ver 3 partidos, ¿cuál es la probabilidad de no ver ganar al Barça en al menos tres partidos consecutivos?\n¿Cuántos partidos se tienen que jugar de media para ver ganar al Barça por primera vez?\n\nCon los datos anteriores, podemos estimar que la probabilidad de que el Barça gane un clásico cualquiera es:\n\\[P(\\mbox{Barça})=\\frac{72}{178}=0.4045.\\]\nPor tanto, podemos modelar la variable \\(X\\), que cuenta el número de veces consecutivas que en un partido de fútbol de la liga el Barça no gana al Madrid, con una ley geométrica empezando en cero con probabilidad de éxito \\(p=P(\\mbox{Barça})=\\frac{72}{178}\\),\n\\[X=Ge\\left(p=\\frac{72}{178}=0.4045\\right)\\]\nAsí que lo que nos pregunta Aina es la siguiente probabilidad\n\\[P(X\\geq 3)=1-P(X\\leq 2)=1-\\left(1-\\frac{72}{178}\\right)^{2+1}=0.7888.\\]\nAsí que Aina tiene una probabilidad del \\(78.88\\%\\) de no ver ganar al Barça en al menos 3 partidos antes de ver uno en el sí que gane.\nPara responder a la segunda pregunta, usando que la distribución de \\(X\\) es:\n\\[X=Ge\\left(p=\\frac{72}{178}=0.4045\\right)\\]\nentonces\n\\[E(X)=\\frac{1-p}{p}=\\frac{1-0.4045}{0.4045}=1.4722\\]\ny\n\\[Var(X)=\\frac{1-p}{p^2}=\\frac{1-0.4045}{0.4045^2}=3.6397\\]\nLa desviación típica es \\[\\sqrt{3.6397}=1.9078.\\]\n\n4.4.6 Cálculos de la distribución geométrica con R\nVeamos los cálculos básicos con R para la distribución geométrica \\(Ge(p=0.25)\\). R implementa la geométrica que cuenta el número de fracasos.\n\\(P(X=0)=(1-0.25)^0\\cdot 0.25^1=0.25\\)\n\ndgeom(0,prob=0.25)\n\n[1] 0.25\n\n\n\\(P(X\\leq 0)=1- (1-0.25)^{0+1}=1-0.75=0.25\\)\n\npgeom(0,prob=0.25)\n\n[1] 0.25\n\n\n\\(P(X\\leq 4)=1-(1-0.25)^{4+1}=1-0.75=1-0.75^5=0.7626953.\\)\n\npgeom(4,prob=0.25)\n\n[1] 0.7626953\n\n\nUna muestra aleatoria de tamaño 25 de una \\(Ge(0.25)\\)\n\nrgeom(n=25,prob=0.25)\n\n [1]  5  4  1  6 10  0  0 10  7  0  6  2  1  3  0  2  5  0  0  5  5  3  3\n[24]  2  2\n\n\n\n\n4.4.7 Gráficas con R\n\n  par(mfrow=c(1,2))\n  p=0.25\n  n=30\n  aux=rep(0,(n+1)*2)\n  aux[seq(2,(n+1)*2,2)]=dgeom(c(0:n),prob=p)\n  plot(x=c(0:n),y=dgeom(c(0:n),prob=p),\n       ylim=c(0,1),xlim=c(-1,n+1),xlab=\"x\",\n       main=paste0(c(\"Función de probabilidad\\n Ge(p=\",p,\")\"),collapse = \"\"))\n  lines(x=rep(0:n,each=2),y=aux, type = \"h\", lty = 2,col=\"blue\")\n  curve(pgeom(x,prob=p),\n        xlim=c(-1,n+1),col=\"blue\",\n        main=paste0(c(\"Función de distribución\\n Ge(p=\",p,\")\"),collapse = \"\"))\n\n\n\n\n\n\n\n  par(mfrow=c(1,1))\n\n\n\n4.4.8 Cálculos de la distribución geométrica con python\nVeamos los cálculos básicos con python para la distribución geométrica \\(Ge(p=0.25)\\). scipy.stats implementa la distribución geométrica que cuenta el número intentos así que empieza en 1\nCargamos la función de la librería\n\nfrom scipy.stats import geom\n\nLa función de probabilidad es geom.pmf(x,p,loc=0)=geom.pmf(x,p) es un geométrica que cuenta el número de intentos para obtener el primer éxito el valor por defecto del último parámetro es loc=0.\nSi queremos la que cuenta el número de fracasos para obtener el primer éxito (la geométrica que empieza en 0) tenemos que usar geom.pmf(x,p,loc=-1).\nEs decir geom.pmf(x,p,loc=-1)=geom.pmf(x-1,p,loc=0)\nVeamos pues los cálculos para la \\(Ge(p)\\) que empieza en \\(0\\).\n\\(P(X=0)=(1-0.25)^0\\cdot 0.25^1=0.25\\)\n\ngeom.pmf(0,p=0.25,loc=-1)\n\n\\(P(X\\leq 0)=1- (1-0.25)^{0+1}=1-0.75=0.25\\)\n\ngeom.cdf(0,p=0.25,loc=-1)\n\n\\(P(X\\leq 4)=1-(1-0.25)^{4+1}=1-0.75=1-0.75^5=0.7626953.\\)\n\ngeom.cdf(4,p=0.25,loc=-1)\n\nUna muestra aleatoria de tamaño 25 de una \\(Ge(0.25)\\)\n\ngeom.rvs(p=0.25, size=20, loc=-1)\n\nEjercicio\nQué probabilidades son las que calcula el siguiente código y qué tipo de variables geométricas son?\n\ngeom.cdf(range(5),p=0.3,loc=0)\ngeom.cdf(range(5),p=0.3,loc=-1)\n\nCon python también podemos calcular directamente algunos parámetros asociado a una función de distribución predefinida\n\ngeom.stats(p=0.25, loc=0, moments='mv')\ngeom.stats(p=0.25, loc=-1, moments='mv')\n\nEjercicio\nComprobad que las medias y las varianzas calculadas en el código anterior, corresponden a una \\(Ge(p=0.3)\\) empezando en \\(1\\) y a una \\(Ge(p=0.3)\\) empezando en \\(0\\).\n¿Son las varianzas siempre iguales?\n\n\n4.4.9 Gráficos con python\n\np = 0.25\nx = np.arange(geom.ppf(0.01, p),geom.ppf(0.99, p))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, geom.pmf(x, p), 'bo', ms=5, label='geom pmf')\nax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\nax.plot(x, geom.cdf(x, p), 'bo', ms=5, label='geom pmf')\nax.vlines(x, 0, geom.cdf(x, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion Geometrica')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n4.5 Distribución binomial negativa\n\n\nEl problema de la puerta con dos cerraduras\n\n\nSupongamos que disponemos de 10 llaves distintas y tenemos que abrir una puerta con dos cerraduras.\nComenzamos por la primera cerradura, de tal forma que cada vez olvidamos qué llave hemos probado.\nUna vez abierta la primera cerradura probamos de igual forma con la segunda hasta que también la abrimos.\nSea \\(X=\\) la v.a. que cuenta el número de fracasos hasta abrir la puerta.\nAcertar una llave de la puerta es un experimento Bernoulli con probabilidad de éxito \\(p=0.1\\). Lo repetiremos hasta obtener 2 éxitos.\n\n\n\n\nDistribución binomial negativa\n\n\nEn general tendremos un experimento de Bernoulli con probabilidad de éxito \\(0&lt;p&lt;1\\) tal que:\n\nRepetimos el experimento hasta obtener el \\(n\\)-ésimo éxito ¡¡abrir la maldita puerta!!.\nSea \\(X\\) la v.a. que cuenta el número fallos hasta abrir la puerta, es decir, hasta conseguir el n-ésimo éxito. Notemos que no contamos los éxitos, solo contamos los fracasos.\n\nSi representamos como es habitual un suceso como una cadena de F’s y E’s, para \\(n=2\\), algunos sucesos elementales serán: \\[\\small{\\{EE,FEE,EFE, FFEE,FEFE,EFFE,FFFEE,FFEFE,FEFFE,EFFFE\\}.}\\]\nCalculemos algunas probabilidades para \\(n=2\\): \\[\n\\small{\n\\begin{array}{rl}\nP(X=0) & =P(\\{EE\\})=p^2, \\\\\nP(X=1) & =P(\\{FEE,EFE\\})=2\\cdot (1-p)\\cdot p^2, \\\\\nP(X=2) & =P(\\{FFEE,FEFE,EFFE\\})=3\\cdot (1-p)^2\\cdot p^2, \\\\\nP(X=3) & =P(\\{FFFEE,FFEFE,FEFFE,EFFFE\\})=4\\cdot (1-p)^3\\cdot p^2.\n\\end{array}\n}\n\\]\n\n\nEn general su función de probabilidad es\n\\[\nP_{X}(k)=P(X=k)=\\left\\{\\begin{array}{ll}\n     {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n & \\mbox{si } k=0,1,\\ldots\\\\\n     0 & \\mbox{en otro caso}\\end{array}\\right.\n\\] Una v.a. con este tipo de distribución recibe el nombre de binomial negativa y la denotaremos por \\(BN(n,p)\\).\nNotemos que \\(BN(1,p)=Ge(p)\\).\nDemostración\nJustifiquemos el resultado. Sea \\(X\\) una \\(BN(n,p)\\) y sea \\(k=0,1,2,\\ldots\\).\n\\[\\scriptsize{P(X=k)=P(\\mbox{Todas las cadenas de E's y F' con $k$ F, con $n$ E y acabadas en E})}\\]\n\\[\n\\scriptsize{\\overbrace{\\underbrace{\\overbrace{EFFF\\ldots EEF}^{n-1 \\quad \\mbox{Éxitos}.}}}_{k \\quad\\mbox{Fracasos}}^{k+n-1\\mbox{ posiciones}}E}\n\\]\nDe estas cadenas hay tantas como maneras de elegir de entre las \\(k+n-1\\) primeras posiciones \\(n-1\\) para colocar los éxitos. Esta cantidad es el número binomial \\({k+n-1\\choose n-1}.\\)\n\n\nNúmeros binomiales negativos\n\n\nDados dos enteros positivos \\(n\\) y \\(k\\) se define el número binomial negativo como\n\\[\\binom{-n}{k}=\\frac{(-n)(-n-1)\\cdots (-n-k+1)}{k!}.\\]\n\n\nLos números binomiales negativos generalizan la fórmula de Newton para exponentes negativos; obnetiendose el binomio geralizado de Newton:\n\\[\n(t+1)^{-n}=\\sum_{k=0}^{+\\infty}\\left(\\begin{array}{c} -n\n\\\\ k\\end{array}\\right) t^{k}\n\\]\nR usa la función choose para calcular números binomiales, sean negativos o no. Veámoslo con un ejemplo:\n\\[\n\\begin{array}{rl}\n{-6\\choose 4}&=\\frac{-6\\cdot (-6-1)\\cdot \\cdot (-6-2)\\cdot (-6-3) }{4!}\\\\\n&=  \\frac{-6\\cdot(-7)\\cdot (-8)\\cdot (-9)}{24}\\\\\n&= \\frac{3024}{24}=126.\n\\end{array}\n\\]\nSi realizamos el cálculo con R obtenemos el mismo resultado:\n\nchoose(-6,4)\n\n[1] 126\n\n\nLa esperanza de una \\(BN(n,p)\\) es\n\\[E(X)=\\sum_{k=0}^{+\\infty} k\\cdot {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n=n\\cdot\\frac{1-p}{p}.\\]\nLa esperanza de \\(X^2\\) es\n\\[E(X^2)=\\sum_{k=0}^{+\\infty} k^2\\cdot {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n=n\\cdot\\frac{1-p}{p^2}+\\left(n\\cdot \\frac{1-p}{p}\\right)^2.\\]\nPor último la varianza es\n\\[\nVar(X)=E(X^2)-E(X)^2=\n\\]\n\\[=n\\cdot \\frac{1-p}{p^2}+\\left(n\\cdot \\frac{1-p}{p}\\right)^2-\\left(n\\cdot \\frac{1-p}{p}\\right)^2=\nn\\cdot \\frac{1-p}{p^2}.\\]\ny por tanto la desviación típica es\n\\[\\sqrt{Var(X)} = \\frac{\\sqrt{n(1-p)}}{p}\\]\n\n4.5.1 Resumen distribución Binomial Negativa \\(BN(n,p)\\)\n\n\nEjercicio: Puerta con dos cerraduras\n\n\nRecordemos nuestra puerta con dos cerraduras que se abren secuencialmente. Tenemos un manojo de 10 llaves casi idénticas de manera que cada vez que probamos una llave olvidamos qué llave hemos usado.\nSea \\(X\\) la v.a que nos da el número de intentos fallidos hasta abrir abrir la puerta.\nEstamos interesado en modelar este problema. La preguntas son:\n\n¿Cuál es la distribución de probabilidad de \\(X\\) la v.a que nos da el número fallos hasta abrir la puerta?\n¿Cuál es la función de probabilidad y de distribución de \\(X\\)?\n¿Cuál es la probabilidad de fallar exactamente 5 veces antes de abrir la puerta?\n¿Cuál es la probabilidad de fallar más de 4?\n¿Cuál es el número esperado de fallos? ¿Y su desviación típica?\n\nSolución 1. ¿Cuál es la distribución de probabilidad de \\(X\\) la v.a que nos da el número fallos hasta abrir la puerta?\nBajo estados condiciones tenemos que la probabilidad de “éxito” de cada intento es \\(p=\\frac{1}{10}=0.1\\). Como cada vez olvidamos qué llave hemos probado, cada intento será independiente del anterior.\nAsí que la variable \\(X\\) que queremos modelar cuenta el número fallos de repeticiones sucesivas e independientes de un experimento \\(Ber(p=0.1)\\) hasta conseguir 2 éxitos en un experimento.\nPor lo tanto podemos asegurar que \\(X\\) sigue un distribución \\(BN(n=2,p=0.1).\\)\nSolución 2. ¿Cuál es la función de probabilidad y de distribución del \\(X\\)?\nEn general la función de probabilidad de una \\(BN(n,p)\\) es\n\\[\nP_X(k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n{k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n & \\mbox{si }  k=0,1,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSi aplicamos la expresión anterior para \\(n=2\\) y \\(p=0.1\\), obtenemos: \\[\nP_X(k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n{k+2-1\\choose 2-1} \\cdot 0.9^{k}\\cdot 0.1^2 & \\mbox{si }  k=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSimplificando\n\\[\nP_X(X=k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n0.01\\cdot (k+1)\\cdot 0.9^{k}, & \\mbox{si }  k=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nLa función de distribución en general es\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0 \\\\\n\\displaystyle\\sum_{i=0}^{k }{i+n-1\\choose n-1} \\cdot (1-p)^{i+n-1}\\cdot p^n\n& \\mbox{si }\\left\\{\\begin{array}{l} k\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\\end{array}\\right.\n\\end{array}\n\\right.\n\\]\nSimplificando para \\(n=2\\), \\(p=0.1\\).\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x&lt;0, \\\\\n\\displaystyle\\sum_{i=0}^{k }0.01\\cdot (i+1) \\cdot 0.9^{i+1},\n& \\mbox{si }\\left\\{\\begin{array}{l} k\\leq x&lt; k+1,\\\\k=0,1,2,\\ldots\\end{array}\\right.\n\\end{array}\n\\right.\n\\]\nSolución 3. ¿Cuál es la probabilidad de fallar exactamente 5 veces antes de abrir la puerta?\n\\[\nP(X=5)= 0.01\\cdot (5+1) \\cdot 0.9^{5}= 0.06 \\cdot 0.9^{5}= 0.0354294.\n\\]\nSolución 4. ¿Cuál es la probabilidad de fallar más de 4?\nNos piden que\n\\[\nP(X&gt;4)=1-P(X\\leq 4).\n\\]\nCalculemos primero \\(P(X\\leq 4):\\)\n\\[\n\\begin{array}{rl}\nP(X\\leq 4) &=  \\displaystyle\\sum_{x=0}^{4} P(X=x)=P(X=0)+P(X=1)+P(X=2)+P(X=3)+P(X=4)\\\\\n&= 0.01\\cdot (0+1) \\cdot 0.9^{0}+0.01\\cdot (1+1) \\cdot 0.9^{1}+0.01\\cdot (2+1) \\cdot 0.9^{2} \\\\ &\\ \\\n+0.01\\cdot (3+1) \\cdot 0.9^{3} + 0.01\\cdot (4+1) \\cdot 0.9^{4} \\\\ & =\n0.01 +0.018+0.0243+0.02916+0.032805 = 0.114265.\n\\end{array}\n\\]\nPor lo tanto\n\\[\nP(X&gt;4)=1-P(X\\leq 4)=1-0.114265=\n0.885735.\n\\]\nSolución 5. ¿Cuál es el número esperado de fallos? ¿Y su desviación típica?\nComo \\(X\\) sigue una ley \\(BN(n=2,p=0.1)\\)\n\\[E(X)=n\\cdot \\frac{1-p}{p}=2\\cdot \\frac{1-0.1}{0.1}=18.\\]\nEl número de fallos esperado es 18. La varianza es\n\\[\nVar(X)=n\\cdot\\frac{1-p}{p^2}=2 \\cdot \\frac{1-0.1}{0.1^2}=180,\n\\]\ny su desviación típica \\(\\sqrt{180}=13.41641.\\)\n\n4.5.2 Cálculos binomial negativa con R\nLa función de R que calcula la función de probabilidad de la binomial negativa con sus parámetros básicos es:\ndnbinom(x, size, prob,...)\ndonde size (\\(n\\)) es el número de éxitos y prob (\\(p\\)), la probabilidad de éxito.\nAsí en el ejemplo de la puerta con dos cerraduras, \\(X\\) es una \\(BN(n=size=2,p=prob=0.1)\\). Por ejemplo, \\(P(X=5)\\) que hemos calculado en el ejemplo anterior, vale:\n\ndnbinom(5,size=2,p=0.1)\n\n[1] 0.0354294\n\n\nDe forma similar calculamos calculamos \\(P(X\\leq 4)\\), \\(P(X&gt;4)=1-P(X\\leq 4)\\) y \\(P(X&gt;4)\\).\n\npnbinom(4,size=2,p=0.1)\n\n[1] 0.114265\n\n1-pnbinom(4,size=2,p=0.1)\n\n[1] 0.885735\n\npnbinom(4,size=2,p=0.1,lower.tail=FALSE)\n\n[1] 0.885735\n\n\n\n\n4.5.3 Cálculos binomial negativa con python\nLa función con python es nbinom.pmf(k, n, p, loc). Hay que cargarla desde scpi.stats\n\nfrom scipy.stats import nbinom\n\nRecordemos que de nuevo se cumple que\n\nnbinom.pmf(k, n, p, loc) = nbinom.pmf(k-loc, n, p)`\n\nCálculos \\(BN(n,p)\\) con python\n\nnbinom.pmf(k=5,n=2,p=0.1)\n\n0.0354294\n\nnbinom.pmf(k=5,n=2,p=0.1,loc=0)\n\n0.0354294\n\nnbinom.cdf(k=4,n=2,p=0.1)\n\n0.11426500000000002\n\n1-nbinom.cdf(k=4,n=2,p=0.1)\n\n0.8857349999999999\n\n\nGeneremos 100 observaciones aleatorias de una \\(BN(n=2,0.1)\\). Es decir serán las veces que hemos fallado hasta abrir la puerta 100 veces.\n\nnbinom.rvs(n=2, p=0.1, size=100)\n\narray([ 9, 34, 38, 38, 34, 45, 45, 20, 20,  9,  6, 15, 16, 26, 21, 37,  6,\n       15,  1, 21, 10, 16, 14, 22,  2, 22, 25, 22, 18, 36, 17, 34, 13, 35,\n       18, 31,  6, 32, 19, 16,  4,  8,  1,  1, 24, 20,  6, 12,  6, 30, 18,\n       27, 18,  5, 18, 24, 21, 22, 16,  8, 12, 11, 18, 16, 18, 26,  6, 32,\n       21, 41, 28, 14,  6,  8, 23, 18, 20,  9,  3, 14, 42, 13, 21,  0, 15,\n       12, 17, 26, 23,  8, 19,  6, 10,  1, 18,  7,  6, 14, 15,  9],\n      dtype=int64)\n\n\nLa esperanza y la varianzade una \\(BN(n=2,0.1)\\) valen:\n\nn, p=2,0.1\nparams = nbinom.stats(n,p,moments='mv')\nprint(\"E(X)={m}\".format(m=params[0]))\n\nE(X)=18.0\n\nprint(\"Var(X)={v}\".format(v=params[1]))\n\nVar(X)=179.99999999999997\n\n\n\n\n4.5.4 Gráficas de la binomial negativa con R\nEl siguiente código de R dibuja las función de probabilidad y la de distribución de una \\(BN(n=2,p=0.1)\\)\n\n\npar(mfrow=c(1,2))\naux=rep(0,22)\naux[seq(2,22,2)]=dnbinom(c(0:10),size=2,prob=0.1)\nplot(x=c(0:10),y=dnbinom(c(0:10),size=2,prob=0.1),\n  ylim=c(0,1),xlim=c(-1,11),xlab=\"x\",\n  main=\"Función de probabilidad\\n BN(n=2,p=0.1)\")\nlines(x=rep(0:10,each=2),y=aux, type = \"h\", lty = 2,col=\"blue\")\ncurve(pnbinom(x,size=2,prob=0,1),\n  xlim=c(-1,11),col=\"blue\",\n  main=\"Función de distribución\\n BN(n=2,p=0.1)\")\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\nEjercicio\nBuscad en los manuales de python cómo se dibuja la función de probabilidad y de distribución de una binomial negativa.\nNecesitamos de nuevo más librerías\n\nimport numpy as np\nfrom scipy.stats import nbinom\nimport matplotlib.pyplot as plt\n\n\n\n4.5.5 Gráficos de la binomial negativa con python\n\n\nn, p = 10, 0.25\nx = np.arange(0,nbinom.ppf(0.99, n, p))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=5, label='nbinom pmf')\nax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\nax.plot(x, nbinom.cdf(x, n, p), 'bo', ms=5, label='nbinom pmf')\nax.vlines(x, 0, nbinom.cdf(x, n, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion Binomial Negativa')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.6 Un ejemplo de la binomial negativa\n\n\nEjemplo: Sistema con tres claves de acceso\n\n\nSupongamos que tenemos un sistema informático tiene un programa de seguridad que genera accesos con claves de 3 dígitos \\(000,001,\\ldots 999\\). En total 1000 posibilidades.\nComo una clave de tres dígitos es fácil de romper proponemos considerar tres claves consecutivas de acceso al sistema, cada una de 3 dígitos.\nPara acceder al sistema hay que dar las tres claves de forma consecutiva y por orden.\nEs decir hasta que no averiguamos la primera clave no pasamos a la segunda clave.\nSupongamos que cada vez que ponemos las dos claves olvidamos el resultado y seguimos poniendo claves al azar hasta adivinar la contraseña.\nAsí hasta conseguir entrar en el sistema.\nSea \\(X\\) la v.a que nos da el número de fallos antes de entrar en el sistema.\nEstamos interesados en modelar este problema. La preguntas son:\n\n¿Cuál es la distribución de probabilidad de \\(X\\), la v.a que nos da el número de fallos antes de acceder al sistema.\n¿Cuál es la función de probabilidad y de distribución del \\(X\\)?\n¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\n¿Cuál es la probabilidad de fallar más de 150 veces antes de entrar en el sistema?\n¿Cuál es el número esperado de fallos antes de acceder al sistema? ¿Y su varianza?\n\nSolución 1. ¿Cuál es la distribución de probabilidad de \\(X\\), la v.a que nos da el número de fallos antes de acceder al sistema?\nBajo estados condiciones tenemos que la probabilidad de “éxito” de cada intento es \\(p=\\frac{1}{1000}=0.001\\). Y como cada vez olvidamos en los dígitos cada intento será independiente del anterior.\nAsí que la variable \\(X\\) cuenta el número de fracasos independientes hasta conseguir 3 éxitos en un experimento \\(Ber(p=0.001)\\) por lo tanto \\(X\\) sigue un distribución \\(BN(n=3,p=0.001).\\)\nSolución 2. ¿Cuál es la función de probabilidad y de distribución del \\(X\\)\nEn general la función de probabilidad de una \\(BN(n,p)\\) es\n\\[\nP_X(X=x)=P(X=x)=\n\\left\\{\n\\begin{array}{cc}\n{x+n-1\\choose n-1} \\cdot (1-p)^{x}\\cdot p^n & \\mbox{si }  x=0,1,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\] En particular la función de probabilidad de una \\(BN(n=3,p=0.001)\\) es\n\\[\nP_X(X=x)=P(X=x)=\n\\left\\{\n\\begin{array}{cc}\n{x+2\\choose 2} \\cdot 0.999^{x}\\cdot 0.001^3 & \\mbox{si }  x=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSolución 3. ¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\nNos piden\n\\[\n\\scriptsize{P(X=150)= {152\\choose 2} \\cdot 0.999^{150}\\cdot 0.001^3.}\n\\]\nLo calcularemos operando con R\n\nchoose(152,2)*0.999^150*0.001^3\n\n[1] 9.876743e-06\n\n\n\ndnbinom(150,size=3,p=0.001)\n\n[1] 9.876743e-06\n\n\nSolución 3. ¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\nNos piden, lo resolveremos con python\n\\[\nP(X=150)= {152\\choose 2} \\cdot 0.999^{150}\\cdot 0.001^3\n\\]\n\nfrom  scipy.special import binom\nbinom(152,2)*0.999**150*0.001**3\n\n9.876743459670526e-06\n\nnbinom.pmf(150,n=3,p=0.001)\n\n9.876743459670532e-06\n\n\nSolución 4. ¿Cuál es la probabilidad de fallar más de 150 veces antes de entrar en el sistema?\n\\[P(X&gt;150)=1-P(X\\leq 150)\\]\nCalculemos \\(P(X\\leq 150)\\)\n\\[\\begin{eqnarray*}\nP(X\\leq 150) &=& P(X=0)+P(X=1)+P(X=2)+\\ldots+P(X=150)\\\\\n&=& \\sum_{k=0}^{150} {k+3-1\\choose 3-1} \\cdot (0.999)^{k}\\cdot 0.001^3\\ldots = \\ldots =5.2320035\\times 10^{-4}\n\\end{eqnarray*}\\]\nCon R\n\npnbinom(150,3,0.001)\n\n[1] 0.0005232003\n\n\nCon python\n\nnbinom.cdf(150,n=3,p=0.001)\n\n0.0005232003490824064\n\n\nEl valor pedido será pues: \\[\nP(X&gt;150)=1-P(X\\leq 150)=1-5.2320035\\times 10^{-4}=0.9994768.\n\\] Vemos que es muy probable que fallemos más de 150 veces antes de entrar en el sistema.\nSolución 5. ¿Cuál es el número esperado de fallos antes de acceder al sistema? ¿Y su desviación típica?\nTenemos que \\(E(X)=n\\cdot \\frac{1-p}{p}=3\\cdot \\frac{1- 0.001}{0.001}=2997\\) y \\(Var(X)=n\\cdot \\frac{1-p}{p^2}=3\\cdot \\frac{1- 0.001^2}{0.001^2}=2.997\\times 10^{6}.\\)\nCon python\n\nparams = nbinom.stats(n=3,p=0.001,moments='mv')\nprint(\"E(X) = {m}\".format(m=params[0]))\n\nE(X) = 2997.0\n\nprint(\"Var(X) = {v}\".format(v=params[1]))\n\nVar(X) = 2997000.0\n\n\n\n\n\n\nEjemplo: ¿Tres claves de tres dígitos o una de 9 dígitos?\n\n\nSupongamos que ponemos una sola clave de 9 dígitos. Estudiemos en este caso la variable aleatoria que da el número de fallos antes de entrar en el sistema y comparemos los resultados.\nSi seguimos suponiendo que cada vez ponemos la contraseña al azar pero esta vez con una clave de 9 dígitos. La probabilidad de éxito será ahora \\(p=\\frac{1}{10^{9}}\\).\nSi llamamos \\(X_9\\) a la variable aleatoria que nos da el número de fallos antes de entra en el sistema seguirá una distribución \\(Ge(p=\\frac{1}{10^9}=0.000000001)\\).\n¿Qué da más seguridad? ¿tres claves de tres dígitos o una de 9 dígitos?\nSu valor esperado es\n\\[\nE(X_9)=\\frac{1-p}{p}=\\frac{1-0.000000001}{0.000000001}=10\\times 10^{8}.\n\\]\n\\(1000 000 000\\) son 1000 millones de fallos esperados hasta abrir la puerta.\nRecordemos que con tres contraseñas de 3 dígitos el valor esperado de fallos es\n\\[3\\cdot \\frac{1-0.001}{0.001}=2997.\\]\nPor lo tanto, desde el punto de vista de la seguridad, es mejor una clave larga de 9 dígitos que tres cortas si escribimos las contraseñas al azar.\n\n\n\n\n4.6 Distribución Poisson\nDefiniremos formamente la distribución de Poisson dadndo su dominio y función de probabilidad\n\n\nDefinición: Distribución de Poisson\n\n\nDiremos que una v.a. discreta \\(X\\) con \\(X(\\Omega)=\\mathbf{N}\\) tiene distribución de Poisson con parámetro \\(\\lambda&gt;0\\), y lo denotaremos por \\(Po(\\lambda)\\) si su función de probabilidad es:\n\\[\nP_{X}(x)=P(X=x)=\n\\left\\{\\begin{array}{ll}\n\\frac{\\lambda^x}{x!} e^{-\\lambda}& \\mbox{ si } x=0,1,\\ldots\\\\\n0 & \\mbox{en otro caso}\\end{array}\\right..\n\\]\n\n\nUsando que el desarrollo en serie de Taylor de la función exponencial es \\[\ne^{\\lambda}=\\sum_{x=0}^{+\\infty} \\frac{\\lambda^x}{x!},\n\\] es fácil comprobar que la suma de la función de probabilidad en todos los valores del dominio de \\(X\\), o sea, los enteros positivos, vale 1.\nAdemás recordemos que dado \\(x\\in\\mathbb{R}-\\{0\\}\\) se tiene que\n\\[\n\\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n=e^x.\n\\]\nUsando la expresión anterior para \\(x=-\\lambda\\), tenemos:\n\\[\n\\lim_{n\\to\\infty} \\left(1-\\frac{\\lambda}{n}\\right)^n=\\lim_{n\\to\\infty} \\left(1+\\frac{-\\lambda}{n}\\right)^n=e^{-\\lambda}.\n\\]\n\n4.6.1 La distribución de Poisson como “límite” de una binomial.\nLa distribución de Poisson (Siméon Denis Poisson) aparece en el conteo de determinados eventos que se producen en un intervalo de tiempo o en el espacio.\nSupongamos que nuestra variable de interés es \\(X\\), el número de eventos en el intervalo de tiempo \\((0,t]\\), como por ejemplo el número de llamadas a un call center en una hora donde suponemos que se cumplen las siguientes condiciones:\n\nEl número promedio de eventos en el intervalo \\((0,t]\\) es \\(\\lambda&gt;0\\).\nEs posible dividir el intervalo de tiempo en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más eventos en un subintervalo es despreciable.\nEl número de ocurrencias de eventos en un intervalo es independiente del número de ocurrencias en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)·\n\n\nBajo estas condiciones, podemos considerar que el número de eventos en el intervalo \\((0,t]\\) será el número de “éxitos” en \\(n\\) repeticiones independientes de un proceso Bernoulli de parámetro \\(p_n\\)\nEntonces si \\(n\\to\\infty\\) y \\(p_n\\cdot n\\) se mantiene igual a \\(\\lambda\\) resulta que la función de probabilidad de \\(X\\) se puede escribir como\n\\[\n\\begin{array}{rl}\nP(X_n=k)&=\\left(\\begin{array}{c} n\\\\ k\\end{array}\\right) \\cdot p_n^k\\cdot  (1-p_n)^{n-k}\n\\\\\n&= {n\\choose k}\\cdot \\left(\\frac{\\lambda}{n}\\right)^{k}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\\n&=\n\\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k}\\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\end{array}\n\\]\nSi hacemos tender \\(n\\) hacia \\(\\infty\\), obtenemos: \\[\n\\lim_{n\\to \\infty} P(X_n=k) = \\lim_{n\\to \\infty} \\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k} \\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\]\nCalculemos el límite de algunos de los factores de la expresión\n\\[\n\\displaystyle\\lim_{n\\to \\infty}\\frac{n!}{(n-k)!\\cdot n^k}= \\lim_{n\\to \\infty}\\frac{n\\cdot (n-1)\\cdots (n-k-1)}{n^k}\n=\\lim_{n\\to \\infty}\\frac{n^{k}+\\cdots}{n^k}=1.\n\\]\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n}=e^{-\\lambda}\n\\]\nY también teniendo en cuenta que \\(k\\) es constante.\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{-k}=\\lim_{n\\to \\infty} 1^{-k}=\\lim_{n\\to \\infty}  1=1.\n\\]\nPara acabar\n\\[\n\\displaystyle\\lim_{n\\to\\infty} P(X_n=k)=\n\\lim_{n\\to\\infty} \\left(\\begin{array}{c} n\\\\ k\\end{array}\\right)\n\\cdot p_n^k \\cdot (1-p_n)^{n-k}= \\frac{\\lambda^k}{k!}\\cdot 1 \\cdot e^{-\\lambda}\\cdot 1=\\frac{\\lambda^k}{k!}\\cdot e^{-\\lambda}.\n\\]\nLo que confirma que límite de una serie de variables \\(B(n,p_n=\\frac{\\lambda}{n})\\) sigue una ley \\(Po(\\lambda)\\).\n\n\n4.6.2 Procesos de Poisson\nLo interesante de las variables Poisson es que podemos modificar (si el modelo lo permite) el intervalo de tiempo \\((0,t]\\) en el que contamos los eventos.\nClaro que esto no tiene que poder ser así.\nPero en general si la variable es poisson en \\((0,t]\\) también lo será en cualquier subintervalo \\((0,t']\\) para todo \\(t'\\) tal que \\(0&lt;t'&lt;t\\).\nAsí que podremos definir una serie de variables \\(X_t\\) de distribución \\(Po(\\lambda\\cdot t)\\).\n\n\nDefinición: Procesos de Poisson\n\n\nConsideremos un experimento Poisson con \\(\\lambda\\) igual al promedio de eventos en una unidad de tiempo (u.t.).\nSi \\(t\\) es una cantidad de tiempo en u.t., la v.a. \\(X_{t}\\)=numero de eventos en el intervalo \\((0,t]\\) es una \\(Po(\\lambda\\cdot t)\\).\nEl conjunto de variables \\(\\{X_t\\}_{t&gt;0}\\) recibe el nombre de proceso de Poisson.\n\n\n\n\n4.6.3 Resumen distribución Poisson \\(X\\sim Po(\\lambda)\\)\n\n\n\n4.6.4 Aproximación de la distribución binomial por la Poisson\nBajo el punto de vista anterior y si \\(p\\) es pequeño y \\(n\\) suficientemente grande la distribución \\(B(n,p)\\) se aproxima a una \\(Po(\\lambda=n\\cdot p)\\).\nExisten distintos criterios (ninguno perfecto) de cuando la aproximación es buena.\nPor ejemplo si\n\\[n\\geq 20\\mbox{ o mejor }n\\geq 30, n\\cdot p &lt; 10 \\mbox{ y } p\\leq 0.05,\\]\nla aproximación de una \\(B(n,p)\\) por una \\(Po(n\\cdot p)\\) es buena. Sobre todo para los valores cercanos a \\(E(X)=\\lambda\\).\nCondición deseable \\(n\\geq 20\\), \\(n\\cdot p &lt; 10\\), \\(p\\leq 0.05\\).\n\n\nEjemplo: Trampa insectos.\n\n\nLa conocida lámpara antiinsectos o insecticida eléctrico atrae a los insectos voladores con una luz ultravioleta y los mata por electrocución.\nConsideremos la v.a. \\(X\\) que cuenta el número de insectos caídos en la trampa en una hora. Supongamos que el número promedio de insectos que captura la trampa en una hora es \\(E(X)=20\\) y que podemos admitir que \\(X\\) sigue una ley de probabilidad \\(Po(\\lambda=20)\\).\nNos piden\n\nComentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\nEscribir de forma explicita la función de probabilidad y de distribución de \\(X\\).\nCalculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nCalculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\n¿Cuál es el valor esperando, la varianza y la desviación típica de \\(X\\)?\n\nSolución 1. Comentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\n\nEl número promedio de eventos en el intervalo \\((0,1]\\), una hora es \\(\\lambda=20&gt;0\\).\nEs posible dividir el intervalo de tiempo de una hora en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más electrocuciones un subintervalo es despreciable. No es posible que dos mosquitos se electrocuten al mismo tiempo.\nEl número de ocurrencias, electrocuciones de insectos, en un intervalo es independiente del número de electrocuciones en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)· Podemos dividir los 20 insectos promedio entre los \\(n\\) intervalos (trozo de hora) de forma que \\(p_n=\\frac{\\lambda}{n}\\).\nPor ejemplo si \\(n=60\\) tenemos que \\(p_n=\\frac{20}{60}=\\frac{1}{3}\\). La probabilidad de que en un minuto la trampa chisporrotee es \\(\\frac{1}{3}\\).\n\n\nSolución 2. Escribid de forma explicita la función de probabilidad y de distribución de \\(X\\).\nLa distribución de probabilidad de un \\(Po(\\lambda)\\) es\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}  \\frac{\\lambda^x}{x!}e^{-\\lambda} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nEn nuestro caso, \\(\\lambda =20\\):\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}\\frac{20^x}{x!}e^{-20} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nLa función de distribución es\n\\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{\\lambda^i}{i!}\\cdot e^{-\\lambda} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nEn nuestro caso \\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{20^i}{i!}\\cdot e^{-20} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nSolución 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nNos piden la probabilidad siguiente: \\[\nP(X=21)=\\frac{20^{21}}{21!} e^{-20}=0.0846051.\n\\]\nPara realizar el cálculo anterior, podemos usar R como calculadora o usar la función dpois que nos calcula la función de distribución de la variable de Poisson:\n\n20^21/factorial(21)*exp(-20)\n\n[1] 0.08460506\n\ndpois(21,lambda = 20)\n\n[1] 0.08460506\n\n\nSolución 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nNos piden la probabilidad siguiente: \\[\n\\begin{array}{rl}\nP(X\\geq 6)&=1- P(X&lt;6)=1-P(X\\leq 5)=1-F_X(5)=1-\\displaystyle\\sum_{x=0}^{5} \\frac{20^{x}}{x!}\\cdot e^{-20}\\\\\n&=\n1-\\left(\\frac{20^{0}}{0!}\\cdot e^{-20}+\\frac{20^{1}}{1!}\\cdot e^{-20}+\\frac{20^{2}}{2!}\\cdot e^{-20}+\\frac{20^{3}}{3!}\\cdot e^{-20}+\\frac{20^{4}}{4!}\\cdot e^{-20}+\\frac{20^{5}}{5!}\\cdot e^{-20}\\right)\\\\\n&=\n1-e^{-20}\\cdot \\left(1+20+\\frac{400}{4}+\\frac{8000}{6}+\\frac{160000}{24}+\\frac{3200000}{120}\\right)\\\\\n&=\n1-e^{-20} \\cdot \\left(\\frac{1 \\cdot 120+20\\cdot 120+400\\cdot 30+8000\\cdot 20+160000\\cdot 24+3200000\\cdot 1}{120}\\right)\\\\\n&= 1-e^{-20}\\cdot\\left(\\frac{4186520}{120}\\right)=1-7.1908841\\times 10^{-5} =0.9999281.\n\\end{array}\n\\]\nSolución 5.** ¿Cuál es el valor esperado, la varianza y la desviación típica de \\(X\\)?\nEl valor esperado del número de insectos caídos en la trampa en una hora es\n\\[E(X)=\\lambda=20\\]\nSu varianza es \\[Var(X)=\\lambda=20\\]\ny su desviación típica vale \\[\\sqrt{Var(X)}=+\\sqrt{\\lambda}=+\\sqrt{20}=4.47214.\\]\n\n\n\n\n4.6.5 Cálculos Poisson con con R\nConsideremos por ejemplo una v.a. \\(X\\) con distribución \\(Po(\\lambda=3)\\). Calculemos \\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) con R:\n\ndpois(0,lambda = 3)\n\n[1] 0.04978707\n\ndpois(1,lambda = 3)\n\n[1] 0.1493612\n\n\nSi quisiéramos hallar la función de distribución en los mismos valores anteriores, \\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\), haríamos lo siguiente:\n\nppois(0,lambda = 3)\n\n[1] 0.04978707\n\nppois(1,lambda = 3)\n\n[1] 0.1991483\n\ndpois(0,lambda = 3)+dpois(1,lambda = 3) ## es igual a ppois(1,lambda=3)\n\n[1] 0.1991483\n\n\nA continuación, comprobemos que \\(F_X(10)=\\sum\\limits_{x=0}^{10} P_X(x)\\):\n\ndpois(0:10,3)\n\n [1] 0.0497870684 0.1493612051 0.2240418077 0.2240418077 0.1680313557\n [6] 0.1008188134 0.0504094067 0.0216040315 0.0081015118 0.0027005039\n[11] 0.0008101512\n\nsum(dpois(0:10,3))\n\n[1] 0.9997077\n\nppois(10,3)\n\n[1] 0.9997077\n\n\nSi quisiéramos generar una secuencia de \\(100\\) observaciones para una distribución de Poisson de parámetro \\(\\lambda=3\\), \\(Po(3)\\), tendríamos que hacer:\n\nrpois(n=100,lambda = 3)\n\n  [1] 2 5 3 3 2 2 5 2 4 4 2 3 2 2 2 2 2 3 3 5 3 3 2 4 2 3 2 1 1 3 4 6 2 5 3\n [36] 4 1 1 6 3 4 1 4 3 4 3 0 2 1 4 3 0 2 4 2 3 5 2 1 3 3 4 2 5 0 3 1 1 4 6\n [71] 4 5 0 4 0 3 3 3 4 1 2 6 2 2 2 2 1 2 5 2 5 3 7 3 5 2 3 2 1 3\n\n\n\n\nEjemplo: Trampa para insectos (continuación)\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con R a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nRecordemos que la probabilidad pedida es \\(P(X=21)\\):\n\ndpois(21,lambda=20)# P(X=21)\n\n[1] 0.08460506\n\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nRecordemos que la probabilidad pedida es \\(P(X\\geq 6)=1-P(X&lt;6)=1-P(X\\leq 5)\\):\n\nppois(5,lambda=20)\n\n[1] 7.190884e-05\n\n1-ppois(5,lambda=20) # es 1-P(X&lt;=5)=P(X&gt;=6)\n\n[1] 0.9999281\n\nppois(5,lambda=20,lower.tail =FALSE ) # acumula hacia arriba \n\n[1] 0.9999281\n\n# P(X&gt;5)=P(X&gt;=6)=P(X=6)+P(X=7)+...\n\n\n\nlambda=20; par(mfrow=c(1,2)); n=qpois(0.99,lambda=lambda)\naux=rep(0,(n+1)*2); aux[seq(2,(n+1)*2,2)]=dpois(c(0:n),lambda=lambda)\nymax=max(ppois(0:n,lambda=lambda)) \nplot(x=c(0:n),y=dpois(c(0:n),lambda=lambda),\n     ylim=c(0,ymax),xlim=c(-1,n+1),xlab=\"x\", ylab=\"Función de probabilidad\",\n     main=paste0(c(\"Función de probabilidad\\n  Po(lambda=\",lambda,\")\")\n                 collapse = \"\"))\nlines(x=rep(0:n,each=2),y=aux,pch=21, type = \"h\", lty = 2,col=\"blue\")\ncurve(ppois(x,lambda=lambda),\n      xlim=c(-1,n+1),col=\"blue\",ylab=\"Función de Distribución\",\n      main=paste0(c(\"Función de distribución \\n Po(lambda=\",lambda,\")\"),\n                  collapse = \"\"))\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.6 Cálculos Poisson con python\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) en este orden son\n\nfrom scipy.stats import poisson\npoisson.pmf(0,mu = 3)\n\n0.049787068367863944\n\npoisson.pmf(1,mu = 3)\n\n0.14936120510359185\n\n\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\) en este orden son\n\npoisson.cdf(0,mu = 3)\n\n0.04978706836786395\n\npoisson.cdf(1,mu = 3)\n\n0.1991482734714558\n\npoisson.pmf(0,mu = 3)+poisson.pmf(1,mu= 3) \n\n0.1991482734714558\n\n## es igual a poisson.cdf(1,lambda=3)\n\nPor ejemplo podemos comprobar que \\(F_X(10)=\\displaystyle\\sum_{0}^{10} P_X(x)\\)\n\n\npoisson.pmf(range(0,10),mu=3)\n\narray([0.04978707, 0.14936121, 0.22404181, 0.22404181, 0.16803136,\n       0.10081881, 0.05040941, 0.02160403, 0.00810151, 0.0027005 ])\n\nsum(poisson.pmf(range(0,10),mu=3))\n\n0.9988975118698846\n\npoisson.cdf(10,mu=3)\n\n0.9997076630493527\n\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con python a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nLa respuesta a la pregunta 3 es calcular \\(P(X=21)\\)\n\npoisson.pmf(21,mu=20)\n\n0.08460506418293791\n\n# P(X=21)\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nLa pregunta 4 nos pide calcular \\(P(X\\geq 6)=1-P(X\\leq 5)\\)\n\n1-poisson.cdf(5,mu=20) \n\n0.9999280911594716\n\n# es 1-P(X&lt;=5)=P(X&gt;=6)\n\nComo ya hemos visto con scipy.stats podemos pedir los momentos de una variable aleatoria \\(Po(3)\\)\n\npoisson.stats(mu=3, moments='mv')\n\n(array(3.), array(3.))\n\n\nY también generar secuencias de observaciones aleatorias de una población \\(Po(3)\\)\n\npoisson.rvs(mu=3,size=40)\n\narray([1, 4, 1, 5, 3, 3, 0, 0, 3, 5, 7, 3, 2, 2, 4, 3, 3, 2, 2, 2, 2, 3,\n       5, 2, 2, 6, 5, 3, 1, 1, 2, 4, 3, 5, 3, 4, 2, 2, 2, 6], dtype=int64)\n\n\n\n\nfrom scipy.stats import poisson\nmu = 10 # mu = lambda\nx = np.arange(poisson.ppf(0.01, mu),poisson.ppf(0.99, mu))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, poisson.pmf(x, mu), 'bo', ms=5, label='poisson pmf')\nax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks(): \n  tick.label.set_fontsize(5) \n\n\n\n\nax = fig.add_subplot(1,2,2)\nax.plot(x, poisson.cdf(x, mu), 'bo', ms=5, label='poisson cdf')\nax.vlines(x, 0, poisson.cdf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion de Poisson')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo:Número de impactos de insectos en la visera de un casco\n\n\nUn colega de trabajo, al que llamaremos JG, es muy aficionado a los grandes premios de velocidad tanto en coches como en motos.\nComo es tan aficionado está obsesionado con muchas de las más extravagantes estadísticas de estos deportes. En particular le propusimos que estudiara el número de insectos que chocan contra la visera de un casco de un motorista GP o de un conductor de fórmula 1 .\nLa idea es que el número de insectos está igualmente repartido por todo el circuito y de promedio impactan \\(\\lambda&gt;0\\) insectos por minuto. También es razonable suponer que:\n\npodemos dividir la superficie de la visera en cuadrados suficientemente pequeños de forma que la probabilidad de que caigan dos insectos en la misma zona es prácticamente 0.\nla probabilidad de que un insecto impacte en un cuadrado cualquiera de la visera es independiente de cualquier otro cuadrado.\nsi hemos dividido la visera en \\(n\\) cuadrados la probabilidad \\(p_n\\) de impacto de un cuadrado vale \\(p_n=\\frac{\\lambda}{n}\\).\n\nBajo estas condiciones, si denotamos por \\(X_t\\) como el número de insectos que ha impactado en la visera en el intervalo \\((0,t]\\) (en \\(t\\) minutos), podemos afirmar que \\(X_t\\) es un proceso de Poisson \\(Po(\\lambda\\cdot t)\\).\nSupongamos que nos dicen que \\(\\lambda=3\\) insectos por minuto. Entonces el proceso de poisson \\(X_t\\) seguirá un ley \\(Po(3\\cdot t).\\)\nAhora estamos en condiciones de preguntar al proceso de Poisson.\n¿Cuál es la probabilidad de que en 10 minutos impacten más de 25 insectos?\nEn este caso \\(t=10\\) \\(X_{10}\\)= número de insectos que impactan en 10 minutos, el intervalo \\([0,10)\\) que sigue una \\(P(3\\cdot 10=30)\\). Por lo tanto\n\\[P(X&gt;25)=1-P(X\\leq 25)\\]\nlo resolvemos con R\n\n1-ppois(25,lambda=30)\n\n[1] 0.7916426\n\n\nOtra pregunta interesante es que tengamos que esperar más de 2 minutos para observar el primer impacto\n\\[P(X_2=0)=\\frac{(3\\cdot 2)^0}{0!}\\cdot e^{-3\\cdot 2}= e^{-6}=0.002479.\\]\nCon R\n\n6^0/factorial(0)*exp(-6)\n\n[1] 0.002478752\n\nppois(0,lambda=3*2)\n\n[1] 0.002478752\n\n\n\n\n\n\n4.7 Distribución hipergeométrica\nSupongamos que disponemos de una urna de de sorteos que contiene \\(m\\) bolas blancas y \\(n\\) bolas rojas.\nEn total en esta urna hay \\(m+n\\) bolas, \\(m\\) blancas y \\(n\\) rojas. Si extraemos dos bolas de la urna lo podemos hacer de dos formas:\n\nExtraer una anotar su color y reponerla. Sacar otra y anotar su color. Hemos extraído la bola con reposición.\nExtraer simultáneamente dos bolas (sin reposición) y contar el número de bolas blancas.\n\nSea \\(X\\) es la v.a. que cuenta el número de bolas blancas extraídas.\n\nEn el primer caso, \\(X\\) es una \\(B(n=2,p=\\frac{m}{m+n})\\) ya que consiste en repetir dos veces el mismo experimento de Bernoulli.\nEn el segundo caso, \\(X\\) sigue una distribución hipergeométrica que estudiaremos en esta sección.\n\n\n\nDistribución hipergeométrica\n\n\nSean \\(n\\), \\(m\\) y \\(k\\) tres número enteros positivos y tales que \\(k&lt;m+n\\).\nConsideremos una urna que contiene \\(m+n\\) bolas de las que \\(m\\) son blancas y las restantes \\(n\\) no (son no blancas).\nEl número total de bolas es \\(m+n\\). Extraemos de forma aleatoria \\(k\\) bolas de la urna sin reemplazarlas.\nSea \\(X\\) la v.a. que cuenta el número de bolas blancas extraídas. Diremos que la distribución de \\(X\\) es hipergeométrica de parámetros \\(m\\), \\(n\\) y \\(k\\) y la denotaremos por \\(H(m,n,k)\\).\nSu dominio es\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\nFijemos ideas de cómo varía el dominio según los paraámetros \\(m\\), \\(n\\) y \\(k\\).\nPara explicarlo, veamos varios ejemplos:\n\n\\(H(m=5,n=2,k=3)\\). Tenemos \\(m=5\\) bolas blancas, \\(n=2\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas extraídas es \\(1=k-n=3-2\\), ya que sólo hay dos no blancas.\nEn cambio, el máximo si es \\(k=3\\), ya que tenemos bolas blancas de “sobra”.\n\n\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\\(H(m=2,n=5,k=3)\\). Tenemos \\(m=2\\) bolas blancas, \\(n=5\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas es \\(0\\) ya que puedo sacar 3 no blancas.\nEn cambio, el máximo si es \\(m=2\\), ya que aunque saquemos \\(k=3\\) bolas, al llegar a 2 ya hemos extraído todas las bolas blancas de la urna.\n\n\\(H(m=10,n=10,k=3)\\). Tenemos \\(m=10\\) bolas blancas, \\(n=10\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso podemos obtener desde \\(0\\) blancas hasta \\(k=3\\) blancas.\n\n\nSu función de probabilidad es: \\[\nP_{X}(x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}}, & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\}, \\mbox { para  } x\\in \\mathbf{N},\\\\\n0,  & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nObservación: otras parametrizaciones\nEn ocasiones se parametriza una v.a. hipergeométrica mediante \\(N=m+n\\), número total de bolas, \\(k\\), número de extracciones y \\(p\\), probabilidad de extraer una bola blanca.\nAsí podemos parametrizar alternativamente la distribución hipergeométrica así\n\\[H(N,k,p)\\mbox{ donde } p=\\frac{m}{N}.\\]\n\n4.7.1 Resumen distribución Hipergeométrica \\(H(m,n,k)\\).\n\n\nEjemplo clásico urna \\(m=15\\) blancas, \\(n=10\\) rojas y \\(k=3\\) extracciones sin reposición.\n\n\nTenemos una urna con 15 bolas blancas y 10 bolas rojas. Extraemos al azar tres bolas de la urna sin reposición. Sea \\(X\\) el número de bolas blancas extraídas. Bajo esta condiciones, la v.a. \\(X\\) sigue una ley de distribución \\(H(m=15,n=10,k=3)\\).\nSu función de probabilidad es\n\\[\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}} & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\} \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.,\n\\]\n\\[\\mbox{sustituyendo }\\scriptsize{\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{15}{x}\\cdot \\binom{10}{3-x}}{\\binom{25}{3}} & \\mbox{ si }\n0\\leq x \\leq 3 \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.\n}\\]\nLa probabilidad de sacar 2 blancas será\n\\[\nP(X=2)=\\frac{\\binom{15}{2}\\cdot \\binom{10}{3-2}}{\\binom{25}{3}}\n\\]\n\nc(choose(15,2), choose(10,1), choose(25,3))\n\n[1]  105   10 2300\n\n\n\\(P(X=2)=\\frac{105\\cdot10 }{2300}=0.4565217.\\)\nLa probabilidad de que saquemos más de 1 bola blanca es\n\\[\n\\begin{array}{rl}\nP(X&gt; 1)&= 1-P(X\\leq 1)=1-(P(X=0)+P(X=1))\\\\\n&=\n1-\\left(\\frac{\\binom{15}{0}\\cdot \\binom{10}{3}}{\\binom{25}{3}}+\n\\frac{\\binom{15}{1}\\cdot \\binom{10}{2}}{\\binom{25}{3}}\\right)\\\\\n&=\n1-\\left(\n\\frac{1\\cdot120 }{2300}+\\frac{15\\cdot45 }{2300}\n\\right)=1-\\frac{120+15\\cdot 45}{2300}=0.6543478.\n\\end{array}\n\\]\nEl número esperado de bolas blancas extraídas para una v.a. \\(X\\) \\(H(m=15,n=10,k=3)\\) es\n\\[E(X)=\\frac{k\\cdot m}{m+n}=\\frac{3\\cdot 15}{15+10}=\\frac{45}{35}=1.285714.\\]\nLa varianza vale: \\[\n\\begin{array}{rl}\nVar(X)&=k\\cdot\\frac{m}{m+n}\\cdot\\left(1-\\frac{m}{m+n}\\right) \\cdot\\frac{m+n-k}{m+n-1}\\\\\n&=3\\cdot\\frac{15}{15+10}\\cdot\\left(1-\\frac{15}{15+10}\\right) \\cdot\\frac{15+10-3}{15+10-1}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\left(1-\\frac{15}{25}\\right) \\cdot\\frac{22}{24}=\n3\\cdot\\frac{15}{25}\\cdot\\frac{25-15}{25} \\cdot\\frac{22}{24}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\frac{10}{25}\\cdot\\frac{22}{24}=0.66.\n\\end{array}\n\\]\nY por lo tanto su desviación típica es \\(+\\sqrt{Var(X)}=+\\sqrt{0.66}=0.812404.\\)\n\n\n\n\n4.7.2 Cálculos y gráficos distribución hipergeométrica con R\nSea \\(X\\) una v.a. \\(H(m,n,k)\\). La función de R para calcular la función de probabilidad en un valor \\(x\\), \\(P(X=x)\\), es dhyper(x,m,n,k) y para calcular la función de distribución en un valor \\(q\\), \\(P(X\\leq q)\\), es phyper(q,m,n,k). Para generar una muestra de valores que siga la distribución \\(H(m,n,k)\\), hay que usar la función rhyper(nn,m,n,k) donde nn es el número de observaciones aleatorias deseado de la muestra.\nPor ejemplo, si \\(X\\) es una \\(H(m=15,n=10,k=3)\\), los valores de \\(P(X=2)\\) y que \\(P(X&gt;1)=1-P(X\\leq 1)\\) son:\n\ndhyper(x=2,m=15,10,k=3)\n\n[1] 0.4565217\n\nphyper(q=1,m=15,n=10,k=3)# sí, le han puesto q ya veremos el porqué\n\n[1] 0.3456522\n\n1-phyper(q=1,m=15,n=10,k=3)\n\n[1] 0.6543478\n\n\nUna muestra aleatoria de este experimento de tamaño 200 sería:\n\nrhyper(nn=200,m=15,n=10,k=3)\n\n  [1] 2 3 1 3 1 2 2 3 2 2 1 2 1 2 2 3 3 1 1 1 1 0 2 3 2 1 3 2 2 2 2 3 2 3 3\n [36] 2 0 1 2 1 3 2 2 3 2 3 2 2 3 2 3 1 2 2 2 2 3 2 2 1 3 2 2 3 1 2 2 2 2 2\n [71] 3 0 2 0 3 2 2 2 1 2 2 3 1 1 1 2 2 2 2 1 1 3 2 2 3 2 2 1 1 1 3 3 2 2 2\n[106] 1 3 2 2 2 1 1 2 3 2 2 1 2 2 2 2 2 2 3 1 2 3 3 1 1 2 2 1 1 3 2 1 1 2 2\n[141] 3 1 1 1 2 1 1 3 1 2 2 3 3 2 3 1 2 1 2 2 2 1 2 3 1 3 3 3 2 2 1 3 3 1 1\n[176] 2 2 2 2 2 3 2 1 2 1 1 1 1 2 1 1 2 2 2 2 3 3 1 0 2\n\n\n\n\n\n\n\n\n\n\n\nSea \\(X\\) una \\(H(m,n,k)\\), las funciones de scipy.stats cambian los parámetros\n\n\\(M\\) es el número total de bolas. Con nuestra parametrización \\(M=m+n\\).\n\\(n\\) es el número de bolas blancas. Con nuestra parametrización \\(n=m\\).\n\\(N\\) es el número de extracciones. Con nuestra parametrización \\(N=k\\).\n\n\nfrom scipy.stats import hypergeom\n\n\n\n4.7.3 Cálculos y gráficos distribución hipergeométrica con python\n\nhypergeom.pmf(1,M=15+10,n=15,N=3)\n\n0.2934782608695652\n\nhypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.3456521739130434\n\n1-hypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.6543478260869566\n\n\nUna muestra aleatoria de este experimento sería…\n\nhypergeom.rvs(M=15+10,n=15,N=3,size=100)\n\narray([1, 2, 1, 3, 1, 3, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 3, 0, 2,\n       1, 1, 3, 2, 1, 2, 3, 3, 0, 2, 3, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 1,\n       2, 1, 2, 2, 1, 3, 1, 2, 1, 2, 0, 3, 1, 2, 3, 2, 2, 2, 2, 3, 2, 1,\n       3, 2, 2, 3, 2, 1, 1, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2,\n       2, 3, 1, 2, 0, 0, 2, 1, 2, 2, 2, 1], dtype=int64)\n\n\n\n\nfrom scipy.stats import hypergeom\n[M, n, N] = [20, 7, 12] ##20 elementos, 7 del tipo, extraemos 12\nx = np.arange(max(0, N-M+n),min(n, N))\nfig =plt.figure(figsize=(5, 2.7))\n =ax = fig.add_subplot(1,2,1)\n =ax.plot(x, hypergeom.pmf(x, M, n, N), 'bo', ms=5, label='hypergeom pmf')\n =ax.vlines(x, 0, hypergeom.pmf(x, M, n, N), colors='b', lw=2, alpha=0.5)\n =ax.set_ylim([0, max(hypergeom.pmf(x, M, n, N))*1.1])\n\n\n\n\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  =tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\n =ax.plot(x, hypergeom.cdf(x, M, n, N), 'bo', ms=5, label='hypergeom cdf')\n =ax.vlines(x, 0, hypergeom.cdf(x, M, n, N), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\n =fig.suptitle('Distribucion Hipergeometrica')\n =plt.show()",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuciones Notables I</span>"
    ]
  },
  {
    "objectID": "p01_04_1_Notables_discretas.html#distribución-geométrica",
    "href": "p01_04_1_Notables_discretas.html#distribución-geométrica",
    "title": "4  Distribuciones Notables I",
    "section": "4.4 Distribución geométrica",
    "text": "4.4 Distribución geométrica\nTodos hemos jugado a, por ejemplo, tirar una moneda hasta que obtengamos la primera cara.\nO también tirar una pelota a una canasta de baloncesto hasta obtener la primera canasta.\nDesde otro punto de vista también podemos intentar modelar el número de veces que accionamos una interruptor y la bombilla se ilumina hasta que falla.\nO también el número de veces que un cajero automático nos da dinero hasta que falla.\nLa modelización de este tipo de problemas se consigue con la llamada distribución geométrica.\n\n\nDefinición: Distribución geométrica\n\n\n\nRepitamos un experimento Bernoulli, de parámetro \\(p\\), de forma independiente hasta obtener el primer éxito.\nSea \\(X\\) la v.a. que cuenta el número de fracasos antes del primer éxito. Por ejemplo que hayamos tenido \\(x\\) fracasos será una cadena de \\(x\\) fracasos culminada con un éxito. Más concretamente\n\n\\[P(\\overbrace{FFF\\ldots F}^{x}E)=P(F)^{x}\\cdot P(E)=(1-p)^{x}\\cdot p=q^{x}\\cdot p.\\]\n\n\nAsí su función de probabilidad es\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}\n(1-p)^{x}\\cdot p & \\mbox{ si } x=0,1,2,\\ldots\\\\\n0 &\\mbox{ en otro caso}\n\\end{array}\\right..\n\\]\nLa v.a. definida anteriormente diremos que sigue una distribución geométrica de parámetro \\(p\\) La denotaremos por \\(Ge(p)\\). Su dominio es \\(D_X=\\{0,1,2,\\ldots\\}\\).\n\n\nEjemplo\n\n\nCalculemos P(\\(X\\leq 3\\)).\nPor la propiedad de la probabilidad del suceso complementario tenemos que\n\\[\nP(X\\leq 3 )=1-P(X&gt; 3)=1-P(X\\geq 4)\n\\]\nEfectivamente, el complementario del evento \\(X\\leq 3\\) nos dice que hemos fracasado más de tres veces hasta conseguir el primer éxito, es decir, hemos fracasado 4 o más veces. Podemos simbolizar dicho evento de la forma siguiente: \\[\n\\{X&gt;3\\}=\\{X\\geq 4\\}= \\{FFFF\\}\n\\]\nAhora, al ser los intentos independientes, tenemos que:\n\\[\\begin{eqnarray*}\nP(X&gt;3) & = & P(\\{FFFF\\})= P(F)\\cdot P(F)\\cdot P(F)\\cdot P(F)\\\\\n&=& (1-p)\\cdot (1-p)\\cdot (1-p)\\cdot (1-p)= (1-p)^{3+1}\\\\\n&=&(1-p)^{4}.\n\\end{eqnarray*}\\]\nEl valor de la función de distribución de \\(X\\) en \\(x=3\\) será, pues: \\[F_X(3)=P(X\\leq 3)=1-P(X&gt;3)=1-(1-p)^{3+1}.\\] Generalizando el resultado anterior a cualquier entero positivo \\(k=0,1,2,\\ldots\\), tenemos: \\[F_X(k)=P(X\\leq k)=1-(1-p)^{k+1},\\mbox{ si } k=0,1,2,\\ldots\\]\n\n\nEn general, tendremos que la función de distribución de una v.a.a \\(Ge(p)\\) es :\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\\begin{array}{ll}\n0, & \\mbox{ si } x&lt;0,\\\\\n1- (1-p),  & \\mbox{ si } k=0\\leq x &lt;1,\\\\\n1- (1-p)^2, & \\mbox{ si } k=1\\leq x &lt;2,\\\\\n1- (1-p)^3, & \\mbox{ si } k=2\\leq x &lt;3,\\\\\n1- (1-p)^{k+1}, & \\mbox{ si } \\left\\{ \\begin{array}{l}k\\leq x&lt; k+1,\\\\\\mbox{para } k=0,1,2,\\ldots\\end{array}\n    \\right.\n\\end{array}\n\\right..\n\\]\n\n4.4.1 Función de distribución geométrica\nDe forma más compacta, tendremos que \\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\\begin{array}{ll}\n0, & \\mbox{ si } x&lt;0,\\\\\n1- (1-p)^{k+1}, & \\mbox{ si } \\left\\{ \\begin{array}{l}k\\leq x&lt; k+1,\\\\\\mbox{para } k=0,1,2,\\ldots\\end{array}\n\\right.\n\\end{array}\n\\right..\n\\]\nNotemos que el límite de la función de distribución es: \\[\n\\displaystyle\\lim_{k\\to +\\infty } F_X(k)=\\lim_{k\\to +\\infty } 1-(1-p)^{k+1}=\n1,\n\\] ya que \\(0&lt;1-p&lt;1\\).\nRecordemos del tema de variables aleatorias que\n\n\nPropiedades\n\n\n\nSi \\(|r|&lt;1\\) también son convergentes las derivadas, respecto de \\(r\\), de la serie geométrica y convergen a la derivada correspondiente. Así tenemos que\n\n\\[\n\\begin{array}{rlrl}\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)'&= \\sum_{k=1}^{+\\infty}k\\cdot\nr^{k-1} &= \\left(\\frac1{1-r}\\right)'=\\frac1{(1-r)^2}\\\\\n\\left(\\sum_{k=0}^{+\\infty} r^k\\right)^{''}&= \\sum_{k=2}^{+\\infty}k \\cdot(k-1)\\cdot\nr^{k-2}&=\\left(\\frac1{1-r}\\right)^{''}=\\frac2{(1-r)^3}\n\\end{array}\n\\]\n\n\nVamos a calcular el valor esperado de una \\(Geo(p)\\) Recordemos que \\(P(X=x)=(1-p)^x\\cdot p\\) si \\(x=0,1,2,\\ldots\\) y aplicado la fórmula anterior con \\(r=1-p\\)\n\\[\\begin{eqnarray*}\nE(X)&=&\\sum_{x=0}^{+\\infty} x\\cdot P_x(x)=\\sum_{x=0}^{+\\infty} x\\cdot (1-p)^x\\cdot p\\\\\n&=& p\\cdot (1-p) \\cdot \\sum_{x=1}^{+\\infty} x\\cdot (1-p)^{x-1}\\\\\n&=& p\\cdot (1-p)\\cdot \\frac{1}{(1-(1-p))^2}=p\\cdot (1-p)\\cdot \\frac{1}{p^2}=\\frac{1-p}{p}\n\\end{eqnarray*}\\]\nAhora caclulamos \\(E(X^2)\\)\n\\[\\begin{eqnarray*}\nE(X^2)&=&\\sum_{x=0}^{+\\infty} x^2\\cdot P_X(x)=\\sum_{x=1}^{+\\infty} x^2\\cdot (1-p)^x\\cdot p\\\\\n&=&\n\\sum_{x=1}^{+\\infty} (x\\cdot (x-1)+x)\\cdot (1-p)^{x}\\cdot p\\\\\n&=&\n\\sum_{x=1}^{+\\infty} x\\cdot (x-1)\\cdot (1-p)^{x}\\cdot p+\\sum_{x=1}^{+\\infty} x \\cdot (1-p)^{x}\\cdot p\\\\\n&=&\n(1-p)^{2}\\cdot p\\cdot \\sum_{x=2}^{+\\infty} x\\cdot (x-1)\\cdot (1-p)^{x-2}\\\\\n&  +&   (1-p)\\cdot p\\sum_{x=1}^{+\\infty} x \\cdot (1-p)^{x-1} = \\ldots\n\\end{eqnarray*}\\].\n\\[\\begin{eqnarray*}\nE(X^2)&=&\\ldots\\\\\n&=&\n(1-p)^{2}\\cdot p\\cdot \\sum_{x=2}^{+\\infty} x\\cdot (x-1)\\cdot (1-p)^{x-2}\\\\\n&  +&   (1-p)\\cdot p\\sum_{x=1}^{+\\infty} x \\cdot (1-p)^{x-1}\\\\\n&=&\np\\cdot (1-p)^2 \\frac{2}{(1-(1-p))^3}+  (1-p)\\cdot p \\frac{1}{(1-(1-p))^2}\\\\\n&=&\np\\cdot (1-p)^2 \\frac{2}{p^3}+  (1-p)\\cdot p \\frac{1}{p^2}\\\\\n&=&\\frac{2\\cdot (1-p)^2}{p^2}+\\frac{1-p}{p}.\n\\end{eqnarray*}\\]\nAhora varianza de una v.a. \\(Ge(p)\\) es\n\\[\\begin{eqnarray*}\nVar(X)&=&E(X^2)-E(X)^2=\\frac{2\\cdot (1-p)^2}{p^2}+\\frac{1-p}{p}-\\left(\\frac{1-p}{p}\\right)^2\\\\\n&=&\n\\frac{2\\cdot (1-p)^2+p\\cdot(1-p)-(1-p)^2}{p^2}=\\frac{(1-p)^2+p\\cdot(1-p)}{p^2}\\\\\n&=&\n\\frac{1-2\\cdot p + p^2+p-p^2}{p^2}\\\\\n&=& \\frac{1-p}{p^2}.\n\\end{eqnarray*}\\]\nY su desviación típica será\n\\[\\sqrt{Var(X)}=\\sqrt{\\frac{1-p}{p^2}}.\\]\n\n\n4.4.2 Resumen distribución geométrica \\(Ge(p)\\) empezando en 0\n\n\n\n4.4.3 La variable geométrica que cuenta los intentos para obtener el primer éxito.\n\nSupongamos que sólo estamos interesados en el número de intentos para obtener el primer éxito.\nSi definimos \\(Y\\)= número de intentos para obtener el primer éxito. Entonces \\(Y=X+1\\) donde \\(X\\sim Ge(p)\\).\nSu dominio es \\(D_Y=\\{1,2,\\ldots\\}\\)\nLa media se incrementa en un intento debido al éxito \\(E(Y)=E(X+1)=E(X)+1=\\frac{1-p}{p}+1=\\frac1{p}\\).\nLa varianza es la misma \\(Var(Y)=Var(X+1)=Var(X)=\\frac{1-p}{p^2}\\).\n\n\n\n4.4.4 Resumen distribución geométrica \\(Ge(p)\\) empezando en \\(1\\).\n\n\n\n4.4.5 Propiedad de la falta de memoria\n\n\nPropiedad de la falta de memoria\n\n\nSea \\(X\\) una v.a. discreta con dominio \\(D_X=\\{0,1,2,\\ldots\\}\\), con \\(P(X=0)=p\\).\nEntonces \\(X\\) sigue una ley \\(Ge(p)\\) si, y sólo si,\n\\[\nP\\left(X&gt; k+j\\big| X\\geq j\\right)=P(X&gt; k)\n\\] para todo \\(k,j=0,1,2,3\\ldots\\).\n\n\nDemostración\nSi \\(X\\) es geométrica entonces el lado derecho de la igualdad es\n\\[\nP(X&gt;k)=1-P(X\\leq k)=1-\\left(1-(1-p)^{k+1}\\right)=(1-p)^{k+1},\n\\] y el lado de izquierdo es\n{ \\[\\begin{eqnarray*}\nP\\left(X&gt; k+j\\big| X\\geq j\\right)&=&\\frac{P\\left(\\{X&gt; k+j\\}\\cap \\{X\\geq j\\} \\right)}{P\\left(X\\geq j\\right)}=\n\\frac{P\\left(X&gt;k+j \\right)}{P\\left(X\\geq j \\right)} = \\frac{1-P(X\\leq k+j)}{1-P(X\\leq j-1)}\\\\\n&=&  \\frac{1-(1-(1-p)^{k+j+1})}{1-(1-(1-p)^{j-1+1})} =\\frac{(1-p)^{k+j+1}}{(1-p)^{j}} = (1-p)^{k+1},\n\\end{eqnarray*}\\] } \nlo que demuestra la igualdad.\nPara demostrar el recíproco, tomemos \\(j=1\\) y \\(k\\geq 0\\). Entonces, por la propiedad de la pérdida de memoria: \\[\nP\\left(X&gt; k+1\\big| X\\geq 1\\right)=P(X&gt; k)\n\\] Como \\(P(X=0)=p\\), tenemos que \\(P(X \\geq 1 )=1-P(X&lt;1)=1-P(X=0)=1-p\\).\nCombinado las igualdades, tenemos que:\n\\[\nP\\left(X&gt; k+1\\big| X\\geq 1\\right)=\\frac{P(X&gt;k+1, X\\geq 1)}{P(X\\geq 1)}=\\frac{P(X&gt;k+1)}{P(X\\geq 1)}=P(X&gt;k).\n\\] Así podemos poner que\n\\[\\begin{eqnarray*}\nP(X&gt;k+1)&=&P(X\\geq 1)\\cdot P(X&gt;k)=\\left(1-P(X&lt;1)\\right)\\cdot P(X&gt;k)\\\\\n&=&\\left(1-P(X=0)\\right)\\cdot P(X&gt;k)=(1-p)\\cdot P(X&gt;k).\n\\end{eqnarray*}\\]\nEs decir en general tenemos que\n\\[\nP(X&gt;k+1)=(1-p)\\cdot P(X&gt;k)\n\\] Del mismo modo para \\(j=2\\)\n\\[\n\\scriptsize{P(X&gt;k+2)=(1-p)\\cdot P(X&gt;k+1)}\n\\]\nRestando la primera igualdad de la última obtenemos.\n\\[\n\\scriptsize{P(X&gt;k+1)-P(X&gt;k+2)=(1-p)\\cdot P(X&gt;k)-(1-p)\\cdot P(X&gt;k+1)}\n\\]\nde donde operando en cada lado de la igualdad obtenemos la recurrencia\n\\[\n\\scriptsize{[1-P(X\\leq k+1)]-[1-P(X\\leq k+2)]=(1-p)\\cdot [P(X&gt;k)-P(X&gt;k+1)]}\n\\]\nAhora operando \\[\nP(X\\leq k+2)-P(X\\leq k+1)=(1-p)\\cdot[1-P(X\\leq k)-\\left(1-P(X\\leq k+1)\\right)]\n\\] \\[\nP(X=k+2)=(1-p)\\cdot[P(X\\leq k+1)-P(X\\leq k)]\n\\] \\[\nP(X=k+2)=(1-p)\\cdot P(X=k+1)\n\\]\nDe forma similar obtenemos\n\\[\nP(X=k+1)=(1-p)\\cdot P(X=k)\n\\] Utilizando la recurrencia anterior, podemos calcular todas las probabilidades \\(P(X=k)\\) a partir de la \\(P(X=0)=p\\): \\[\n\\scriptsize{\n\\begin{array}{rl}\nP(X=0)&= p,\\\\\nP(X=1)&=P(X=0+1)= (1-p)\\cdot P(X=0) =(1-p)\\cdot  p,\\\\\nP(X=2)&=P(X=1+1)= (1-p)\\cdot P(X=1)=(1-p)\\cdot (1-p)\\cdot p=(1-p)^2\\cdot p,\\\\\n\\vdots &    \\vdots \\\\\nP(X=k)&=P(X=(k-1)+1)= (1-p)\\cdot P(X=k-1)=(1-p)\\cdot (1-p)^{k-1}\\cdot p=(1-p)^{k}\\cdot p,\n\\end{array}\n}\n\\] lo que demuestra el recíproco, es decir, que \\(X\\) es \\(Geom(p)\\).\n\n\nObservación: Interpretación de la propiedad\n\n\nLa propiedad de la falta de memoria \\[\nP(X&gt; k+j\\big|X \\geq j)=P(X &gt; k),\n\\]\nsignifica que, aunque ya llevemos al menos \\(j\\) fracasos, la probabilidad de que fracasemos \\(k\\) veces más no disminuye, es la misma que era cuando empezamos el experimento.\nA este efecto se le suele etiquetar con la frase el experimento carece de memoria o es un experimento sin memoria (Memoryless Property).\n\n\nUn ejemplo muy sencillo nos aclarará el alcance de esta propiedad\n\n\nEjemplo:La llave que abre la puerta\n\n\nTenemos un llavero con 10 llaves, solo una de ellas abre una puerta. Cada vez que probamos una llave y falla olvidamos que llave hemos probado. ¿Cuál es la probabilidad de que si ya lo hemos intentado 5 veces necesitemos más de 4 intentos adicionales para abrir la puerta?\nTomemos \\(k=4,j=5\\), aplicando la propiedad de la falta de memoria\n\\[\nP(X&gt; 4+5/X \\geq 5)=P(X &gt; 4)\n\\]\nDespués de 5 fracasos no estamos “más cerca” de abrir la puerta. La propiedad de la falta de memoria nos dice que en después de cada intento es como si empezásemos de nuevo a abrir la puerta. Tras 5 fracasos la probabilidad de que fallemos más de 4 veces más es la misma que cuando lo intentamos la primera vez.\n¿Cuál es el número esperado de fracasos hasta abrir la puerta?\n\\[\nE(X)=\\frac{1-p}{p}=\\frac{1-\\frac{1}{10}}{\\frac{1}{10}}=\\frac{\\frac{9}{10}}{\\frac{1}{10}}=9.\n\\]\nLa varianza es\n\\[\nVar(X)=\\frac{1-p}{p^2}=\\frac{1-\\frac{1}{10}}{\\left(\\frac{1}{10}\\right)^2}=\\frac{\\frac{9}{10}}{\\frac{1}{100}}=\n90.\n\\]\nLa desviación típica es \\(\\sqrt{90}=9.486833.\\)\n\n\n\n\nEjemplo: partidos hasta que el Barça gana al Madrid\n\n\nEjemplo: partidos hasta que el Barça gana al Madrid\nLos partidos Real Madrid vs FC Barcelona de la liga española se suelen denominar El Clásico, sean en el Bernabeu (estadio del Real Madrid) o en el Camp Nou (estadio del Barça)\nSea \\(X\\) la variable que cuenta el número de veces consecutivas que en un partido de fútbol de la liga el Barça no gana al Madrid sea en el Camp Nou o el Bernabeu.\nNuestra amiga Aina es muy culé (hincha del Barça) y quiere averiguar cuántos partidos consecutivos de El Clásico tiene que ver hasta ver ganar al Barça por primera vez.\nLe interesa estimar cuánto le va a costar este capricho. Tendrá que comprar las entradas y pagar los viajes de Barcelona a Madrid.\nEn datos historicos de El clásico en la wikipedia están los datos hasta el 3 de marzo de 2019: se han jugado en total 178 Clásicos donde el Real Madrid ganó en 72 ocasiones, el Barça, en 72 y empataron 34 veces.\nNos hacemos las siguientes preguntas:\n\nSi Aina solo tiene dinero para ir a ver 3 partidos, ¿cuál es la probabilidad de no ver ganar al Barça en al menos tres partidos consecutivos?\n¿Cuántos partidos se tienen que jugar de media para ver ganar al Barça por primera vez?\n\nCon los datos anteriores, podemos estimar que la probabilidad de que el Barça gane un clásico cualquiera es:\n\\[P(\\mbox{Barça})=\\frac{72}{178}=0.4045.\\]\nPor tanto, podemos modelar la variable \\(X\\), que cuenta el número de veces consecutivas que en un partido de fútbol de la liga el Barça no gana al Madrid, con una ley geométrica empezando en cero con probabilidad de éxito \\(p=P(\\mbox{Barça})=\\frac{72}{178}\\),\n\\[X=Ge\\left(p=\\frac{72}{178}=0.4045\\right)\\]\nAsí que lo que nos pregunta Aina es la siguiente probabilidad\n\\[P(X\\geq 3)=1-P(X\\leq 2)=1-\\left(1-\\frac{72}{178}\\right)^{2+1}=0.7888.\\]\nAsí que Aina tiene una probabilidad del \\(78.88\\%\\) de no ver ganar al Barça en al menos 3 partidos antes de ver uno en el sí que gane.\nPara responder a la segunda pregunta, usando que la distribución de \\(X\\) es:\n\\[X=Ge\\left(p=\\frac{72}{178}=0.4045\\right)\\]\nentonces\n\\[E(X)=\\frac{1-p}{p}=\\frac{1-0.4045}{0.4045}=1.4722\\]\ny\n\\[Var(X)=\\frac{1-p}{p^2}=\\frac{1-0.4045}{0.4045^2}=3.6397\\]\nLa desviación típica es \\[\\sqrt{3.6397}=1.9078.\\]\n\n4.4.6 Cálculos de la distribución geométrica con R\nVeamos los cálculos básicos con R para la distribución geométrica \\(Ge(p=0.25)\\). R implementa la geométrica que cuenta el número de fracasos.\n\\(P(X=0)=(1-0.25)^0\\cdot 0.25^1=0.25\\)\n\ndgeom(0,prob=0.25)\n\n[1] 0.25\n\n\n\\(P(X\\leq 0)=1- (1-0.25)^{0+1}=1-0.75=0.25\\)\n\npgeom(0,prob=0.25)\n\n[1] 0.25\n\n\n\\(P(X\\leq 4)=1-(1-0.25)^{4+1}=1-0.75=1-0.75^5=0.7626953.\\)\n\npgeom(4,prob=0.25)\n\n[1] 0.7626953\n\n\nUna muestra aleatoria de tamaño 25 de una \\(Ge(0.25)\\)\n\nrgeom(n=25,prob=0.25)\n\n [1]  5  4  1  6 10  0  0 10  7  0  6  2  1  3  0  2  5  0  0  5  5  3  3\n[24]  2  2\n\n\n\n\n4.4.7 Gráficas con R\n\n  par(mfrow=c(1,2))\n  p=0.25\n  n=30\n  aux=rep(0,(n+1)*2)\n  aux[seq(2,(n+1)*2,2)]=dgeom(c(0:n),prob=p)\n  plot(x=c(0:n),y=dgeom(c(0:n),prob=p),\n       ylim=c(0,1),xlim=c(-1,n+1),xlab=\"x\",\n       main=paste0(c(\"Función de probabilidad\\n Ge(p=\",p,\")\"),collapse = \"\"))\n  lines(x=rep(0:n,each=2),y=aux, type = \"h\", lty = 2,col=\"blue\")\n  curve(pgeom(x,prob=p),\n        xlim=c(-1,n+1),col=\"blue\",\n        main=paste0(c(\"Función de distribución\\n Ge(p=\",p,\")\"),collapse = \"\"))\n\n\n\n\n\n\n\n  par(mfrow=c(1,1))\n\n\n\n4.4.8 Cálculos de la distribución geométrica con python\nVeamos los cálculos básicos con python para la distribución geométrica \\(Ge(p=0.25)\\). scipy.stats implementa la distribución geométrica que cuenta el número intentos así que empieza en 1\nCargamos la función de la librería\n\nfrom scipy.stats import geom\n\nLa función de probabilidad es geom.pmf(x,p,loc=0)=geom.pmf(x,p) es un geométrica que cuenta el número de intentos para obtener el primer éxito el valor por defecto del último parámetro es loc=0.\nSi queremos la que cuenta el número de fracasos para obtener el primer éxito (la geométrica que empieza en 0) tenemos que usar geom.pmf(x,p,loc=-1).\nEs decir geom.pmf(x,p,loc=-1)=geom.pmf(x-1,p,loc=0)\nVeamos pues los cálculos para la \\(Ge(p)\\) que empieza en \\(0\\).\n\\(P(X=0)=(1-0.25)^0\\cdot 0.25^1=0.25\\)\n\ngeom.pmf(0,p=0.25,loc=-1)\n\n\\(P(X\\leq 0)=1- (1-0.25)^{0+1}=1-0.75=0.25\\)\n\ngeom.cdf(0,p=0.25,loc=-1)\n\n\\(P(X\\leq 4)=1-(1-0.25)^{4+1}=1-0.75=1-0.75^5=0.7626953.\\)\n\ngeom.cdf(4,p=0.25,loc=-1)\n\nUna muestra aleatoria de tamaño 25 de una \\(Ge(0.25)\\)\n\ngeom.rvs(p=0.25, size=20, loc=-1)\n\nEjercicio\nQué probabilidades son las que calcula el siguiente código y qué tipo de variables geométricas son?\n\ngeom.cdf(range(5),p=0.3,loc=0)\ngeom.cdf(range(5),p=0.3,loc=-1)\n\nCon python también podemos calcular directamente algunos parámetros asociado a una función de distribución predefinida\n\ngeom.stats(p=0.25, loc=0, moments='mv')\ngeom.stats(p=0.25, loc=-1, moments='mv')\n\nEjercicio\nComprobad que las medias y las varianzas calculadas en el código anterior, corresponden a una \\(Ge(p=0.3)\\) empezando en \\(1\\) y a una \\(Ge(p=0.3)\\) empezando en \\(0\\).\n¿Son las varianzas siempre iguales?\n\n\n4.4.9 Gráficos con python\n\np = 0.25\nx = np.arange(geom.ppf(0.01, p),geom.ppf(0.99, p))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, geom.pmf(x, p), 'bo', ms=5, label='geom pmf')\nax.vlines(x, 0, geom.pmf(x, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\nax.plot(x, geom.cdf(x, p), 'bo', ms=5, label='geom pmf')\nax.vlines(x, 0, geom.cdf(x, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion Geometrica')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n4.5 Distribución binomial negativa\n\n\nEl problema de la puerta con dos cerraduras\n\n\nSupongamos que disponemos de 10 llaves distintas y tenemos que abrir una puerta con dos cerraduras.\nComenzamos por la primera cerradura, de tal forma que cada vez olvidamos qué llave hemos probado.\nUna vez abierta la primera cerradura probamos de igual forma con la segunda hasta que también la abrimos.\nSea \\(X=\\) la v.a. que cuenta el número de fracasos hasta abrir la puerta.\nAcertar una llave de la puerta es un experimento Bernoulli con probabilidad de éxito \\(p=0.1\\). Lo repetiremos hasta obtener 2 éxitos.\n\n\n\n\nDistribución binomial negativa\n\n\nEn general tendremos un experimento de Bernoulli con probabilidad de éxito \\(0&lt;p&lt;1\\) tal que:\n\nRepetimos el experimento hasta obtener el \\(n\\)-ésimo éxito ¡¡abrir la maldita puerta!!.\nSea \\(X\\) la v.a. que cuenta el número fallos hasta abrir la puerta, es decir, hasta conseguir el n-ésimo éxito. Notemos que no contamos los éxitos, solo contamos los fracasos.\n\nSi representamos como es habitual un suceso como una cadena de F’s y E’s, para \\(n=2\\), algunos sucesos elementales serán: \\[\\small{\\{EE,FEE,EFE, FFEE,FEFE,EFFE,FFFEE,FFEFE,FEFFE,EFFFE\\}.}\\]\nCalculemos algunas probabilidades para \\(n=2\\): \\[\n\\small{\n\\begin{array}{rl}\nP(X=0) & =P(\\{EE\\})=p^2, \\\\\nP(X=1) & =P(\\{FEE,EFE\\})=2\\cdot (1-p)\\cdot p^2, \\\\\nP(X=2) & =P(\\{FFEE,FEFE,EFFE\\})=3\\cdot (1-p)^2\\cdot p^2, \\\\\nP(X=3) & =P(\\{FFFEE,FFEFE,FEFFE,EFFFE\\})=4\\cdot (1-p)^3\\cdot p^2.\n\\end{array}\n}\n\\]\n\n\nEn general su función de probabilidad es\n\\[\nP_{X}(k)=P(X=k)=\\left\\{\\begin{array}{ll}\n     {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n & \\mbox{si } k=0,1,\\ldots\\\\\n     0 & \\mbox{en otro caso}\\end{array}\\right.\n\\] Una v.a. con este tipo de distribución recibe el nombre de binomial negativa y la denotaremos por \\(BN(n,p)\\).\nNotemos que \\(BN(1,p)=Ge(p)\\).\nDemostración\nJustifiquemos el resultado. Sea \\(X\\) una \\(BN(n,p)\\) y sea \\(k=0,1,2,\\ldots\\).\n\\[\\scriptsize{P(X=k)=P(\\mbox{Todas las cadenas de E's y F' con $k$ F, con $n$ E y acabadas en E})}\\]\n\\[\n\\scriptsize{\\overbrace{\\underbrace{\\overbrace{EFFF\\ldots EEF}^{n-1 \\quad \\mbox{Éxitos}.}}}_{k \\quad\\mbox{Fracasos}}^{k+n-1\\mbox{ posiciones}}E}\n\\]\nDe estas cadenas hay tantas como maneras de elegir de entre las \\(k+n-1\\) primeras posiciones \\(n-1\\) para colocar los éxitos. Esta cantidad es el número binomial \\({k+n-1\\choose n-1}.\\)\n\n\nNúmeros binomiales negativos\n\n\nDados dos enteros positivos \\(n\\) y \\(k\\) se define el número binomial negativo como\n\\[\\binom{-n}{k}=\\frac{(-n)(-n-1)\\cdots (-n-k+1)}{k!}.\\]\n\n\nLos números binomiales negativos generalizan la fórmula de Newton para exponentes negativos; obnetiendose el binomio geralizado de Newton:\n\\[\n(t+1)^{-n}=\\sum_{k=0}^{+\\infty}\\left(\\begin{array}{c} -n\n\\\\ k\\end{array}\\right) t^{k}\n\\]\nR usa la función choose para calcular números binomiales, sean negativos o no. Veámoslo con un ejemplo:\n\\[\n\\begin{array}{rl}\n{-6\\choose 4}&=\\frac{-6\\cdot (-6-1)\\cdot \\cdot (-6-2)\\cdot (-6-3) }{4!}\\\\\n&=  \\frac{-6\\cdot(-7)\\cdot (-8)\\cdot (-9)}{24}\\\\\n&= \\frac{3024}{24}=126.\n\\end{array}\n\\]\nSi realizamos el cálculo con R obtenemos el mismo resultado:\n\nchoose(-6,4)\n\n[1] 126\n\n\nLa esperanza de una \\(BN(n,p)\\) es\n\\[E(X)=\\sum_{k=0}^{+\\infty} k\\cdot {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n=n\\cdot\\frac{1-p}{p}.\\]\nLa esperanza de \\(X^2\\) es\n\\[E(X^2)=\\sum_{k=0}^{+\\infty} k^2\\cdot {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n=n\\cdot\\frac{1-p}{p^2}+\\left(n\\cdot \\frac{1-p}{p}\\right)^2.\\]\nPor último la varianza es\n\\[\nVar(X)=E(X^2)-E(X)^2=\n\\]\n\\[=n\\cdot \\frac{1-p}{p^2}+\\left(n\\cdot \\frac{1-p}{p}\\right)^2-\\left(n\\cdot \\frac{1-p}{p}\\right)^2=\nn\\cdot \\frac{1-p}{p^2}.\\]\ny por tanto la desviación típica es\n\\[\\sqrt{Var(X)} = \\frac{\\sqrt{n(1-p)}}{p}\\]\n\n4.5.1 Resumen distribución Binomial Negativa \\(BN(n,p)\\)\n\n\nEjercicio: Puerta con dos cerraduras\n\n\nRecordemos nuestra puerta con dos cerraduras que se abren secuencialmente. Tenemos un manojo de 10 llaves casi idénticas de manera que cada vez que probamos una llave olvidamos qué llave hemos usado.\nSea \\(X\\) la v.a que nos da el número de intentos fallidos hasta abrir abrir la puerta.\nEstamos interesado en modelar este problema. La preguntas son:\n\n¿Cuál es la distribución de probabilidad de \\(X\\) la v.a que nos da el número fallos hasta abrir la puerta?\n¿Cuál es la función de probabilidad y de distribución de \\(X\\)?\n¿Cuál es la probabilidad de fallar exactamente 5 veces antes de abrir la puerta?\n¿Cuál es la probabilidad de fallar más de 4?\n¿Cuál es el número esperado de fallos? ¿Y su desviación típica?\n\nSolución 1. ¿Cuál es la distribución de probabilidad de \\(X\\) la v.a que nos da el número fallos hasta abrir la puerta?\nBajo estados condiciones tenemos que la probabilidad de “éxito” de cada intento es \\(p=\\frac{1}{10}=0.1\\). Como cada vez olvidamos qué llave hemos probado, cada intento será independiente del anterior.\nAsí que la variable \\(X\\) que queremos modelar cuenta el número fallos de repeticiones sucesivas e independientes de un experimento \\(Ber(p=0.1)\\) hasta conseguir 2 éxitos en un experimento.\nPor lo tanto podemos asegurar que \\(X\\) sigue un distribución \\(BN(n=2,p=0.1).\\)\nSolución 2. ¿Cuál es la función de probabilidad y de distribución del \\(X\\)?\nEn general la función de probabilidad de una \\(BN(n,p)\\) es\n\\[\nP_X(k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n{k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n & \\mbox{si }  k=0,1,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSi aplicamos la expresión anterior para \\(n=2\\) y \\(p=0.1\\), obtenemos: \\[\nP_X(k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n{k+2-1\\choose 2-1} \\cdot 0.9^{k}\\cdot 0.1^2 & \\mbox{si }  k=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSimplificando\n\\[\nP_X(X=k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n0.01\\cdot (k+1)\\cdot 0.9^{k}, & \\mbox{si }  k=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nLa función de distribución en general es\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0 \\\\\n\\displaystyle\\sum_{i=0}^{k }{i+n-1\\choose n-1} \\cdot (1-p)^{i+n-1}\\cdot p^n\n& \\mbox{si }\\left\\{\\begin{array}{l} k\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\\end{array}\\right.\n\\end{array}\n\\right.\n\\]\nSimplificando para \\(n=2\\), \\(p=0.1\\).\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x&lt;0, \\\\\n\\displaystyle\\sum_{i=0}^{k }0.01\\cdot (i+1) \\cdot 0.9^{i+1},\n& \\mbox{si }\\left\\{\\begin{array}{l} k\\leq x&lt; k+1,\\\\k=0,1,2,\\ldots\\end{array}\\right.\n\\end{array}\n\\right.\n\\]\nSolución 3. ¿Cuál es la probabilidad de fallar exactamente 5 veces antes de abrir la puerta?\n\\[\nP(X=5)= 0.01\\cdot (5+1) \\cdot 0.9^{5}= 0.06 \\cdot 0.9^{5}= 0.0354294.\n\\]\nSolución 4. ¿Cuál es la probabilidad de fallar más de 4?\nNos piden que\n\\[\nP(X&gt;4)=1-P(X\\leq 4).\n\\]\nCalculemos primero \\(P(X\\leq 4):\\)\n\\[\n\\begin{array}{rl}\nP(X\\leq 4) &=  \\displaystyle\\sum_{x=0}^{4} P(X=x)=P(X=0)+P(X=1)+P(X=2)+P(X=3)+P(X=4)\\\\\n&= 0.01\\cdot (0+1) \\cdot 0.9^{0}+0.01\\cdot (1+1) \\cdot 0.9^{1}+0.01\\cdot (2+1) \\cdot 0.9^{2} \\\\ &\\ \\\n+0.01\\cdot (3+1) \\cdot 0.9^{3} + 0.01\\cdot (4+1) \\cdot 0.9^{4} \\\\ & =\n0.01 +0.018+0.0243+0.02916+0.032805 = 0.114265.\n\\end{array}\n\\]\nPor lo tanto\n\\[\nP(X&gt;4)=1-P(X\\leq 4)=1-0.114265=\n0.885735.\n\\]\nSolución 5. ¿Cuál es el número esperado de fallos? ¿Y su desviación típica?\nComo \\(X\\) sigue una ley \\(BN(n=2,p=0.1)\\)\n\\[E(X)=n\\cdot \\frac{1-p}{p}=2\\cdot \\frac{1-0.1}{0.1}=18.\\]\nEl número de fallos esperado es 18. La varianza es\n\\[\nVar(X)=n\\cdot\\frac{1-p}{p^2}=2 \\cdot \\frac{1-0.1}{0.1^2}=180,\n\\]\ny su desviación típica \\(\\sqrt{180}=13.41641.\\)\n\n4.5.2 Cálculos binomial negativa con R\nLa función de R que calcula la función de probabilidad de la binomial negativa con sus parámetros básicos es:\ndnbinom(x, size, prob,...)\ndonde size (\\(n\\)) es el número de éxitos y prob (\\(p\\)), la probabilidad de éxito.\nAsí en el ejemplo de la puerta con dos cerraduras, \\(X\\) es una \\(BN(n=size=2,p=prob=0.1)\\). Por ejemplo, \\(P(X=5)\\) que hemos calculado en el ejemplo anterior, vale:\n\ndnbinom(5,size=2,p=0.1)\n\n[1] 0.0354294\n\n\nDe forma similar calculamos calculamos \\(P(X\\leq 4)\\), \\(P(X&gt;4)=1-P(X\\leq 4)\\) y \\(P(X&gt;4)\\).\n\npnbinom(4,size=2,p=0.1)\n\n[1] 0.114265\n\n1-pnbinom(4,size=2,p=0.1)\n\n[1] 0.885735\n\npnbinom(4,size=2,p=0.1,lower.tail=FALSE)\n\n[1] 0.885735\n\n\n\n\n4.5.3 Cálculos binomial negativa con python\nLa función con python es nbinom.pmf(k, n, p, loc). Hay que cargarla desde scpi.stats\n\nfrom scipy.stats import nbinom\n\nRecordemos que de nuevo se cumple que\n\nnbinom.pmf(k, n, p, loc) = nbinom.pmf(k-loc, n, p)`\n\nCálculos \\(BN(n,p)\\) con python\n\nnbinom.pmf(k=5,n=2,p=0.1)\n\n0.0354294\n\nnbinom.pmf(k=5,n=2,p=0.1,loc=0)\n\n0.0354294\n\nnbinom.cdf(k=4,n=2,p=0.1)\n\n0.11426500000000002\n\n1-nbinom.cdf(k=4,n=2,p=0.1)\n\n0.8857349999999999\n\n\nGeneremos 100 observaciones aleatorias de una \\(BN(n=2,0.1)\\). Es decir serán las veces que hemos fallado hasta abrir la puerta 100 veces.\n\nnbinom.rvs(n=2, p=0.1, size=100)\n\narray([ 9, 34, 38, 38, 34, 45, 45, 20, 20,  9,  6, 15, 16, 26, 21, 37,  6,\n       15,  1, 21, 10, 16, 14, 22,  2, 22, 25, 22, 18, 36, 17, 34, 13, 35,\n       18, 31,  6, 32, 19, 16,  4,  8,  1,  1, 24, 20,  6, 12,  6, 30, 18,\n       27, 18,  5, 18, 24, 21, 22, 16,  8, 12, 11, 18, 16, 18, 26,  6, 32,\n       21, 41, 28, 14,  6,  8, 23, 18, 20,  9,  3, 14, 42, 13, 21,  0, 15,\n       12, 17, 26, 23,  8, 19,  6, 10,  1, 18,  7,  6, 14, 15,  9],\n      dtype=int64)\n\n\nLa esperanza y la varianzade una \\(BN(n=2,0.1)\\) valen:\n\nn, p=2,0.1\nparams = nbinom.stats(n,p,moments='mv')\nprint(\"E(X)={m}\".format(m=params[0]))\n\nE(X)=18.0\n\nprint(\"Var(X)={v}\".format(v=params[1]))\n\nVar(X)=179.99999999999997\n\n\n\n\n4.5.4 Gráficas de la binomial negativa con R\nEl siguiente código de R dibuja las función de probabilidad y la de distribución de una \\(BN(n=2,p=0.1)\\)\n\n\npar(mfrow=c(1,2))\naux=rep(0,22)\naux[seq(2,22,2)]=dnbinom(c(0:10),size=2,prob=0.1)\nplot(x=c(0:10),y=dnbinom(c(0:10),size=2,prob=0.1),\n  ylim=c(0,1),xlim=c(-1,11),xlab=\"x\",\n  main=\"Función de probabilidad\\n BN(n=2,p=0.1)\")\nlines(x=rep(0:10,each=2),y=aux, type = \"h\", lty = 2,col=\"blue\")\ncurve(pnbinom(x,size=2,prob=0,1),\n  xlim=c(-1,11),col=\"blue\",\n  main=\"Función de distribución\\n BN(n=2,p=0.1)\")\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\nEjercicio\nBuscad en los manuales de python cómo se dibuja la función de probabilidad y de distribución de una binomial negativa.\nNecesitamos de nuevo más librerías\n\nimport numpy as np\nfrom scipy.stats import nbinom\nimport matplotlib.pyplot as plt\n\n\n\n4.5.5 Gráficos de la binomial negativa con python\n\n\nn, p = 10, 0.25\nx = np.arange(0,nbinom.ppf(0.99, n, p))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=5, label='nbinom pmf')\nax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\nax.plot(x, nbinom.cdf(x, n, p), 'bo', ms=5, label='nbinom pmf')\nax.vlines(x, 0, nbinom.cdf(x, n, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion Binomial Negativa')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.6 Un ejemplo de la binomial negativa\n\n\nEjemplo: Sistema con tres claves de acceso\n\n\nSupongamos que tenemos un sistema informático tiene un programa de seguridad que genera accesos con claves de 3 dígitos \\(000,001,\\ldots 999\\). En total 1000 posibilidades.\nComo una clave de tres dígitos es fácil de romper proponemos considerar tres claves consecutivas de acceso al sistema, cada una de 3 dígitos.\nPara acceder al sistema hay que dar las tres claves de forma consecutiva y por orden.\nEs decir hasta que no averiguamos la primera clave no pasamos a la segunda clave.\nSupongamos que cada vez que ponemos las dos claves olvidamos el resultado y seguimos poniendo claves al azar hasta adivinar la contraseña.\nAsí hasta conseguir entrar en el sistema.\nSea \\(X\\) la v.a que nos da el número de fallos antes de entrar en el sistema.\nEstamos interesados en modelar este problema. La preguntas son:\n\n¿Cuál es la distribución de probabilidad de \\(X\\), la v.a que nos da el número de fallos antes de acceder al sistema.\n¿Cuál es la función de probabilidad y de distribución del \\(X\\)?\n¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\n¿Cuál es la probabilidad de fallar más de 150 veces antes de entrar en el sistema?\n¿Cuál es el número esperado de fallos antes de acceder al sistema? ¿Y su varianza?\n\nSolución 1. ¿Cuál es la distribución de probabilidad de \\(X\\), la v.a que nos da el número de fallos antes de acceder al sistema?\nBajo estados condiciones tenemos que la probabilidad de “éxito” de cada intento es \\(p=\\frac{1}{1000}=0.001\\). Y como cada vez olvidamos en los dígitos cada intento será independiente del anterior.\nAsí que la variable \\(X\\) cuenta el número de fracasos independientes hasta conseguir 3 éxitos en un experimento \\(Ber(p=0.001)\\) por lo tanto \\(X\\) sigue un distribución \\(BN(n=3,p=0.001).\\)\nSolución 2. ¿Cuál es la función de probabilidad y de distribución del \\(X\\)\nEn general la función de probabilidad de una \\(BN(n,p)\\) es\n\\[\nP_X(X=x)=P(X=x)=\n\\left\\{\n\\begin{array}{cc}\n{x+n-1\\choose n-1} \\cdot (1-p)^{x}\\cdot p^n & \\mbox{si }  x=0,1,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\] En particular la función de probabilidad de una \\(BN(n=3,p=0.001)\\) es\n\\[\nP_X(X=x)=P(X=x)=\n\\left\\{\n\\begin{array}{cc}\n{x+2\\choose 2} \\cdot 0.999^{x}\\cdot 0.001^3 & \\mbox{si }  x=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSolución 3. ¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\nNos piden\n\\[\n\\scriptsize{P(X=150)= {152\\choose 2} \\cdot 0.999^{150}\\cdot 0.001^3.}\n\\]\nLo calcularemos operando con R\n\nchoose(152,2)*0.999^150*0.001^3\n\n[1] 9.876743e-06\n\n\n\ndnbinom(150,size=3,p=0.001)\n\n[1] 9.876743e-06\n\n\nSolución 3. ¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\nNos piden, lo resolveremos con python\n\\[\nP(X=150)= {152\\choose 2} \\cdot 0.999^{150}\\cdot 0.001^3\n\\]\n\nfrom  scipy.special import binom\nbinom(152,2)*0.999**150*0.001**3\n\n9.876743459670526e-06\n\nnbinom.pmf(150,n=3,p=0.001)\n\n9.876743459670532e-06\n\n\nSolución 4. ¿Cuál es la probabilidad de fallar más de 150 veces antes de entrar en el sistema?\n\\[P(X&gt;150)=1-P(X\\leq 150)\\]\nCalculemos \\(P(X\\leq 150)\\)\n\\[\\begin{eqnarray*}\nP(X\\leq 150) &=& P(X=0)+P(X=1)+P(X=2)+\\ldots+P(X=150)\\\\\n&=& \\sum_{k=0}^{150} {k+3-1\\choose 3-1} \\cdot (0.999)^{k}\\cdot 0.001^3\\ldots = \\ldots =5.2320035\\times 10^{-4}\n\\end{eqnarray*}\\]\nCon R\n\npnbinom(150,3,0.001)\n\n[1] 0.0005232003\n\n\nCon python\n\nnbinom.cdf(150,n=3,p=0.001)\n\n0.0005232003490824064\n\n\nEl valor pedido será pues: \\[\nP(X&gt;150)=1-P(X\\leq 150)=1-5.2320035\\times 10^{-4}=0.9994768.\n\\] Vemos que es muy probable que fallemos más de 150 veces antes de entrar en el sistema.\nSolución 5. ¿Cuál es el número esperado de fallos antes de acceder al sistema? ¿Y su desviación típica?\nTenemos que \\(E(X)=n\\cdot \\frac{1-p}{p}=3\\cdot \\frac{1- 0.001}{0.001}=2997\\) y \\(Var(X)=n\\cdot \\frac{1-p}{p^2}=3\\cdot \\frac{1- 0.001^2}{0.001^2}=2.997\\times 10^{6}.\\)\nCon python\n\nparams = nbinom.stats(n=3,p=0.001,moments='mv')\nprint(\"E(X) = {m}\".format(m=params[0]))\n\nE(X) = 2997.0\n\nprint(\"Var(X) = {v}\".format(v=params[1]))\n\nVar(X) = 2997000.0\n\n\n\n\n\n\nEjemplo: ¿Tres claves de tres dígitos o una de 9 dígitos?\n\n\nSupongamos que ponemos una sola clave de 9 dígitos. Estudiemos en este caso la variable aleatoria que da el número de fallos antes de entrar en el sistema y comparemos los resultados.\nSi seguimos suponiendo que cada vez ponemos la contraseña al azar pero esta vez con una clave de 9 dígitos. La probabilidad de éxito será ahora \\(p=\\frac{1}{10^{9}}\\).\nSi llamamos \\(X_9\\) a la variable aleatoria que nos da el número de fallos antes de entra en el sistema seguirá una distribución \\(Ge(p=\\frac{1}{10^9}=0.000000001)\\).\n¿Qué da más seguridad? ¿tres claves de tres dígitos o una de 9 dígitos?\nSu valor esperado es\n\\[\nE(X_9)=\\frac{1-p}{p}=\\frac{1-0.000000001}{0.000000001}=10\\times 10^{8}.\n\\]\n\\(1000 000 000\\) son 1000 millones de fallos esperados hasta abrir la puerta.\nRecordemos que con tres contraseñas de 3 dígitos el valor esperado de fallos es\n\\[3\\cdot \\frac{1-0.001}{0.001}=2997.\\]\nPor lo tanto, desde el punto de vista de la seguridad, es mejor una clave larga de 9 dígitos que tres cortas si escribimos las contraseñas al azar.\n\n\n\n\n4.6 Distribución Poisson\nDefiniremos formamente la distribución de Poisson dadndo su dominio y función de probabilidad\n\n\nDefinición: Distribución de Poisson\n\n\nDiremos que una v.a. discreta \\(X\\) con \\(X(\\Omega)=\\mathbf{N}\\) tiene distribución de Poisson con parámetro \\(\\lambda&gt;0\\), y lo denotaremos por \\(Po(\\lambda)\\) si su función de probabilidad es:\n\\[\nP_{X}(x)=P(X=x)=\n\\left\\{\\begin{array}{ll}\n\\frac{\\lambda^x}{x!} e^{-\\lambda}& \\mbox{ si } x=0,1,\\ldots\\\\\n0 & \\mbox{en otro caso}\\end{array}\\right..\n\\]\n\n\nUsando que el desarrollo en serie de Taylor de la función exponencial es \\[\ne^{\\lambda}=\\sum_{x=0}^{+\\infty} \\frac{\\lambda^x}{x!},\n\\] es fácil comprobar que la suma de la función de probabilidad en todos los valores del dominio de \\(X\\), o sea, los enteros positivos, vale 1.\nAdemás recordemos que dado \\(x\\in\\mathbb{R}-\\{0\\}\\) se tiene que\n\\[\n\\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n=e^x.\n\\]\nUsando la expresión anterior para \\(x=-\\lambda\\), tenemos:\n\\[\n\\lim_{n\\to\\infty} \\left(1-\\frac{\\lambda}{n}\\right)^n=\\lim_{n\\to\\infty} \\left(1+\\frac{-\\lambda}{n}\\right)^n=e^{-\\lambda}.\n\\]\n\n4.6.1 La distribución de Poisson como “límite” de una binomial.\nLa distribución de Poisson (Siméon Denis Poisson) aparece en el conteo de determinados eventos que se producen en un intervalo de tiempo o en el espacio.\nSupongamos que nuestra variable de interés es \\(X\\), el número de eventos en el intervalo de tiempo \\((0,t]\\), como por ejemplo el número de llamadas a un call center en una hora donde suponemos que se cumplen las siguientes condiciones:\n\nEl número promedio de eventos en el intervalo \\((0,t]\\) es \\(\\lambda&gt;0\\).\nEs posible dividir el intervalo de tiempo en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más eventos en un subintervalo es despreciable.\nEl número de ocurrencias de eventos en un intervalo es independiente del número de ocurrencias en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)·\n\n\nBajo estas condiciones, podemos considerar que el número de eventos en el intervalo \\((0,t]\\) será el número de “éxitos” en \\(n\\) repeticiones independientes de un proceso Bernoulli de parámetro \\(p_n\\)\nEntonces si \\(n\\to\\infty\\) y \\(p_n\\cdot n\\) se mantiene igual a \\(\\lambda\\) resulta que la función de probabilidad de \\(X\\) se puede escribir como\n\\[\n\\begin{array}{rl}\nP(X_n=k)&=\\left(\\begin{array}{c} n\\\\ k\\end{array}\\right) \\cdot p_n^k\\cdot  (1-p_n)^{n-k}\n\\\\\n&= {n\\choose k}\\cdot \\left(\\frac{\\lambda}{n}\\right)^{k}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\\n&=\n\\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k}\\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\end{array}\n\\]\nSi hacemos tender \\(n\\) hacia \\(\\infty\\), obtenemos: \\[\n\\lim_{n\\to \\infty} P(X_n=k) = \\lim_{n\\to \\infty} \\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k} \\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\]\nCalculemos el límite de algunos de los factores de la expresión\n\\[\n\\displaystyle\\lim_{n\\to \\infty}\\frac{n!}{(n-k)!\\cdot n^k}= \\lim_{n\\to \\infty}\\frac{n\\cdot (n-1)\\cdots (n-k-1)}{n^k}\n=\\lim_{n\\to \\infty}\\frac{n^{k}+\\cdots}{n^k}=1.\n\\]\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n}=e^{-\\lambda}\n\\]\nY también teniendo en cuenta que \\(k\\) es constante.\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{-k}=\\lim_{n\\to \\infty} 1^{-k}=\\lim_{n\\to \\infty}  1=1.\n\\]\nPara acabar\n\\[\n\\displaystyle\\lim_{n\\to\\infty} P(X_n=k)=\n\\lim_{n\\to\\infty} \\left(\\begin{array}{c} n\\\\ k\\end{array}\\right)\n\\cdot p_n^k \\cdot (1-p_n)^{n-k}= \\frac{\\lambda^k}{k!}\\cdot 1 \\cdot e^{-\\lambda}\\cdot 1=\\frac{\\lambda^k}{k!}\\cdot e^{-\\lambda}.\n\\]\nLo que confirma que límite de una serie de variables \\(B(n,p_n=\\frac{\\lambda}{n})\\) sigue una ley \\(Po(\\lambda)\\).\n\n\n4.6.2 Procesos de Poisson\nLo interesante de las variables Poisson es que podemos modificar (si el modelo lo permite) el intervalo de tiempo \\((0,t]\\) en el que contamos los eventos.\nClaro que esto no tiene que poder ser así.\nPero en general si la variable es poisson en \\((0,t]\\) también lo será en cualquier subintervalo \\((0,t']\\) para todo \\(t'\\) tal que \\(0&lt;t'&lt;t\\).\nAsí que podremos definir una serie de variables \\(X_t\\) de distribución \\(Po(\\lambda\\cdot t)\\).\n\n\nDefinición: Procesos de Poisson\n\n\nConsideremos un experimento Poisson con \\(\\lambda\\) igual al promedio de eventos en una unidad de tiempo (u.t.).\nSi \\(t\\) es una cantidad de tiempo en u.t., la v.a. \\(X_{t}\\)=numero de eventos en el intervalo \\((0,t]\\) es una \\(Po(\\lambda\\cdot t)\\).\nEl conjunto de variables \\(\\{X_t\\}_{t&gt;0}\\) recibe el nombre de proceso de Poisson.\n\n\n\n\n4.6.3 Resumen distribución Poisson \\(X\\sim Po(\\lambda)\\)\n\n\n\n4.6.4 Aproximación de la distribución binomial por la Poisson\nBajo el punto de vista anterior y si \\(p\\) es pequeño y \\(n\\) suficientemente grande la distribución \\(B(n,p)\\) se aproxima a una \\(Po(\\lambda=n\\cdot p)\\).\nExisten distintos criterios (ninguno perfecto) de cuando la aproximación es buena.\nPor ejemplo si\n\\[n\\geq 20\\mbox{ o mejor }n\\geq 30, n\\cdot p &lt; 10 \\mbox{ y } p\\leq 0.05,\\]\nla aproximación de una \\(B(n,p)\\) por una \\(Po(n\\cdot p)\\) es buena. Sobre todo para los valores cercanos a \\(E(X)=\\lambda\\).\nCondición deseable \\(n\\geq 20\\), \\(n\\cdot p &lt; 10\\), \\(p\\leq 0.05\\).\n\n\nEjemplo: Trampa insectos.\n\n\nLa conocida lámpara antiinsectos o insecticida eléctrico atrae a los insectos voladores con una luz ultravioleta y los mata por electrocución.\nConsideremos la v.a. \\(X\\) que cuenta el número de insectos caídos en la trampa en una hora. Supongamos que el número promedio de insectos que captura la trampa en una hora es \\(E(X)=20\\) y que podemos admitir que \\(X\\) sigue una ley de probabilidad \\(Po(\\lambda=20)\\).\nNos piden\n\nComentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\nEscribir de forma explicita la función de probabilidad y de distribución de \\(X\\).\nCalculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nCalculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\n¿Cuál es el valor esperando, la varianza y la desviación típica de \\(X\\)?\n\nSolución 1. Comentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\n\nEl número promedio de eventos en el intervalo \\((0,1]\\), una hora es \\(\\lambda=20&gt;0\\).\nEs posible dividir el intervalo de tiempo de una hora en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más electrocuciones un subintervalo es despreciable. No es posible que dos mosquitos se electrocuten al mismo tiempo.\nEl número de ocurrencias, electrocuciones de insectos, en un intervalo es independiente del número de electrocuciones en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)· Podemos dividir los 20 insectos promedio entre los \\(n\\) intervalos (trozo de hora) de forma que \\(p_n=\\frac{\\lambda}{n}\\).\nPor ejemplo si \\(n=60\\) tenemos que \\(p_n=\\frac{20}{60}=\\frac{1}{3}\\). La probabilidad de que en un minuto la trampa chisporrotee es \\(\\frac{1}{3}\\).\n\n\nSolución 2. Escribid de forma explicita la función de probabilidad y de distribución de \\(X\\).\nLa distribución de probabilidad de un \\(Po(\\lambda)\\) es\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}  \\frac{\\lambda^x}{x!}e^{-\\lambda} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nEn nuestro caso, \\(\\lambda =20\\):\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}\\frac{20^x}{x!}e^{-20} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nLa función de distribución es\n\\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{\\lambda^i}{i!}\\cdot e^{-\\lambda} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nEn nuestro caso \\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{20^i}{i!}\\cdot e^{-20} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nSolución 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nNos piden la probabilidad siguiente: \\[\nP(X=21)=\\frac{20^{21}}{21!} e^{-20}=0.0846051.\n\\]\nPara realizar el cálculo anterior, podemos usar R como calculadora o usar la función dpois que nos calcula la función de distribución de la variable de Poisson:\n\n20^21/factorial(21)*exp(-20)\n\n[1] 0.08460506\n\ndpois(21,lambda = 20)\n\n[1] 0.08460506\n\n\nSolución 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nNos piden la probabilidad siguiente: \\[\n\\begin{array}{rl}\nP(X\\geq 6)&=1- P(X&lt;6)=1-P(X\\leq 5)=1-F_X(5)=1-\\displaystyle\\sum_{x=0}^{5} \\frac{20^{x}}{x!}\\cdot e^{-20}\\\\\n&=\n1-\\left(\\frac{20^{0}}{0!}\\cdot e^{-20}+\\frac{20^{1}}{1!}\\cdot e^{-20}+\\frac{20^{2}}{2!}\\cdot e^{-20}+\\frac{20^{3}}{3!}\\cdot e^{-20}+\\frac{20^{4}}{4!}\\cdot e^{-20}+\\frac{20^{5}}{5!}\\cdot e^{-20}\\right)\\\\\n&=\n1-e^{-20}\\cdot \\left(1+20+\\frac{400}{4}+\\frac{8000}{6}+\\frac{160000}{24}+\\frac{3200000}{120}\\right)\\\\\n&=\n1-e^{-20} \\cdot \\left(\\frac{1 \\cdot 120+20\\cdot 120+400\\cdot 30+8000\\cdot 20+160000\\cdot 24+3200000\\cdot 1}{120}\\right)\\\\\n&= 1-e^{-20}\\cdot\\left(\\frac{4186520}{120}\\right)=1-7.1908841\\times 10^{-5} =0.9999281.\n\\end{array}\n\\]\nSolución 5.** ¿Cuál es el valor esperado, la varianza y la desviación típica de \\(X\\)?\nEl valor esperado del número de insectos caídos en la trampa en una hora es\n\\[E(X)=\\lambda=20\\]\nSu varianza es \\[Var(X)=\\lambda=20\\]\ny su desviación típica vale \\[\\sqrt{Var(X)}=+\\sqrt{\\lambda}=+\\sqrt{20}=4.47214.\\]\n\n\n\n\n4.6.5 Cálculos Poisson con con R\nConsideremos por ejemplo una v.a. \\(X\\) con distribución \\(Po(\\lambda=3)\\). Calculemos \\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) con R:\n\ndpois(0,lambda = 3)\n\n[1] 0.04978707\n\ndpois(1,lambda = 3)\n\n[1] 0.1493612\n\n\nSi quisiéramos hallar la función de distribución en los mismos valores anteriores, \\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\), haríamos lo siguiente:\n\nppois(0,lambda = 3)\n\n[1] 0.04978707\n\nppois(1,lambda = 3)\n\n[1] 0.1991483\n\ndpois(0,lambda = 3)+dpois(1,lambda = 3) ## es igual a ppois(1,lambda=3)\n\n[1] 0.1991483\n\n\nA continuación, comprobemos que \\(F_X(10)=\\sum\\limits_{x=0}^{10} P_X(x)\\):\n\ndpois(0:10,3)\n\n [1] 0.0497870684 0.1493612051 0.2240418077 0.2240418077 0.1680313557\n [6] 0.1008188134 0.0504094067 0.0216040315 0.0081015118 0.0027005039\n[11] 0.0008101512\n\nsum(dpois(0:10,3))\n\n[1] 0.9997077\n\nppois(10,3)\n\n[1] 0.9997077\n\n\nSi quisiéramos generar una secuencia de \\(100\\) observaciones para una distribución de Poisson de parámetro \\(\\lambda=3\\), \\(Po(3)\\), tendríamos que hacer:\n\nrpois(n=100,lambda = 3)\n\n  [1] 2 5 3 3 2 2 5 2 4 4 2 3 2 2 2 2 2 3 3 5 3 3 2 4 2 3 2 1 1 3 4 6 2 5 3\n [36] 4 1 1 6 3 4 1 4 3 4 3 0 2 1 4 3 0 2 4 2 3 5 2 1 3 3 4 2 5 0 3 1 1 4 6\n [71] 4 5 0 4 0 3 3 3 4 1 2 6 2 2 2 2 1 2 5 2 5 3 7 3 5 2 3 2 1 3\n\n\n\n\nEjemplo: Trampa para insectos (continuación)\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con R a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nRecordemos que la probabilidad pedida es \\(P(X=21)\\):\n\ndpois(21,lambda=20)# P(X=21)\n\n[1] 0.08460506\n\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nRecordemos que la probabilidad pedida es \\(P(X\\geq 6)=1-P(X&lt;6)=1-P(X\\leq 5)\\):\n\nppois(5,lambda=20)\n\n[1] 7.190884e-05\n\n1-ppois(5,lambda=20) # es 1-P(X&lt;=5)=P(X&gt;=6)\n\n[1] 0.9999281\n\nppois(5,lambda=20,lower.tail =FALSE ) # acumula hacia arriba \n\n[1] 0.9999281\n\n# P(X&gt;5)=P(X&gt;=6)=P(X=6)+P(X=7)+...\n\n\n\nlambda=20; par(mfrow=c(1,2)); n=qpois(0.99,lambda=lambda)\naux=rep(0,(n+1)*2); aux[seq(2,(n+1)*2,2)]=dpois(c(0:n),lambda=lambda)\nymax=max(ppois(0:n,lambda=lambda)) \nplot(x=c(0:n),y=dpois(c(0:n),lambda=lambda),\n     ylim=c(0,ymax),xlim=c(-1,n+1),xlab=\"x\", ylab=\"Función de probabilidad\",\n     main=paste0(c(\"Función de probabilidad\\n  Po(lambda=\",lambda,\")\")\n                 collapse = \"\"))\nlines(x=rep(0:n,each=2),y=aux,pch=21, type = \"h\", lty = 2,col=\"blue\")\ncurve(ppois(x,lambda=lambda),\n      xlim=c(-1,n+1),col=\"blue\",ylab=\"Función de Distribución\",\n      main=paste0(c(\"Función de distribución \\n Po(lambda=\",lambda,\")\"),\n                  collapse = \"\"))\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.6 Cálculos Poisson con python\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) en este orden son\n\nfrom scipy.stats import poisson\npoisson.pmf(0,mu = 3)\n\n0.049787068367863944\n\npoisson.pmf(1,mu = 3)\n\n0.14936120510359185\n\n\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\) en este orden son\n\npoisson.cdf(0,mu = 3)\n\n0.04978706836786395\n\npoisson.cdf(1,mu = 3)\n\n0.1991482734714558\n\npoisson.pmf(0,mu = 3)+poisson.pmf(1,mu= 3) \n\n0.1991482734714558\n\n## es igual a poisson.cdf(1,lambda=3)\n\nPor ejemplo podemos comprobar que \\(F_X(10)=\\displaystyle\\sum_{0}^{10} P_X(x)\\)\n\n\npoisson.pmf(range(0,10),mu=3)\n\narray([0.04978707, 0.14936121, 0.22404181, 0.22404181, 0.16803136,\n       0.10081881, 0.05040941, 0.02160403, 0.00810151, 0.0027005 ])\n\nsum(poisson.pmf(range(0,10),mu=3))\n\n0.9988975118698846\n\npoisson.cdf(10,mu=3)\n\n0.9997076630493527\n\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con python a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nLa respuesta a la pregunta 3 es calcular \\(P(X=21)\\)\n\npoisson.pmf(21,mu=20)\n\n0.08460506418293791\n\n# P(X=21)\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nLa pregunta 4 nos pide calcular \\(P(X\\geq 6)=1-P(X\\leq 5)\\)\n\n1-poisson.cdf(5,mu=20) \n\n0.9999280911594716\n\n# es 1-P(X&lt;=5)=P(X&gt;=6)\n\nComo ya hemos visto con scipy.stats podemos pedir los momentos de una variable aleatoria \\(Po(3)\\)\n\npoisson.stats(mu=3, moments='mv')\n\n(array(3.), array(3.))\n\n\nY también generar secuencias de observaciones aleatorias de una población \\(Po(3)\\)\n\npoisson.rvs(mu=3,size=40)\n\narray([1, 4, 1, 5, 3, 3, 0, 0, 3, 5, 7, 3, 2, 2, 4, 3, 3, 2, 2, 2, 2, 3,\n       5, 2, 2, 6, 5, 3, 1, 1, 2, 4, 3, 5, 3, 4, 2, 2, 2, 6], dtype=int64)\n\n\n\n\nfrom scipy.stats import poisson\nmu = 10 # mu = lambda\nx = np.arange(poisson.ppf(0.01, mu),poisson.ppf(0.99, mu))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, poisson.pmf(x, mu), 'bo', ms=5, label='poisson pmf')\nax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks(): \n  tick.label.set_fontsize(5) \n\n\n\n\nax = fig.add_subplot(1,2,2)\nax.plot(x, poisson.cdf(x, mu), 'bo', ms=5, label='poisson cdf')\nax.vlines(x, 0, poisson.cdf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion de Poisson')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo:Número de impactos de insectos en la visera de un casco\n\n\nUn colega de trabajo, al que llamaremos JG, es muy aficionado a los grandes premios de velocidad tanto en coches como en motos.\nComo es tan aficionado está obsesionado con muchas de las más extravagantes estadísticas de estos deportes. En particular le propusimos que estudiara el número de insectos que chocan contra la visera de un casco de un motorista GP o de un conductor de fórmula 1 .\nLa idea es que el número de insectos está igualmente repartido por todo el circuito y de promedio impactan \\(\\lambda&gt;0\\) insectos por minuto. También es razonable suponer que:\n\npodemos dividir la superficie de la visera en cuadrados suficientemente pequeños de forma que la probabilidad de que caigan dos insectos en la misma zona es prácticamente 0.\nla probabilidad de que un insecto impacte en un cuadrado cualquiera de la visera es independiente de cualquier otro cuadrado.\nsi hemos dividido la visera en \\(n\\) cuadrados la probabilidad \\(p_n\\) de impacto de un cuadrado vale \\(p_n=\\frac{\\lambda}{n}\\).\n\nBajo estas condiciones, si denotamos por \\(X_t\\) como el número de insectos que ha impactado en la visera en el intervalo \\((0,t]\\) (en \\(t\\) minutos), podemos afirmar que \\(X_t\\) es un proceso de Poisson \\(Po(\\lambda\\cdot t)\\).\nSupongamos que nos dicen que \\(\\lambda=3\\) insectos por minuto. Entonces el proceso de poisson \\(X_t\\) seguirá un ley \\(Po(3\\cdot t).\\)\nAhora estamos en condiciones de preguntar al proceso de Poisson.\n¿Cuál es la probabilidad de que en 10 minutos impacten más de 25 insectos?\nEn este caso \\(t=10\\) \\(X_{10}\\)= número de insectos que impactan en 10 minutos, el intervalo \\([0,10)\\) que sigue una \\(P(3\\cdot 10=30)\\). Por lo tanto\n\\[P(X&gt;25)=1-P(X\\leq 25)\\]\nlo resolvemos con R\n\n1-ppois(25,lambda=30)\n\n[1] 0.7916426\n\n\nOtra pregunta interesante es que tengamos que esperar más de 2 minutos para observar el primer impacto\n\\[P(X_2=0)=\\frac{(3\\cdot 2)^0}{0!}\\cdot e^{-3\\cdot 2}= e^{-6}=0.002479.\\]\nCon R\n\n6^0/factorial(0)*exp(-6)\n\n[1] 0.002478752\n\nppois(0,lambda=3*2)\n\n[1] 0.002478752\n\n\n\n\n\n\n4.7 Distribución hipergeométrica\nSupongamos que disponemos de una urna de de sorteos que contiene \\(m\\) bolas blancas y \\(n\\) bolas rojas.\nEn total en esta urna hay \\(m+n\\) bolas, \\(m\\) blancas y \\(n\\) rojas. Si extraemos dos bolas de la urna lo podemos hacer de dos formas:\n\nExtraer una anotar su color y reponerla. Sacar otra y anotar su color. Hemos extraído la bola con reposición.\nExtraer simultáneamente dos bolas (sin reposición) y contar el número de bolas blancas.\n\nSea \\(X\\) es la v.a. que cuenta el número de bolas blancas extraídas.\n\nEn el primer caso, \\(X\\) es una \\(B(n=2,p=\\frac{m}{m+n})\\) ya que consiste en repetir dos veces el mismo experimento de Bernoulli.\nEn el segundo caso, \\(X\\) sigue una distribución hipergeométrica que estudiaremos en esta sección.\n\n\n\nDistribución hipergeométrica\n\n\nSean \\(n\\), \\(m\\) y \\(k\\) tres número enteros positivos y tales que \\(k&lt;m+n\\).\nConsideremos una urna que contiene \\(m+n\\) bolas de las que \\(m\\) son blancas y las restantes \\(n\\) no (son no blancas).\nEl número total de bolas es \\(m+n\\). Extraemos de forma aleatoria \\(k\\) bolas de la urna sin reemplazarlas.\nSea \\(X\\) la v.a. que cuenta el número de bolas blancas extraídas. Diremos que la distribución de \\(X\\) es hipergeométrica de parámetros \\(m\\), \\(n\\) y \\(k\\) y la denotaremos por \\(H(m,n,k)\\).\nSu dominio es\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\nFijemos ideas de cómo varía el dominio según los paraámetros \\(m\\), \\(n\\) y \\(k\\).\nPara explicarlo, veamos varios ejemplos:\n\n\\(H(m=5,n=2,k=3)\\). Tenemos \\(m=5\\) bolas blancas, \\(n=2\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas extraídas es \\(1=k-n=3-2\\), ya que sólo hay dos no blancas.\nEn cambio, el máximo si es \\(k=3\\), ya que tenemos bolas blancas de “sobra”.\n\n\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\\(H(m=2,n=5,k=3)\\). Tenemos \\(m=2\\) bolas blancas, \\(n=5\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas es \\(0\\) ya que puedo sacar 3 no blancas.\nEn cambio, el máximo si es \\(m=2\\), ya que aunque saquemos \\(k=3\\) bolas, al llegar a 2 ya hemos extraído todas las bolas blancas de la urna.\n\n\\(H(m=10,n=10,k=3)\\). Tenemos \\(m=10\\) bolas blancas, \\(n=10\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso podemos obtener desde \\(0\\) blancas hasta \\(k=3\\) blancas.\n\n\nSu función de probabilidad es: \\[\nP_{X}(x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}}, & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\}, \\mbox { para  } x\\in \\mathbf{N},\\\\\n0,  & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nObservación: otras parametrizaciones\nEn ocasiones se parametriza una v.a. hipergeométrica mediante \\(N=m+n\\), número total de bolas, \\(k\\), número de extracciones y \\(p\\), probabilidad de extraer una bola blanca.\nAsí podemos parametrizar alternativamente la distribución hipergeométrica así\n\\[H(N,k,p)\\mbox{ donde } p=\\frac{m}{N}.\\]\n\n4.7.1 Resumen distribución Hipergeométrica \\(H(m,n,k)\\).\n\n\nEjemplo clásico urna \\(m=15\\) blancas, \\(n=10\\) rojas y \\(k=3\\) extracciones sin reposición.\n\n\nTenemos una urna con 15 bolas blancas y 10 bolas rojas. Extraemos al azar tres bolas de la urna sin reposición. Sea \\(X\\) el número de bolas blancas extraídas. Bajo esta condiciones, la v.a. \\(X\\) sigue una ley de distribución \\(H(m=15,n=10,k=3)\\).\nSu función de probabilidad es\n\\[\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}} & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\} \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.,\n\\]\n\\[\\mbox{sustituyendo }\\scriptsize{\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{15}{x}\\cdot \\binom{10}{3-x}}{\\binom{25}{3}} & \\mbox{ si }\n0\\leq x \\leq 3 \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.\n}\\]\nLa probabilidad de sacar 2 blancas será\n\\[\nP(X=2)=\\frac{\\binom{15}{2}\\cdot \\binom{10}{3-2}}{\\binom{25}{3}}\n\\]\n\nc(choose(15,2), choose(10,1), choose(25,3))\n\n[1]  105   10 2300\n\n\n\\(P(X=2)=\\frac{105\\cdot10 }{2300}=0.4565217.\\)\nLa probabilidad de que saquemos más de 1 bola blanca es\n\\[\n\\begin{array}{rl}\nP(X&gt; 1)&= 1-P(X\\leq 1)=1-(P(X=0)+P(X=1))\\\\\n&=\n1-\\left(\\frac{\\binom{15}{0}\\cdot \\binom{10}{3}}{\\binom{25}{3}}+\n\\frac{\\binom{15}{1}\\cdot \\binom{10}{2}}{\\binom{25}{3}}\\right)\\\\\n&=\n1-\\left(\n\\frac{1\\cdot120 }{2300}+\\frac{15\\cdot45 }{2300}\n\\right)=1-\\frac{120+15\\cdot 45}{2300}=0.6543478.\n\\end{array}\n\\]\nEl número esperado de bolas blancas extraídas para una v.a. \\(X\\) \\(H(m=15,n=10,k=3)\\) es\n\\[E(X)=\\frac{k\\cdot m}{m+n}=\\frac{3\\cdot 15}{15+10}=\\frac{45}{35}=1.285714.\\]\nLa varianza vale: \\[\n\\begin{array}{rl}\nVar(X)&=k\\cdot\\frac{m}{m+n}\\cdot\\left(1-\\frac{m}{m+n}\\right) \\cdot\\frac{m+n-k}{m+n-1}\\\\\n&=3\\cdot\\frac{15}{15+10}\\cdot\\left(1-\\frac{15}{15+10}\\right) \\cdot\\frac{15+10-3}{15+10-1}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\left(1-\\frac{15}{25}\\right) \\cdot\\frac{22}{24}=\n3\\cdot\\frac{15}{25}\\cdot\\frac{25-15}{25} \\cdot\\frac{22}{24}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\frac{10}{25}\\cdot\\frac{22}{24}=0.66.\n\\end{array}\n\\]\nY por lo tanto su desviación típica es \\(+\\sqrt{Var(X)}=+\\sqrt{0.66}=0.812404.\\)\n\n\n\n\n4.7.2 Cálculos y gráficos distribución hipergeométrica con R\nSea \\(X\\) una v.a. \\(H(m,n,k)\\). La función de R para calcular la función de probabilidad en un valor \\(x\\), \\(P(X=x)\\), es dhyper(x,m,n,k) y para calcular la función de distribución en un valor \\(q\\), \\(P(X\\leq q)\\), es phyper(q,m,n,k). Para generar una muestra de valores que siga la distribución \\(H(m,n,k)\\), hay que usar la función rhyper(nn,m,n,k) donde nn es el número de observaciones aleatorias deseado de la muestra.\nPor ejemplo, si \\(X\\) es una \\(H(m=15,n=10,k=3)\\), los valores de \\(P(X=2)\\) y que \\(P(X&gt;1)=1-P(X\\leq 1)\\) son:\n\ndhyper(x=2,m=15,10,k=3)\n\n[1] 0.4565217\n\nphyper(q=1,m=15,n=10,k=3)# sí, le han puesto q ya veremos el porqué\n\n[1] 0.3456522\n\n1-phyper(q=1,m=15,n=10,k=3)\n\n[1] 0.6543478\n\n\nUna muestra aleatoria de este experimento de tamaño 200 sería:\n\nrhyper(nn=200,m=15,n=10,k=3)\n\n  [1] 2 3 1 3 1 2 2 3 2 2 1 2 1 2 2 3 3 1 1 1 1 0 2 3 2 1 3 2 2 2 2 3 2 3 3\n [36] 2 0 1 2 1 3 2 2 3 2 3 2 2 3 2 3 1 2 2 2 2 3 2 2 1 3 2 2 3 1 2 2 2 2 2\n [71] 3 0 2 0 3 2 2 2 1 2 2 3 1 1 1 2 2 2 2 1 1 3 2 2 3 2 2 1 1 1 3 3 2 2 2\n[106] 1 3 2 2 2 1 1 2 3 2 2 1 2 2 2 2 2 2 3 1 2 3 3 1 1 2 2 1 1 3 2 1 1 2 2\n[141] 3 1 1 1 2 1 1 3 1 2 2 3 3 2 3 1 2 1 2 2 2 1 2 3 1 3 3 3 2 2 1 3 3 1 1\n[176] 2 2 2 2 2 3 2 1 2 1 1 1 1 2 1 1 2 2 2 2 3 3 1 0 2\n\n\n\n\n\n\n\n\n\n\n\nSea \\(X\\) una \\(H(m,n,k)\\), las funciones de scipy.stats cambian los parámetros\n\n\\(M\\) es el número total de bolas. Con nuestra parametrización \\(M=m+n\\).\n\\(n\\) es el número de bolas blancas. Con nuestra parametrización \\(n=m\\).\n\\(N\\) es el número de extracciones. Con nuestra parametrización \\(N=k\\).\n\n\nfrom scipy.stats import hypergeom\n\n\n\n4.7.3 Cálculos y gráficos distribución hipergeométrica con python\n\nhypergeom.pmf(1,M=15+10,n=15,N=3)\n\n0.2934782608695652\n\nhypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.3456521739130434\n\n1-hypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.6543478260869566\n\n\nUna muestra aleatoria de este experimento sería…\n\nhypergeom.rvs(M=15+10,n=15,N=3,size=100)\n\narray([1, 2, 1, 3, 1, 3, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 3, 0, 2,\n       1, 1, 3, 2, 1, 2, 3, 3, 0, 2, 3, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 1,\n       2, 1, 2, 2, 1, 3, 1, 2, 1, 2, 0, 3, 1, 2, 3, 2, 2, 2, 2, 3, 2, 1,\n       3, 2, 2, 3, 2, 1, 1, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2,\n       2, 3, 1, 2, 0, 0, 2, 1, 2, 2, 2, 1], dtype=int64)\n\n\n\n\nfrom scipy.stats import hypergeom\n[M, n, N] = [20, 7, 12] ##20 elementos, 7 del tipo, extraemos 12\nx = np.arange(max(0, N-M+n),min(n, N))\nfig =plt.figure(figsize=(5, 2.7))\n =ax = fig.add_subplot(1,2,1)\n =ax.plot(x, hypergeom.pmf(x, M, n, N), 'bo', ms=5, label='hypergeom pmf')\n =ax.vlines(x, 0, hypergeom.pmf(x, M, n, N), colors='b', lw=2, alpha=0.5)\n =ax.set_ylim([0, max(hypergeom.pmf(x, M, n, N))*1.1])\n\n\n\n\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  =tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\n =ax.plot(x, hypergeom.cdf(x, M, n, N), 'bo', ms=5, label='hypergeom cdf')\n =ax.vlines(x, 0, hypergeom.cdf(x, M, n, N), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\n =fig.suptitle('Distribucion Hipergeometrica')\n =plt.show()",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuciones Notables I</span>"
    ]
  },
  {
    "objectID": "p01_04_1_Notables_discretas.html#distribución-binomial-negativa",
    "href": "p01_04_1_Notables_discretas.html#distribución-binomial-negativa",
    "title": "4  Distribuciones Notables I",
    "section": "4.5 Distribución binomial negativa",
    "text": "4.5 Distribución binomial negativa\n\n\nEl problema de la puerta con dos cerraduras\n\n\nSupongamos que disponemos de 10 llaves distintas y tenemos que abrir una puerta con dos cerraduras.\nComenzamos por la primera cerradura, de tal forma que cada vez olvidamos qué llave hemos probado.\nUna vez abierta la primera cerradura probamos de igual forma con la segunda hasta que también la abrimos.\nSea \\(X=\\) la v.a. que cuenta el número de fracasos hasta abrir la puerta.\nAcertar una llave de la puerta es un experimento Bernoulli con probabilidad de éxito \\(p=0.1\\). Lo repetiremos hasta obtener 2 éxitos.\n\n\n\n\nDistribución binomial negativa\n\n\nEn general tendremos un experimento de Bernoulli con probabilidad de éxito \\(0&lt;p&lt;1\\) tal que:\n\nRepetimos el experimento hasta obtener el \\(n\\)-ésimo éxito ¡¡abrir la maldita puerta!!.\nSea \\(X\\) la v.a. que cuenta el número fallos hasta abrir la puerta, es decir, hasta conseguir el n-ésimo éxito. Notemos que no contamos los éxitos, solo contamos los fracasos.\n\nSi representamos como es habitual un suceso como una cadena de F’s y E’s, para \\(n=2\\), algunos sucesos elementales serán: \\[\\small{\\{EE,FEE,EFE, FFEE,FEFE,EFFE,FFFEE,FFEFE,FEFFE,EFFFE\\}.}\\]\nCalculemos algunas probabilidades para \\(n=2\\): \\[\n\\small{\n\\begin{array}{rl}\nP(X=0) & =P(\\{EE\\})=p^2, \\\\\nP(X=1) & =P(\\{FEE,EFE\\})=2\\cdot (1-p)\\cdot p^2, \\\\\nP(X=2) & =P(\\{FFEE,FEFE,EFFE\\})=3\\cdot (1-p)^2\\cdot p^2, \\\\\nP(X=3) & =P(\\{FFFEE,FFEFE,FEFFE,EFFFE\\})=4\\cdot (1-p)^3\\cdot p^2.\n\\end{array}\n}\n\\]\n\n\nEn general su función de probabilidad es\n\\[\nP_{X}(k)=P(X=k)=\\left\\{\\begin{array}{ll}\n     {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n & \\mbox{si } k=0,1,\\ldots\\\\\n     0 & \\mbox{en otro caso}\\end{array}\\right.\n\\] Una v.a. con este tipo de distribución recibe el nombre de binomial negativa y la denotaremos por \\(BN(n,p)\\).\nNotemos que \\(BN(1,p)=Ge(p)\\).\nDemostración\nJustifiquemos el resultado. Sea \\(X\\) una \\(BN(n,p)\\) y sea \\(k=0,1,2,\\ldots\\).\n\\[\\scriptsize{P(X=k)=P(\\mbox{Todas las cadenas de E's y F' con $k$ F, con $n$ E y acabadas en E})}\\]\n\\[\n\\scriptsize{\\overbrace{\\underbrace{\\overbrace{EFFF\\ldots EEF}^{n-1 \\quad \\mbox{Éxitos}.}}}_{k \\quad\\mbox{Fracasos}}^{k+n-1\\mbox{ posiciones}}E}\n\\]\nDe estas cadenas hay tantas como maneras de elegir de entre las \\(k+n-1\\) primeras posiciones \\(n-1\\) para colocar los éxitos. Esta cantidad es el número binomial \\({k+n-1\\choose n-1}.\\)\n\n\nNúmeros binomiales negativos\n\n\nDados dos enteros positivos \\(n\\) y \\(k\\) se define el número binomial negativo como\n\\[\\binom{-n}{k}=\\frac{(-n)(-n-1)\\cdots (-n-k+1)}{k!}.\\]\n\n\nLos números binomiales negativos generalizan la fórmula de Newton para exponentes negativos; obnetiendose el binomio geralizado de Newton:\n\\[\n(t+1)^{-n}=\\sum_{k=0}^{+\\infty}\\left(\\begin{array}{c} -n\n\\\\ k\\end{array}\\right) t^{k}\n\\]\nR usa la función choose para calcular números binomiales, sean negativos o no. Veámoslo con un ejemplo:\n\\[\n\\begin{array}{rl}\n{-6\\choose 4}&=\\frac{-6\\cdot (-6-1)\\cdot \\cdot (-6-2)\\cdot (-6-3) }{4!}\\\\\n&=  \\frac{-6\\cdot(-7)\\cdot (-8)\\cdot (-9)}{24}\\\\\n&= \\frac{3024}{24}=126.\n\\end{array}\n\\]\nSi realizamos el cálculo con R obtenemos el mismo resultado:\n\nchoose(-6,4)\n\n[1] 126\n\n\nLa esperanza de una \\(BN(n,p)\\) es\n\\[E(X)=\\sum_{k=0}^{+\\infty} k\\cdot {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n=n\\cdot\\frac{1-p}{p}.\\]\nLa esperanza de \\(X^2\\) es\n\\[E(X^2)=\\sum_{k=0}^{+\\infty} k^2\\cdot {k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n=n\\cdot\\frac{1-p}{p^2}+\\left(n\\cdot \\frac{1-p}{p}\\right)^2.\\]\nPor último la varianza es\n\\[\nVar(X)=E(X^2)-E(X)^2=\n\\]\n\\[=n\\cdot \\frac{1-p}{p^2}+\\left(n\\cdot \\frac{1-p}{p}\\right)^2-\\left(n\\cdot \\frac{1-p}{p}\\right)^2=\nn\\cdot \\frac{1-p}{p^2}.\\]\ny por tanto la desviación típica es\n\\[\\sqrt{Var(X)} = \\frac{\\sqrt{n(1-p)}}{p}\\]\n\n4.5.1 Resumen distribución Binomial Negativa \\(BN(n,p)\\)\n\n\nEjercicio: Puerta con dos cerraduras\n\n\nRecordemos nuestra puerta con dos cerraduras que se abren secuencialmente. Tenemos un manojo de 10 llaves casi idénticas de manera que cada vez que probamos una llave olvidamos qué llave hemos usado.\nSea \\(X\\) la v.a que nos da el número de intentos fallidos hasta abrir abrir la puerta.\nEstamos interesado en modelar este problema. La preguntas son:\n\n¿Cuál es la distribución de probabilidad de \\(X\\) la v.a que nos da el número fallos hasta abrir la puerta?\n¿Cuál es la función de probabilidad y de distribución de \\(X\\)?\n¿Cuál es la probabilidad de fallar exactamente 5 veces antes de abrir la puerta?\n¿Cuál es la probabilidad de fallar más de 4?\n¿Cuál es el número esperado de fallos? ¿Y su desviación típica?\n\nSolución 1. ¿Cuál es la distribución de probabilidad de \\(X\\) la v.a que nos da el número fallos hasta abrir la puerta?\nBajo estados condiciones tenemos que la probabilidad de “éxito” de cada intento es \\(p=\\frac{1}{10}=0.1\\). Como cada vez olvidamos qué llave hemos probado, cada intento será independiente del anterior.\nAsí que la variable \\(X\\) que queremos modelar cuenta el número fallos de repeticiones sucesivas e independientes de un experimento \\(Ber(p=0.1)\\) hasta conseguir 2 éxitos en un experimento.\nPor lo tanto podemos asegurar que \\(X\\) sigue un distribución \\(BN(n=2,p=0.1).\\)\nSolución 2. ¿Cuál es la función de probabilidad y de distribución del \\(X\\)?\nEn general la función de probabilidad de una \\(BN(n,p)\\) es\n\\[\nP_X(k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n{k+n-1\\choose n-1} \\cdot (1-p)^{k}\\cdot p^n & \\mbox{si }  k=0,1,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSi aplicamos la expresión anterior para \\(n=2\\) y \\(p=0.1\\), obtenemos: \\[\nP_X(k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n{k+2-1\\choose 2-1} \\cdot 0.9^{k}\\cdot 0.1^2 & \\mbox{si }  k=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSimplificando\n\\[\nP_X(X=k)=P(X=k)=\n\\left\\{\n\\begin{array}{cc}\n0.01\\cdot (k+1)\\cdot 0.9^{k}, & \\mbox{si }  k=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nLa función de distribución en general es\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0 \\\\\n\\displaystyle\\sum_{i=0}^{k }{i+n-1\\choose n-1} \\cdot (1-p)^{i+n-1}\\cdot p^n\n& \\mbox{si }\\left\\{\\begin{array}{l} k\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\\end{array}\\right.\n\\end{array}\n\\right.\n\\]\nSimplificando para \\(n=2\\), \\(p=0.1\\).\n\\[\nF_X(x)=P(X\\leq x)=\n\\left\\{\n\\begin{array}{ll}\n0, & \\mbox{si } x&lt;0, \\\\\n\\displaystyle\\sum_{i=0}^{k }0.01\\cdot (i+1) \\cdot 0.9^{i+1},\n& \\mbox{si }\\left\\{\\begin{array}{l} k\\leq x&lt; k+1,\\\\k=0,1,2,\\ldots\\end{array}\\right.\n\\end{array}\n\\right.\n\\]\nSolución 3. ¿Cuál es la probabilidad de fallar exactamente 5 veces antes de abrir la puerta?\n\\[\nP(X=5)= 0.01\\cdot (5+1) \\cdot 0.9^{5}= 0.06 \\cdot 0.9^{5}= 0.0354294.\n\\]\nSolución 4. ¿Cuál es la probabilidad de fallar más de 4?\nNos piden que\n\\[\nP(X&gt;4)=1-P(X\\leq 4).\n\\]\nCalculemos primero \\(P(X\\leq 4):\\)\n\\[\n\\begin{array}{rl}\nP(X\\leq 4) &=  \\displaystyle\\sum_{x=0}^{4} P(X=x)=P(X=0)+P(X=1)+P(X=2)+P(X=3)+P(X=4)\\\\\n&= 0.01\\cdot (0+1) \\cdot 0.9^{0}+0.01\\cdot (1+1) \\cdot 0.9^{1}+0.01\\cdot (2+1) \\cdot 0.9^{2} \\\\ &\\ \\\n+0.01\\cdot (3+1) \\cdot 0.9^{3} + 0.01\\cdot (4+1) \\cdot 0.9^{4} \\\\ & =\n0.01 +0.018+0.0243+0.02916+0.032805 = 0.114265.\n\\end{array}\n\\]\nPor lo tanto\n\\[\nP(X&gt;4)=1-P(X\\leq 4)=1-0.114265=\n0.885735.\n\\]\nSolución 5. ¿Cuál es el número esperado de fallos? ¿Y su desviación típica?\nComo \\(X\\) sigue una ley \\(BN(n=2,p=0.1)\\)\n\\[E(X)=n\\cdot \\frac{1-p}{p}=2\\cdot \\frac{1-0.1}{0.1}=18.\\]\nEl número de fallos esperado es 18. La varianza es\n\\[\nVar(X)=n\\cdot\\frac{1-p}{p^2}=2 \\cdot \\frac{1-0.1}{0.1^2}=180,\n\\]\ny su desviación típica \\(\\sqrt{180}=13.41641.\\)\n\n4.5.2 Cálculos binomial negativa con R\nLa función de R que calcula la función de probabilidad de la binomial negativa con sus parámetros básicos es:\ndnbinom(x, size, prob,...)\ndonde size (\\(n\\)) es el número de éxitos y prob (\\(p\\)), la probabilidad de éxito.\nAsí en el ejemplo de la puerta con dos cerraduras, \\(X\\) es una \\(BN(n=size=2,p=prob=0.1)\\). Por ejemplo, \\(P(X=5)\\) que hemos calculado en el ejemplo anterior, vale:\n\ndnbinom(5,size=2,p=0.1)\n\n[1] 0.0354294\n\n\nDe forma similar calculamos calculamos \\(P(X\\leq 4)\\), \\(P(X&gt;4)=1-P(X\\leq 4)\\) y \\(P(X&gt;4)\\).\n\npnbinom(4,size=2,p=0.1)\n\n[1] 0.114265\n\n1-pnbinom(4,size=2,p=0.1)\n\n[1] 0.885735\n\npnbinom(4,size=2,p=0.1,lower.tail=FALSE)\n\n[1] 0.885735\n\n\n\n\n4.5.3 Cálculos binomial negativa con python\nLa función con python es nbinom.pmf(k, n, p, loc). Hay que cargarla desde scpi.stats\n\nfrom scipy.stats import nbinom\n\nRecordemos que de nuevo se cumple que\n\nnbinom.pmf(k, n, p, loc) = nbinom.pmf(k-loc, n, p)`\n\nCálculos \\(BN(n,p)\\) con python\n\nnbinom.pmf(k=5,n=2,p=0.1)\n\n0.0354294\n\nnbinom.pmf(k=5,n=2,p=0.1,loc=0)\n\n0.0354294\n\nnbinom.cdf(k=4,n=2,p=0.1)\n\n0.11426500000000002\n\n1-nbinom.cdf(k=4,n=2,p=0.1)\n\n0.8857349999999999\n\n\nGeneremos 100 observaciones aleatorias de una \\(BN(n=2,0.1)\\). Es decir serán las veces que hemos fallado hasta abrir la puerta 100 veces.\n\nnbinom.rvs(n=2, p=0.1, size=100)\n\narray([ 9, 34, 38, 38, 34, 45, 45, 20, 20,  9,  6, 15, 16, 26, 21, 37,  6,\n       15,  1, 21, 10, 16, 14, 22,  2, 22, 25, 22, 18, 36, 17, 34, 13, 35,\n       18, 31,  6, 32, 19, 16,  4,  8,  1,  1, 24, 20,  6, 12,  6, 30, 18,\n       27, 18,  5, 18, 24, 21, 22, 16,  8, 12, 11, 18, 16, 18, 26,  6, 32,\n       21, 41, 28, 14,  6,  8, 23, 18, 20,  9,  3, 14, 42, 13, 21,  0, 15,\n       12, 17, 26, 23,  8, 19,  6, 10,  1, 18,  7,  6, 14, 15,  9],\n      dtype=int64)\n\n\nLa esperanza y la varianzade una \\(BN(n=2,0.1)\\) valen:\n\nn, p=2,0.1\nparams = nbinom.stats(n,p,moments='mv')\nprint(\"E(X)={m}\".format(m=params[0]))\n\nE(X)=18.0\n\nprint(\"Var(X)={v}\".format(v=params[1]))\n\nVar(X)=179.99999999999997\n\n\n\n\n4.5.4 Gráficas de la binomial negativa con R\nEl siguiente código de R dibuja las función de probabilidad y la de distribución de una \\(BN(n=2,p=0.1)\\)\n\n\npar(mfrow=c(1,2))\naux=rep(0,22)\naux[seq(2,22,2)]=dnbinom(c(0:10),size=2,prob=0.1)\nplot(x=c(0:10),y=dnbinom(c(0:10),size=2,prob=0.1),\n  ylim=c(0,1),xlim=c(-1,11),xlab=\"x\",\n  main=\"Función de probabilidad\\n BN(n=2,p=0.1)\")\nlines(x=rep(0:10,each=2),y=aux, type = \"h\", lty = 2,col=\"blue\")\ncurve(pnbinom(x,size=2,prob=0,1),\n  xlim=c(-1,11),col=\"blue\",\n  main=\"Función de distribución\\n BN(n=2,p=0.1)\")\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\nEjercicio\nBuscad en los manuales de python cómo se dibuja la función de probabilidad y de distribución de una binomial negativa.\nNecesitamos de nuevo más librerías\n\nimport numpy as np\nfrom scipy.stats import nbinom\nimport matplotlib.pyplot as plt\n\n\n\n4.5.5 Gráficos de la binomial negativa con python\n\n\nn, p = 10, 0.25\nx = np.arange(0,nbinom.ppf(0.99, n, p))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, nbinom.pmf(x, n, p), 'bo', ms=5, label='nbinom pmf')\nax.vlines(x, 0, nbinom.pmf(x, n, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\nax.plot(x, nbinom.cdf(x, n, p), 'bo', ms=5, label='nbinom pmf')\nax.vlines(x, 0, nbinom.cdf(x, n, p), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion Binomial Negativa')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5.6 Un ejemplo de la binomial negativa\n\n\nEjemplo: Sistema con tres claves de acceso\n\n\nSupongamos que tenemos un sistema informático tiene un programa de seguridad que genera accesos con claves de 3 dígitos \\(000,001,\\ldots 999\\). En total 1000 posibilidades.\nComo una clave de tres dígitos es fácil de romper proponemos considerar tres claves consecutivas de acceso al sistema, cada una de 3 dígitos.\nPara acceder al sistema hay que dar las tres claves de forma consecutiva y por orden.\nEs decir hasta que no averiguamos la primera clave no pasamos a la segunda clave.\nSupongamos que cada vez que ponemos las dos claves olvidamos el resultado y seguimos poniendo claves al azar hasta adivinar la contraseña.\nAsí hasta conseguir entrar en el sistema.\nSea \\(X\\) la v.a que nos da el número de fallos antes de entrar en el sistema.\nEstamos interesados en modelar este problema. La preguntas son:\n\n¿Cuál es la distribución de probabilidad de \\(X\\), la v.a que nos da el número de fallos antes de acceder al sistema.\n¿Cuál es la función de probabilidad y de distribución del \\(X\\)?\n¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\n¿Cuál es la probabilidad de fallar más de 150 veces antes de entrar en el sistema?\n¿Cuál es el número esperado de fallos antes de acceder al sistema? ¿Y su varianza?\n\nSolución 1. ¿Cuál es la distribución de probabilidad de \\(X\\), la v.a que nos da el número de fallos antes de acceder al sistema?\nBajo estados condiciones tenemos que la probabilidad de “éxito” de cada intento es \\(p=\\frac{1}{1000}=0.001\\). Y como cada vez olvidamos en los dígitos cada intento será independiente del anterior.\nAsí que la variable \\(X\\) cuenta el número de fracasos independientes hasta conseguir 3 éxitos en un experimento \\(Ber(p=0.001)\\) por lo tanto \\(X\\) sigue un distribución \\(BN(n=3,p=0.001).\\)\nSolución 2. ¿Cuál es la función de probabilidad y de distribución del \\(X\\)\nEn general la función de probabilidad de una \\(BN(n,p)\\) es\n\\[\nP_X(X=x)=P(X=x)=\n\\left\\{\n\\begin{array}{cc}\n{x+n-1\\choose n-1} \\cdot (1-p)^{x}\\cdot p^n & \\mbox{si }  x=0,1,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\] En particular la función de probabilidad de una \\(BN(n=3,p=0.001)\\) es\n\\[\nP_X(X=x)=P(X=x)=\n\\left\\{\n\\begin{array}{cc}\n{x+2\\choose 2} \\cdot 0.999^{x}\\cdot 0.001^3 & \\mbox{si }  x=0,1,2,\\ldots \\\\ 0 & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nSolución 3. ¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\nNos piden\n\\[\n\\scriptsize{P(X=150)= {152\\choose 2} \\cdot 0.999^{150}\\cdot 0.001^3.}\n\\]\nLo calcularemos operando con R\n\nchoose(152,2)*0.999^150*0.001^3\n\n[1] 9.876743e-06\n\n\n\ndnbinom(150,size=3,p=0.001)\n\n[1] 9.876743e-06\n\n\nSolución 3. ¿Cuál es la probabilidad de fallar 150 veces antes de acceder en el sistema?\nNos piden, lo resolveremos con python\n\\[\nP(X=150)= {152\\choose 2} \\cdot 0.999^{150}\\cdot 0.001^3\n\\]\n\nfrom  scipy.special import binom\nbinom(152,2)*0.999**150*0.001**3\n\n9.876743459670526e-06\n\nnbinom.pmf(150,n=3,p=0.001)\n\n9.876743459670532e-06\n\n\nSolución 4. ¿Cuál es la probabilidad de fallar más de 150 veces antes de entrar en el sistema?\n\\[P(X&gt;150)=1-P(X\\leq 150)\\]\nCalculemos \\(P(X\\leq 150)\\)\n\\[\\begin{eqnarray*}\nP(X\\leq 150) &=& P(X=0)+P(X=1)+P(X=2)+\\ldots+P(X=150)\\\\\n&=& \\sum_{k=0}^{150} {k+3-1\\choose 3-1} \\cdot (0.999)^{k}\\cdot 0.001^3\\ldots = \\ldots =5.2320035\\times 10^{-4}\n\\end{eqnarray*}\\]\nCon R\n\npnbinom(150,3,0.001)\n\n[1] 0.0005232003\n\n\nCon python\n\nnbinom.cdf(150,n=3,p=0.001)\n\n0.0005232003490824064\n\n\nEl valor pedido será pues: \\[\nP(X&gt;150)=1-P(X\\leq 150)=1-5.2320035\\times 10^{-4}=0.9994768.\n\\] Vemos que es muy probable que fallemos más de 150 veces antes de entrar en el sistema.\nSolución 5. ¿Cuál es el número esperado de fallos antes de acceder al sistema? ¿Y su desviación típica?\nTenemos que \\(E(X)=n\\cdot \\frac{1-p}{p}=3\\cdot \\frac{1- 0.001}{0.001}=2997\\) y \\(Var(X)=n\\cdot \\frac{1-p}{p^2}=3\\cdot \\frac{1- 0.001^2}{0.001^2}=2.997\\times 10^{6}.\\)\nCon python\n\nparams = nbinom.stats(n=3,p=0.001,moments='mv')\nprint(\"E(X) = {m}\".format(m=params[0]))\n\nE(X) = 2997.0\n\nprint(\"Var(X) = {v}\".format(v=params[1]))\n\nVar(X) = 2997000.0\n\n\n\n\n\n\nEjemplo: ¿Tres claves de tres dígitos o una de 9 dígitos?\n\n\nSupongamos que ponemos una sola clave de 9 dígitos. Estudiemos en este caso la variable aleatoria que da el número de fallos antes de entrar en el sistema y comparemos los resultados.\nSi seguimos suponiendo que cada vez ponemos la contraseña al azar pero esta vez con una clave de 9 dígitos. La probabilidad de éxito será ahora \\(p=\\frac{1}{10^{9}}\\).\nSi llamamos \\(X_9\\) a la variable aleatoria que nos da el número de fallos antes de entra en el sistema seguirá una distribución \\(Ge(p=\\frac{1}{10^9}=0.000000001)\\).\n¿Qué da más seguridad? ¿tres claves de tres dígitos o una de 9 dígitos?\nSu valor esperado es\n\\[\nE(X_9)=\\frac{1-p}{p}=\\frac{1-0.000000001}{0.000000001}=10\\times 10^{8}.\n\\]\n\\(1000 000 000\\) son 1000 millones de fallos esperados hasta abrir la puerta.\nRecordemos que con tres contraseñas de 3 dígitos el valor esperado de fallos es\n\\[3\\cdot \\frac{1-0.001}{0.001}=2997.\\]\nPor lo tanto, desde el punto de vista de la seguridad, es mejor una clave larga de 9 dígitos que tres cortas si escribimos las contraseñas al azar.\n\n\n\n\n4.6 Distribución Poisson\nDefiniremos formamente la distribución de Poisson dadndo su dominio y función de probabilidad\n\n\nDefinición: Distribución de Poisson\n\n\nDiremos que una v.a. discreta \\(X\\) con \\(X(\\Omega)=\\mathbf{N}\\) tiene distribución de Poisson con parámetro \\(\\lambda&gt;0\\), y lo denotaremos por \\(Po(\\lambda)\\) si su función de probabilidad es:\n\\[\nP_{X}(x)=P(X=x)=\n\\left\\{\\begin{array}{ll}\n\\frac{\\lambda^x}{x!} e^{-\\lambda}& \\mbox{ si } x=0,1,\\ldots\\\\\n0 & \\mbox{en otro caso}\\end{array}\\right..\n\\]\n\n\nUsando que el desarrollo en serie de Taylor de la función exponencial es \\[\ne^{\\lambda}=\\sum_{x=0}^{+\\infty} \\frac{\\lambda^x}{x!},\n\\] es fácil comprobar que la suma de la función de probabilidad en todos los valores del dominio de \\(X\\), o sea, los enteros positivos, vale 1.\nAdemás recordemos que dado \\(x\\in\\mathbb{R}-\\{0\\}\\) se tiene que\n\\[\n\\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n=e^x.\n\\]\nUsando la expresión anterior para \\(x=-\\lambda\\), tenemos:\n\\[\n\\lim_{n\\to\\infty} \\left(1-\\frac{\\lambda}{n}\\right)^n=\\lim_{n\\to\\infty} \\left(1+\\frac{-\\lambda}{n}\\right)^n=e^{-\\lambda}.\n\\]\n\n4.6.1 La distribución de Poisson como “límite” de una binomial.\nLa distribución de Poisson (Siméon Denis Poisson) aparece en el conteo de determinados eventos que se producen en un intervalo de tiempo o en el espacio.\nSupongamos que nuestra variable de interés es \\(X\\), el número de eventos en el intervalo de tiempo \\((0,t]\\), como por ejemplo el número de llamadas a un call center en una hora donde suponemos que se cumplen las siguientes condiciones:\n\nEl número promedio de eventos en el intervalo \\((0,t]\\) es \\(\\lambda&gt;0\\).\nEs posible dividir el intervalo de tiempo en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más eventos en un subintervalo es despreciable.\nEl número de ocurrencias de eventos en un intervalo es independiente del número de ocurrencias en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)·\n\n\nBajo estas condiciones, podemos considerar que el número de eventos en el intervalo \\((0,t]\\) será el número de “éxitos” en \\(n\\) repeticiones independientes de un proceso Bernoulli de parámetro \\(p_n\\)\nEntonces si \\(n\\to\\infty\\) y \\(p_n\\cdot n\\) se mantiene igual a \\(\\lambda\\) resulta que la función de probabilidad de \\(X\\) se puede escribir como\n\\[\n\\begin{array}{rl}\nP(X_n=k)&=\\left(\\begin{array}{c} n\\\\ k\\end{array}\\right) \\cdot p_n^k\\cdot  (1-p_n)^{n-k}\n\\\\\n&= {n\\choose k}\\cdot \\left(\\frac{\\lambda}{n}\\right)^{k}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\\n&=\n\\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k}\\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\end{array}\n\\]\nSi hacemos tender \\(n\\) hacia \\(\\infty\\), obtenemos: \\[\n\\lim_{n\\to \\infty} P(X_n=k) = \\lim_{n\\to \\infty} \\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k} \\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\]\nCalculemos el límite de algunos de los factores de la expresión\n\\[\n\\displaystyle\\lim_{n\\to \\infty}\\frac{n!}{(n-k)!\\cdot n^k}= \\lim_{n\\to \\infty}\\frac{n\\cdot (n-1)\\cdots (n-k-1)}{n^k}\n=\\lim_{n\\to \\infty}\\frac{n^{k}+\\cdots}{n^k}=1.\n\\]\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n}=e^{-\\lambda}\n\\]\nY también teniendo en cuenta que \\(k\\) es constante.\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{-k}=\\lim_{n\\to \\infty} 1^{-k}=\\lim_{n\\to \\infty}  1=1.\n\\]\nPara acabar\n\\[\n\\displaystyle\\lim_{n\\to\\infty} P(X_n=k)=\n\\lim_{n\\to\\infty} \\left(\\begin{array}{c} n\\\\ k\\end{array}\\right)\n\\cdot p_n^k \\cdot (1-p_n)^{n-k}= \\frac{\\lambda^k}{k!}\\cdot 1 \\cdot e^{-\\lambda}\\cdot 1=\\frac{\\lambda^k}{k!}\\cdot e^{-\\lambda}.\n\\]\nLo que confirma que límite de una serie de variables \\(B(n,p_n=\\frac{\\lambda}{n})\\) sigue una ley \\(Po(\\lambda)\\).\n\n\n4.6.2 Procesos de Poisson\nLo interesante de las variables Poisson es que podemos modificar (si el modelo lo permite) el intervalo de tiempo \\((0,t]\\) en el que contamos los eventos.\nClaro que esto no tiene que poder ser así.\nPero en general si la variable es poisson en \\((0,t]\\) también lo será en cualquier subintervalo \\((0,t']\\) para todo \\(t'\\) tal que \\(0&lt;t'&lt;t\\).\nAsí que podremos definir una serie de variables \\(X_t\\) de distribución \\(Po(\\lambda\\cdot t)\\).\n\n\nDefinición: Procesos de Poisson\n\n\nConsideremos un experimento Poisson con \\(\\lambda\\) igual al promedio de eventos en una unidad de tiempo (u.t.).\nSi \\(t\\) es una cantidad de tiempo en u.t., la v.a. \\(X_{t}\\)=numero de eventos en el intervalo \\((0,t]\\) es una \\(Po(\\lambda\\cdot t)\\).\nEl conjunto de variables \\(\\{X_t\\}_{t&gt;0}\\) recibe el nombre de proceso de Poisson.\n\n\n\n\n4.6.3 Resumen distribución Poisson \\(X\\sim Po(\\lambda)\\)\n\n\n\n4.6.4 Aproximación de la distribución binomial por la Poisson\nBajo el punto de vista anterior y si \\(p\\) es pequeño y \\(n\\) suficientemente grande la distribución \\(B(n,p)\\) se aproxima a una \\(Po(\\lambda=n\\cdot p)\\).\nExisten distintos criterios (ninguno perfecto) de cuando la aproximación es buena.\nPor ejemplo si\n\\[n\\geq 20\\mbox{ o mejor }n\\geq 30, n\\cdot p &lt; 10 \\mbox{ y } p\\leq 0.05,\\]\nla aproximación de una \\(B(n,p)\\) por una \\(Po(n\\cdot p)\\) es buena. Sobre todo para los valores cercanos a \\(E(X)=\\lambda\\).\nCondición deseable \\(n\\geq 20\\), \\(n\\cdot p &lt; 10\\), \\(p\\leq 0.05\\).\n\n\nEjemplo: Trampa insectos.\n\n\nLa conocida lámpara antiinsectos o insecticida eléctrico atrae a los insectos voladores con una luz ultravioleta y los mata por electrocución.\nConsideremos la v.a. \\(X\\) que cuenta el número de insectos caídos en la trampa en una hora. Supongamos que el número promedio de insectos que captura la trampa en una hora es \\(E(X)=20\\) y que podemos admitir que \\(X\\) sigue una ley de probabilidad \\(Po(\\lambda=20)\\).\nNos piden\n\nComentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\nEscribir de forma explicita la función de probabilidad y de distribución de \\(X\\).\nCalculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nCalculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\n¿Cuál es el valor esperando, la varianza y la desviación típica de \\(X\\)?\n\nSolución 1. Comentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\n\nEl número promedio de eventos en el intervalo \\((0,1]\\), una hora es \\(\\lambda=20&gt;0\\).\nEs posible dividir el intervalo de tiempo de una hora en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más electrocuciones un subintervalo es despreciable. No es posible que dos mosquitos se electrocuten al mismo tiempo.\nEl número de ocurrencias, electrocuciones de insectos, en un intervalo es independiente del número de electrocuciones en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)· Podemos dividir los 20 insectos promedio entre los \\(n\\) intervalos (trozo de hora) de forma que \\(p_n=\\frac{\\lambda}{n}\\).\nPor ejemplo si \\(n=60\\) tenemos que \\(p_n=\\frac{20}{60}=\\frac{1}{3}\\). La probabilidad de que en un minuto la trampa chisporrotee es \\(\\frac{1}{3}\\).\n\n\nSolución 2. Escribid de forma explicita la función de probabilidad y de distribución de \\(X\\).\nLa distribución de probabilidad de un \\(Po(\\lambda)\\) es\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}  \\frac{\\lambda^x}{x!}e^{-\\lambda} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nEn nuestro caso, \\(\\lambda =20\\):\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}\\frac{20^x}{x!}e^{-20} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nLa función de distribución es\n\\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{\\lambda^i}{i!}\\cdot e^{-\\lambda} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nEn nuestro caso \\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{20^i}{i!}\\cdot e^{-20} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nSolución 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nNos piden la probabilidad siguiente: \\[\nP(X=21)=\\frac{20^{21}}{21!} e^{-20}=0.0846051.\n\\]\nPara realizar el cálculo anterior, podemos usar R como calculadora o usar la función dpois que nos calcula la función de distribución de la variable de Poisson:\n\n20^21/factorial(21)*exp(-20)\n\n[1] 0.08460506\n\ndpois(21,lambda = 20)\n\n[1] 0.08460506\n\n\nSolución 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nNos piden la probabilidad siguiente: \\[\n\\begin{array}{rl}\nP(X\\geq 6)&=1- P(X&lt;6)=1-P(X\\leq 5)=1-F_X(5)=1-\\displaystyle\\sum_{x=0}^{5} \\frac{20^{x}}{x!}\\cdot e^{-20}\\\\\n&=\n1-\\left(\\frac{20^{0}}{0!}\\cdot e^{-20}+\\frac{20^{1}}{1!}\\cdot e^{-20}+\\frac{20^{2}}{2!}\\cdot e^{-20}+\\frac{20^{3}}{3!}\\cdot e^{-20}+\\frac{20^{4}}{4!}\\cdot e^{-20}+\\frac{20^{5}}{5!}\\cdot e^{-20}\\right)\\\\\n&=\n1-e^{-20}\\cdot \\left(1+20+\\frac{400}{4}+\\frac{8000}{6}+\\frac{160000}{24}+\\frac{3200000}{120}\\right)\\\\\n&=\n1-e^{-20} \\cdot \\left(\\frac{1 \\cdot 120+20\\cdot 120+400\\cdot 30+8000\\cdot 20+160000\\cdot 24+3200000\\cdot 1}{120}\\right)\\\\\n&= 1-e^{-20}\\cdot\\left(\\frac{4186520}{120}\\right)=1-7.1908841\\times 10^{-5} =0.9999281.\n\\end{array}\n\\]\nSolución 5.** ¿Cuál es el valor esperado, la varianza y la desviación típica de \\(X\\)?\nEl valor esperado del número de insectos caídos en la trampa en una hora es\n\\[E(X)=\\lambda=20\\]\nSu varianza es \\[Var(X)=\\lambda=20\\]\ny su desviación típica vale \\[\\sqrt{Var(X)}=+\\sqrt{\\lambda}=+\\sqrt{20}=4.47214.\\]\n\n\n\n\n4.6.5 Cálculos Poisson con con R\nConsideremos por ejemplo una v.a. \\(X\\) con distribución \\(Po(\\lambda=3)\\). Calculemos \\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) con R:\n\ndpois(0,lambda = 3)\n\n[1] 0.04978707\n\ndpois(1,lambda = 3)\n\n[1] 0.1493612\n\n\nSi quisiéramos hallar la función de distribución en los mismos valores anteriores, \\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\), haríamos lo siguiente:\n\nppois(0,lambda = 3)\n\n[1] 0.04978707\n\nppois(1,lambda = 3)\n\n[1] 0.1991483\n\ndpois(0,lambda = 3)+dpois(1,lambda = 3) ## es igual a ppois(1,lambda=3)\n\n[1] 0.1991483\n\n\nA continuación, comprobemos que \\(F_X(10)=\\sum\\limits_{x=0}^{10} P_X(x)\\):\n\ndpois(0:10,3)\n\n [1] 0.0497870684 0.1493612051 0.2240418077 0.2240418077 0.1680313557\n [6] 0.1008188134 0.0504094067 0.0216040315 0.0081015118 0.0027005039\n[11] 0.0008101512\n\nsum(dpois(0:10,3))\n\n[1] 0.9997077\n\nppois(10,3)\n\n[1] 0.9997077\n\n\nSi quisiéramos generar una secuencia de \\(100\\) observaciones para una distribución de Poisson de parámetro \\(\\lambda=3\\), \\(Po(3)\\), tendríamos que hacer:\n\nrpois(n=100,lambda = 3)\n\n  [1] 2 5 3 3 2 2 5 2 4 4 2 3 2 2 2 2 2 3 3 5 3 3 2 4 2 3 2 1 1 3 4 6 2 5 3\n [36] 4 1 1 6 3 4 1 4 3 4 3 0 2 1 4 3 0 2 4 2 3 5 2 1 3 3 4 2 5 0 3 1 1 4 6\n [71] 4 5 0 4 0 3 3 3 4 1 2 6 2 2 2 2 1 2 5 2 5 3 7 3 5 2 3 2 1 3\n\n\n\n\nEjemplo: Trampa para insectos (continuación)\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con R a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nRecordemos que la probabilidad pedida es \\(P(X=21)\\):\n\ndpois(21,lambda=20)# P(X=21)\n\n[1] 0.08460506\n\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nRecordemos que la probabilidad pedida es \\(P(X\\geq 6)=1-P(X&lt;6)=1-P(X\\leq 5)\\):\n\nppois(5,lambda=20)\n\n[1] 7.190884e-05\n\n1-ppois(5,lambda=20) # es 1-P(X&lt;=5)=P(X&gt;=6)\n\n[1] 0.9999281\n\nppois(5,lambda=20,lower.tail =FALSE ) # acumula hacia arriba \n\n[1] 0.9999281\n\n# P(X&gt;5)=P(X&gt;=6)=P(X=6)+P(X=7)+...\n\n\n\nlambda=20; par(mfrow=c(1,2)); n=qpois(0.99,lambda=lambda)\naux=rep(0,(n+1)*2); aux[seq(2,(n+1)*2,2)]=dpois(c(0:n),lambda=lambda)\nymax=max(ppois(0:n,lambda=lambda)) \nplot(x=c(0:n),y=dpois(c(0:n),lambda=lambda),\n     ylim=c(0,ymax),xlim=c(-1,n+1),xlab=\"x\", ylab=\"Función de probabilidad\",\n     main=paste0(c(\"Función de probabilidad\\n  Po(lambda=\",lambda,\")\")\n                 collapse = \"\"))\nlines(x=rep(0:n,each=2),y=aux,pch=21, type = \"h\", lty = 2,col=\"blue\")\ncurve(ppois(x,lambda=lambda),\n      xlim=c(-1,n+1),col=\"blue\",ylab=\"Función de Distribución\",\n      main=paste0(c(\"Función de distribución \\n Po(lambda=\",lambda,\")\"),\n                  collapse = \"\"))\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.6 Cálculos Poisson con python\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) en este orden son\n\nfrom scipy.stats import poisson\npoisson.pmf(0,mu = 3)\n\n0.049787068367863944\n\npoisson.pmf(1,mu = 3)\n\n0.14936120510359185\n\n\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\) en este orden son\n\npoisson.cdf(0,mu = 3)\n\n0.04978706836786395\n\npoisson.cdf(1,mu = 3)\n\n0.1991482734714558\n\npoisson.pmf(0,mu = 3)+poisson.pmf(1,mu= 3) \n\n0.1991482734714558\n\n## es igual a poisson.cdf(1,lambda=3)\n\nPor ejemplo podemos comprobar que \\(F_X(10)=\\displaystyle\\sum_{0}^{10} P_X(x)\\)\n\n\npoisson.pmf(range(0,10),mu=3)\n\narray([0.04978707, 0.14936121, 0.22404181, 0.22404181, 0.16803136,\n       0.10081881, 0.05040941, 0.02160403, 0.00810151, 0.0027005 ])\n\nsum(poisson.pmf(range(0,10),mu=3))\n\n0.9988975118698846\n\npoisson.cdf(10,mu=3)\n\n0.9997076630493527\n\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con python a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nLa respuesta a la pregunta 3 es calcular \\(P(X=21)\\)\n\npoisson.pmf(21,mu=20)\n\n0.08460506418293791\n\n# P(X=21)\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nLa pregunta 4 nos pide calcular \\(P(X\\geq 6)=1-P(X\\leq 5)\\)\n\n1-poisson.cdf(5,mu=20) \n\n0.9999280911594716\n\n# es 1-P(X&lt;=5)=P(X&gt;=6)\n\nComo ya hemos visto con scipy.stats podemos pedir los momentos de una variable aleatoria \\(Po(3)\\)\n\npoisson.stats(mu=3, moments='mv')\n\n(array(3.), array(3.))\n\n\nY también generar secuencias de observaciones aleatorias de una población \\(Po(3)\\)\n\npoisson.rvs(mu=3,size=40)\n\narray([1, 4, 1, 5, 3, 3, 0, 0, 3, 5, 7, 3, 2, 2, 4, 3, 3, 2, 2, 2, 2, 3,\n       5, 2, 2, 6, 5, 3, 1, 1, 2, 4, 3, 5, 3, 4, 2, 2, 2, 6], dtype=int64)\n\n\n\n\nfrom scipy.stats import poisson\nmu = 10 # mu = lambda\nx = np.arange(poisson.ppf(0.01, mu),poisson.ppf(0.99, mu))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, poisson.pmf(x, mu), 'bo', ms=5, label='poisson pmf')\nax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks(): \n  tick.label.set_fontsize(5) \n\n\n\n\nax = fig.add_subplot(1,2,2)\nax.plot(x, poisson.cdf(x, mu), 'bo', ms=5, label='poisson cdf')\nax.vlines(x, 0, poisson.cdf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion de Poisson')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo:Número de impactos de insectos en la visera de un casco\n\n\nUn colega de trabajo, al que llamaremos JG, es muy aficionado a los grandes premios de velocidad tanto en coches como en motos.\nComo es tan aficionado está obsesionado con muchas de las más extravagantes estadísticas de estos deportes. En particular le propusimos que estudiara el número de insectos que chocan contra la visera de un casco de un motorista GP o de un conductor de fórmula 1 .\nLa idea es que el número de insectos está igualmente repartido por todo el circuito y de promedio impactan \\(\\lambda&gt;0\\) insectos por minuto. También es razonable suponer que:\n\npodemos dividir la superficie de la visera en cuadrados suficientemente pequeños de forma que la probabilidad de que caigan dos insectos en la misma zona es prácticamente 0.\nla probabilidad de que un insecto impacte en un cuadrado cualquiera de la visera es independiente de cualquier otro cuadrado.\nsi hemos dividido la visera en \\(n\\) cuadrados la probabilidad \\(p_n\\) de impacto de un cuadrado vale \\(p_n=\\frac{\\lambda}{n}\\).\n\nBajo estas condiciones, si denotamos por \\(X_t\\) como el número de insectos que ha impactado en la visera en el intervalo \\((0,t]\\) (en \\(t\\) minutos), podemos afirmar que \\(X_t\\) es un proceso de Poisson \\(Po(\\lambda\\cdot t)\\).\nSupongamos que nos dicen que \\(\\lambda=3\\) insectos por minuto. Entonces el proceso de poisson \\(X_t\\) seguirá un ley \\(Po(3\\cdot t).\\)\nAhora estamos en condiciones de preguntar al proceso de Poisson.\n¿Cuál es la probabilidad de que en 10 minutos impacten más de 25 insectos?\nEn este caso \\(t=10\\) \\(X_{10}\\)= número de insectos que impactan en 10 minutos, el intervalo \\([0,10)\\) que sigue una \\(P(3\\cdot 10=30)\\). Por lo tanto\n\\[P(X&gt;25)=1-P(X\\leq 25)\\]\nlo resolvemos con R\n\n1-ppois(25,lambda=30)\n\n[1] 0.7916426\n\n\nOtra pregunta interesante es que tengamos que esperar más de 2 minutos para observar el primer impacto\n\\[P(X_2=0)=\\frac{(3\\cdot 2)^0}{0!}\\cdot e^{-3\\cdot 2}= e^{-6}=0.002479.\\]\nCon R\n\n6^0/factorial(0)*exp(-6)\n\n[1] 0.002478752\n\nppois(0,lambda=3*2)\n\n[1] 0.002478752\n\n\n\n\n\n\n4.7 Distribución hipergeométrica\nSupongamos que disponemos de una urna de de sorteos que contiene \\(m\\) bolas blancas y \\(n\\) bolas rojas.\nEn total en esta urna hay \\(m+n\\) bolas, \\(m\\) blancas y \\(n\\) rojas. Si extraemos dos bolas de la urna lo podemos hacer de dos formas:\n\nExtraer una anotar su color y reponerla. Sacar otra y anotar su color. Hemos extraído la bola con reposición.\nExtraer simultáneamente dos bolas (sin reposición) y contar el número de bolas blancas.\n\nSea \\(X\\) es la v.a. que cuenta el número de bolas blancas extraídas.\n\nEn el primer caso, \\(X\\) es una \\(B(n=2,p=\\frac{m}{m+n})\\) ya que consiste en repetir dos veces el mismo experimento de Bernoulli.\nEn el segundo caso, \\(X\\) sigue una distribución hipergeométrica que estudiaremos en esta sección.\n\n\n\nDistribución hipergeométrica\n\n\nSean \\(n\\), \\(m\\) y \\(k\\) tres número enteros positivos y tales que \\(k&lt;m+n\\).\nConsideremos una urna que contiene \\(m+n\\) bolas de las que \\(m\\) son blancas y las restantes \\(n\\) no (son no blancas).\nEl número total de bolas es \\(m+n\\). Extraemos de forma aleatoria \\(k\\) bolas de la urna sin reemplazarlas.\nSea \\(X\\) la v.a. que cuenta el número de bolas blancas extraídas. Diremos que la distribución de \\(X\\) es hipergeométrica de parámetros \\(m\\), \\(n\\) y \\(k\\) y la denotaremos por \\(H(m,n,k)\\).\nSu dominio es\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\nFijemos ideas de cómo varía el dominio según los paraámetros \\(m\\), \\(n\\) y \\(k\\).\nPara explicarlo, veamos varios ejemplos:\n\n\\(H(m=5,n=2,k=3)\\). Tenemos \\(m=5\\) bolas blancas, \\(n=2\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas extraídas es \\(1=k-n=3-2\\), ya que sólo hay dos no blancas.\nEn cambio, el máximo si es \\(k=3\\), ya que tenemos bolas blancas de “sobra”.\n\n\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\\(H(m=2,n=5,k=3)\\). Tenemos \\(m=2\\) bolas blancas, \\(n=5\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas es \\(0\\) ya que puedo sacar 3 no blancas.\nEn cambio, el máximo si es \\(m=2\\), ya que aunque saquemos \\(k=3\\) bolas, al llegar a 2 ya hemos extraído todas las bolas blancas de la urna.\n\n\\(H(m=10,n=10,k=3)\\). Tenemos \\(m=10\\) bolas blancas, \\(n=10\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso podemos obtener desde \\(0\\) blancas hasta \\(k=3\\) blancas.\n\n\nSu función de probabilidad es: \\[\nP_{X}(x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}}, & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\}, \\mbox { para  } x\\in \\mathbf{N},\\\\\n0,  & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nObservación: otras parametrizaciones\nEn ocasiones se parametriza una v.a. hipergeométrica mediante \\(N=m+n\\), número total de bolas, \\(k\\), número de extracciones y \\(p\\), probabilidad de extraer una bola blanca.\nAsí podemos parametrizar alternativamente la distribución hipergeométrica así\n\\[H(N,k,p)\\mbox{ donde } p=\\frac{m}{N}.\\]\n\n4.7.1 Resumen distribución Hipergeométrica \\(H(m,n,k)\\).\n\n\nEjemplo clásico urna \\(m=15\\) blancas, \\(n=10\\) rojas y \\(k=3\\) extracciones sin reposición.\n\n\nTenemos una urna con 15 bolas blancas y 10 bolas rojas. Extraemos al azar tres bolas de la urna sin reposición. Sea \\(X\\) el número de bolas blancas extraídas. Bajo esta condiciones, la v.a. \\(X\\) sigue una ley de distribución \\(H(m=15,n=10,k=3)\\).\nSu función de probabilidad es\n\\[\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}} & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\} \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.,\n\\]\n\\[\\mbox{sustituyendo }\\scriptsize{\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{15}{x}\\cdot \\binom{10}{3-x}}{\\binom{25}{3}} & \\mbox{ si }\n0\\leq x \\leq 3 \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.\n}\\]\nLa probabilidad de sacar 2 blancas será\n\\[\nP(X=2)=\\frac{\\binom{15}{2}\\cdot \\binom{10}{3-2}}{\\binom{25}{3}}\n\\]\n\nc(choose(15,2), choose(10,1), choose(25,3))\n\n[1]  105   10 2300\n\n\n\\(P(X=2)=\\frac{105\\cdot10 }{2300}=0.4565217.\\)\nLa probabilidad de que saquemos más de 1 bola blanca es\n\\[\n\\begin{array}{rl}\nP(X&gt; 1)&= 1-P(X\\leq 1)=1-(P(X=0)+P(X=1))\\\\\n&=\n1-\\left(\\frac{\\binom{15}{0}\\cdot \\binom{10}{3}}{\\binom{25}{3}}+\n\\frac{\\binom{15}{1}\\cdot \\binom{10}{2}}{\\binom{25}{3}}\\right)\\\\\n&=\n1-\\left(\n\\frac{1\\cdot120 }{2300}+\\frac{15\\cdot45 }{2300}\n\\right)=1-\\frac{120+15\\cdot 45}{2300}=0.6543478.\n\\end{array}\n\\]\nEl número esperado de bolas blancas extraídas para una v.a. \\(X\\) \\(H(m=15,n=10,k=3)\\) es\n\\[E(X)=\\frac{k\\cdot m}{m+n}=\\frac{3\\cdot 15}{15+10}=\\frac{45}{35}=1.285714.\\]\nLa varianza vale: \\[\n\\begin{array}{rl}\nVar(X)&=k\\cdot\\frac{m}{m+n}\\cdot\\left(1-\\frac{m}{m+n}\\right) \\cdot\\frac{m+n-k}{m+n-1}\\\\\n&=3\\cdot\\frac{15}{15+10}\\cdot\\left(1-\\frac{15}{15+10}\\right) \\cdot\\frac{15+10-3}{15+10-1}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\left(1-\\frac{15}{25}\\right) \\cdot\\frac{22}{24}=\n3\\cdot\\frac{15}{25}\\cdot\\frac{25-15}{25} \\cdot\\frac{22}{24}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\frac{10}{25}\\cdot\\frac{22}{24}=0.66.\n\\end{array}\n\\]\nY por lo tanto su desviación típica es \\(+\\sqrt{Var(X)}=+\\sqrt{0.66}=0.812404.\\)\n\n\n\n\n4.7.2 Cálculos y gráficos distribución hipergeométrica con R\nSea \\(X\\) una v.a. \\(H(m,n,k)\\). La función de R para calcular la función de probabilidad en un valor \\(x\\), \\(P(X=x)\\), es dhyper(x,m,n,k) y para calcular la función de distribución en un valor \\(q\\), \\(P(X\\leq q)\\), es phyper(q,m,n,k). Para generar una muestra de valores que siga la distribución \\(H(m,n,k)\\), hay que usar la función rhyper(nn,m,n,k) donde nn es el número de observaciones aleatorias deseado de la muestra.\nPor ejemplo, si \\(X\\) es una \\(H(m=15,n=10,k=3)\\), los valores de \\(P(X=2)\\) y que \\(P(X&gt;1)=1-P(X\\leq 1)\\) son:\n\ndhyper(x=2,m=15,10,k=3)\n\n[1] 0.4565217\n\nphyper(q=1,m=15,n=10,k=3)# sí, le han puesto q ya veremos el porqué\n\n[1] 0.3456522\n\n1-phyper(q=1,m=15,n=10,k=3)\n\n[1] 0.6543478\n\n\nUna muestra aleatoria de este experimento de tamaño 200 sería:\n\nrhyper(nn=200,m=15,n=10,k=3)\n\n  [1] 2 3 1 3 1 2 2 3 2 2 1 2 1 2 2 3 3 1 1 1 1 0 2 3 2 1 3 2 2 2 2 3 2 3 3\n [36] 2 0 1 2 1 3 2 2 3 2 3 2 2 3 2 3 1 2 2 2 2 3 2 2 1 3 2 2 3 1 2 2 2 2 2\n [71] 3 0 2 0 3 2 2 2 1 2 2 3 1 1 1 2 2 2 2 1 1 3 2 2 3 2 2 1 1 1 3 3 2 2 2\n[106] 1 3 2 2 2 1 1 2 3 2 2 1 2 2 2 2 2 2 3 1 2 3 3 1 1 2 2 1 1 3 2 1 1 2 2\n[141] 3 1 1 1 2 1 1 3 1 2 2 3 3 2 3 1 2 1 2 2 2 1 2 3 1 3 3 3 2 2 1 3 3 1 1\n[176] 2 2 2 2 2 3 2 1 2 1 1 1 1 2 1 1 2 2 2 2 3 3 1 0 2\n\n\n\n\n\n\n\n\n\n\n\nSea \\(X\\) una \\(H(m,n,k)\\), las funciones de scipy.stats cambian los parámetros\n\n\\(M\\) es el número total de bolas. Con nuestra parametrización \\(M=m+n\\).\n\\(n\\) es el número de bolas blancas. Con nuestra parametrización \\(n=m\\).\n\\(N\\) es el número de extracciones. Con nuestra parametrización \\(N=k\\).\n\n\nfrom scipy.stats import hypergeom\n\n\n\n4.7.3 Cálculos y gráficos distribución hipergeométrica con python\n\nhypergeom.pmf(1,M=15+10,n=15,N=3)\n\n0.2934782608695652\n\nhypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.3456521739130434\n\n1-hypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.6543478260869566\n\n\nUna muestra aleatoria de este experimento sería…\n\nhypergeom.rvs(M=15+10,n=15,N=3,size=100)\n\narray([1, 2, 1, 3, 1, 3, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 3, 0, 2,\n       1, 1, 3, 2, 1, 2, 3, 3, 0, 2, 3, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 1,\n       2, 1, 2, 2, 1, 3, 1, 2, 1, 2, 0, 3, 1, 2, 3, 2, 2, 2, 2, 3, 2, 1,\n       3, 2, 2, 3, 2, 1, 1, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2,\n       2, 3, 1, 2, 0, 0, 2, 1, 2, 2, 2, 1], dtype=int64)\n\n\n\n\nfrom scipy.stats import hypergeom\n[M, n, N] = [20, 7, 12] ##20 elementos, 7 del tipo, extraemos 12\nx = np.arange(max(0, N-M+n),min(n, N))\nfig =plt.figure(figsize=(5, 2.7))\n =ax = fig.add_subplot(1,2,1)\n =ax.plot(x, hypergeom.pmf(x, M, n, N), 'bo', ms=5, label='hypergeom pmf')\n =ax.vlines(x, 0, hypergeom.pmf(x, M, n, N), colors='b', lw=2, alpha=0.5)\n =ax.set_ylim([0, max(hypergeom.pmf(x, M, n, N))*1.1])\n\n\n\n\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  =tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\n =ax.plot(x, hypergeom.cdf(x, M, n, N), 'bo', ms=5, label='hypergeom cdf')\n =ax.vlines(x, 0, hypergeom.cdf(x, M, n, N), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\n =fig.suptitle('Distribucion Hipergeometrica')\n =plt.show()",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuciones Notables I</span>"
    ]
  },
  {
    "objectID": "p01_04_1_Notables_discretas.html#distribución-poisson",
    "href": "p01_04_1_Notables_discretas.html#distribución-poisson",
    "title": "4  Distribuciones Notables I",
    "section": "4.6 Distribución Poisson",
    "text": "4.6 Distribución Poisson\nDefiniremos formamente la distribución de Poisson dadndo su dominio y función de probabilidad\n\n\nDefinición: Distribución de Poisson\n\n\nDiremos que una v.a. discreta \\(X\\) con \\(X(\\Omega)=\\mathbf{N}\\) tiene distribución de Poisson con parámetro \\(\\lambda&gt;0\\), y lo denotaremos por \\(Po(\\lambda)\\) si su función de probabilidad es:\n\\[\nP_{X}(x)=P(X=x)=\n\\left\\{\\begin{array}{ll}\n\\frac{\\lambda^x}{x!} e^{-\\lambda}& \\mbox{ si } x=0,1,\\ldots\\\\\n0 & \\mbox{en otro caso}\\end{array}\\right..\n\\]\n\n\nUsando que el desarrollo en serie de Taylor de la función exponencial es \\[\ne^{\\lambda}=\\sum_{x=0}^{+\\infty} \\frac{\\lambda^x}{x!},\n\\] es fácil comprobar que la suma de la función de probabilidad en todos los valores del dominio de \\(X\\), o sea, los enteros positivos, vale 1.\nAdemás recordemos que dado \\(x\\in\\mathbb{R}-\\{0\\}\\) se tiene que\n\\[\n\\lim_{n\\to\\infty} \\left(1+\\frac{x}{n}\\right)^n=e^x.\n\\]\nUsando la expresión anterior para \\(x=-\\lambda\\), tenemos:\n\\[\n\\lim_{n\\to\\infty} \\left(1-\\frac{\\lambda}{n}\\right)^n=\\lim_{n\\to\\infty} \\left(1+\\frac{-\\lambda}{n}\\right)^n=e^{-\\lambda}.\n\\]\n\n4.6.1 La distribución de Poisson como “límite” de una binomial.\nLa distribución de Poisson (Siméon Denis Poisson) aparece en el conteo de determinados eventos que se producen en un intervalo de tiempo o en el espacio.\nSupongamos que nuestra variable de interés es \\(X\\), el número de eventos en el intervalo de tiempo \\((0,t]\\), como por ejemplo el número de llamadas a un call center en una hora donde suponemos que se cumplen las siguientes condiciones:\n\nEl número promedio de eventos en el intervalo \\((0,t]\\) es \\(\\lambda&gt;0\\).\nEs posible dividir el intervalo de tiempo en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más eventos en un subintervalo es despreciable.\nEl número de ocurrencias de eventos en un intervalo es independiente del número de ocurrencias en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)·\n\n\nBajo estas condiciones, podemos considerar que el número de eventos en el intervalo \\((0,t]\\) será el número de “éxitos” en \\(n\\) repeticiones independientes de un proceso Bernoulli de parámetro \\(p_n\\)\nEntonces si \\(n\\to\\infty\\) y \\(p_n\\cdot n\\) se mantiene igual a \\(\\lambda\\) resulta que la función de probabilidad de \\(X\\) se puede escribir como\n\\[\n\\begin{array}{rl}\nP(X_n=k)&=\\left(\\begin{array}{c} n\\\\ k\\end{array}\\right) \\cdot p_n^k\\cdot  (1-p_n)^{n-k}\n\\\\\n&= {n\\choose k}\\cdot \\left(\\frac{\\lambda}{n}\\right)^{k}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{n-k}\\\\\n&=\n\\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k}\\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\end{array}\n\\]\nSi hacemos tender \\(n\\) hacia \\(\\infty\\), obtenemos: \\[\n\\lim_{n\\to \\infty} P(X_n=k) = \\lim_{n\\to \\infty} \\frac{\\lambda^k}{k!}\\cdot\\frac{n!}{(n-k)!\\cdot n^k} \\cdot\n\\left(1-\\frac{\\lambda}{n}\\right)^{n}\\cdot \\left(1-\\frac{\\lambda}{n}\\right)^{-k}.\n\\]\nCalculemos el límite de algunos de los factores de la expresión\n\\[\n\\displaystyle\\lim_{n\\to \\infty}\\frac{n!}{(n-k)!\\cdot n^k}= \\lim_{n\\to \\infty}\\frac{n\\cdot (n-1)\\cdots (n-k-1)}{n^k}\n=\\lim_{n\\to \\infty}\\frac{n^{k}+\\cdots}{n^k}=1.\n\\]\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{n}=e^{-\\lambda}\n\\]\nY también teniendo en cuenta que \\(k\\) es constante.\n\\[\n\\lim_{n\\to \\infty} \\left(1-\\frac{\\lambda}{n}\\right)^{-k}=\\lim_{n\\to \\infty} 1^{-k}=\\lim_{n\\to \\infty}  1=1.\n\\]\nPara acabar\n\\[\n\\displaystyle\\lim_{n\\to\\infty} P(X_n=k)=\n\\lim_{n\\to\\infty} \\left(\\begin{array}{c} n\\\\ k\\end{array}\\right)\n\\cdot p_n^k \\cdot (1-p_n)^{n-k}= \\frac{\\lambda^k}{k!}\\cdot 1 \\cdot e^{-\\lambda}\\cdot 1=\\frac{\\lambda^k}{k!}\\cdot e^{-\\lambda}.\n\\]\nLo que confirma que límite de una serie de variables \\(B(n,p_n=\\frac{\\lambda}{n})\\) sigue una ley \\(Po(\\lambda)\\).\n\n\n4.6.2 Procesos de Poisson\nLo interesante de las variables Poisson es que podemos modificar (si el modelo lo permite) el intervalo de tiempo \\((0,t]\\) en el que contamos los eventos.\nClaro que esto no tiene que poder ser así.\nPero en general si la variable es poisson en \\((0,t]\\) también lo será en cualquier subintervalo \\((0,t']\\) para todo \\(t'\\) tal que \\(0&lt;t'&lt;t\\).\nAsí que podremos definir una serie de variables \\(X_t\\) de distribución \\(Po(\\lambda\\cdot t)\\).\n\n\nDefinición: Procesos de Poisson\n\n\nConsideremos un experimento Poisson con \\(\\lambda\\) igual al promedio de eventos en una unidad de tiempo (u.t.).\nSi \\(t\\) es una cantidad de tiempo en u.t., la v.a. \\(X_{t}\\)=numero de eventos en el intervalo \\((0,t]\\) es una \\(Po(\\lambda\\cdot t)\\).\nEl conjunto de variables \\(\\{X_t\\}_{t&gt;0}\\) recibe el nombre de proceso de Poisson.\n\n\n\n\n4.6.3 Resumen distribución Poisson \\(X\\sim Po(\\lambda)\\)\n\n\n\n4.6.4 Aproximación de la distribución binomial por la Poisson\nBajo el punto de vista anterior y si \\(p\\) es pequeño y \\(n\\) suficientemente grande la distribución \\(B(n,p)\\) se aproxima a una \\(Po(\\lambda=n\\cdot p)\\).\nExisten distintos criterios (ninguno perfecto) de cuando la aproximación es buena.\nPor ejemplo si\n\\[n\\geq 20\\mbox{ o mejor }n\\geq 30, n\\cdot p &lt; 10 \\mbox{ y } p\\leq 0.05,\\]\nla aproximación de una \\(B(n,p)\\) por una \\(Po(n\\cdot p)\\) es buena. Sobre todo para los valores cercanos a \\(E(X)=\\lambda\\).\nCondición deseable \\(n\\geq 20\\), \\(n\\cdot p &lt; 10\\), \\(p\\leq 0.05\\).\n\n\nEjemplo: Trampa insectos.\n\n\nLa conocida lámpara antiinsectos o insecticida eléctrico atrae a los insectos voladores con una luz ultravioleta y los mata por electrocución.\nConsideremos la v.a. \\(X\\) que cuenta el número de insectos caídos en la trampa en una hora. Supongamos que el número promedio de insectos que captura la trampa en una hora es \\(E(X)=20\\) y que podemos admitir que \\(X\\) sigue una ley de probabilidad \\(Po(\\lambda=20)\\).\nNos piden\n\nComentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\nEscribir de forma explicita la función de probabilidad y de distribución de \\(X\\).\nCalculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nCalculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\n¿Cuál es el valor esperando, la varianza y la desviación típica de \\(X\\)?\n\nSolución 1. Comentar de forma breve si se cumplen intuitivamente las condiciones para tener una distribución Poisson.\n\nEl número promedio de eventos en el intervalo \\((0,1]\\), una hora es \\(\\lambda=20&gt;0\\).\nEs posible dividir el intervalo de tiempo de una hora en un gran número de subintervalos (denotemos por \\(n\\) al número de intervalos) de forma que:\n\nLa probabilidad de que se produzcan dos o más electrocuciones un subintervalo es despreciable. No es posible que dos mosquitos se electrocuten al mismo tiempo.\nEl número de ocurrencias, electrocuciones de insectos, en un intervalo es independiente del número de electrocuciones en otro intervalo.\nLa probabilidad de que un evento ocurra en un subintervalo es \\(p_n=\\frac{\\lambda}{n}\\)· Podemos dividir los 20 insectos promedio entre los \\(n\\) intervalos (trozo de hora) de forma que \\(p_n=\\frac{\\lambda}{n}\\).\nPor ejemplo si \\(n=60\\) tenemos que \\(p_n=\\frac{20}{60}=\\frac{1}{3}\\). La probabilidad de que en un minuto la trampa chisporrotee es \\(\\frac{1}{3}\\).\n\n\nSolución 2. Escribid de forma explicita la función de probabilidad y de distribución de \\(X\\).\nLa distribución de probabilidad de un \\(Po(\\lambda)\\) es\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}  \\frac{\\lambda^x}{x!}e^{-\\lambda} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nEn nuestro caso, \\(\\lambda =20\\):\n\\[\nP_X(x)=P(X=x)=\\left\\{\\begin{array}{ll}\\frac{20^x}{x!}e^{-20} & \\mbox{ si } x=0,1,\\ldots\\\\ 0  & \\mbox{ en otro caso.}\\end{array}\\right.\n\\]\nLa función de distribución es\n\\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{\\lambda^i}{i!}\\cdot e^{-\\lambda} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nEn nuestro caso \\[\nF_X(x)=P(X\\leq X)=\n\\left\\{\\begin{array}{ll}\n0 & \\mbox{si } x&lt;0\\\\\n\\displaystyle\\sum_{i=0}^{k} P(X=i)=\\sum_{i=0}^{k}\\frac{20^i}{i!}\\cdot e^{-20} & \\mbox{si  }\n\\left\\{\\begin{array}{l}\nk\\leq x&lt; k+1\\\\k=0,1,2,\\ldots\n\\end{array}\n\\right.\n\\end{array}\n\\right.\n\\]\nSolución 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nNos piden la probabilidad siguiente: \\[\nP(X=21)=\\frac{20^{21}}{21!} e^{-20}=0.0846051.\n\\]\nPara realizar el cálculo anterior, podemos usar R como calculadora o usar la función dpois que nos calcula la función de distribución de la variable de Poisson:\n\n20^21/factorial(21)*exp(-20)\n\n[1] 0.08460506\n\ndpois(21,lambda = 20)\n\n[1] 0.08460506\n\n\nSolución 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nNos piden la probabilidad siguiente: \\[\n\\begin{array}{rl}\nP(X\\geq 6)&=1- P(X&lt;6)=1-P(X\\leq 5)=1-F_X(5)=1-\\displaystyle\\sum_{x=0}^{5} \\frac{20^{x}}{x!}\\cdot e^{-20}\\\\\n&=\n1-\\left(\\frac{20^{0}}{0!}\\cdot e^{-20}+\\frac{20^{1}}{1!}\\cdot e^{-20}+\\frac{20^{2}}{2!}\\cdot e^{-20}+\\frac{20^{3}}{3!}\\cdot e^{-20}+\\frac{20^{4}}{4!}\\cdot e^{-20}+\\frac{20^{5}}{5!}\\cdot e^{-20}\\right)\\\\\n&=\n1-e^{-20}\\cdot \\left(1+20+\\frac{400}{4}+\\frac{8000}{6}+\\frac{160000}{24}+\\frac{3200000}{120}\\right)\\\\\n&=\n1-e^{-20} \\cdot \\left(\\frac{1 \\cdot 120+20\\cdot 120+400\\cdot 30+8000\\cdot 20+160000\\cdot 24+3200000\\cdot 1}{120}\\right)\\\\\n&= 1-e^{-20}\\cdot\\left(\\frac{4186520}{120}\\right)=1-7.1908841\\times 10^{-5} =0.9999281.\n\\end{array}\n\\]\nSolución 5.** ¿Cuál es el valor esperado, la varianza y la desviación típica de \\(X\\)?\nEl valor esperado del número de insectos caídos en la trampa en una hora es\n\\[E(X)=\\lambda=20\\]\nSu varianza es \\[Var(X)=\\lambda=20\\]\ny su desviación típica vale \\[\\sqrt{Var(X)}=+\\sqrt{\\lambda}=+\\sqrt{20}=4.47214.\\]\n\n\n\n\n4.6.5 Cálculos Poisson con con R\nConsideremos por ejemplo una v.a. \\(X\\) con distribución \\(Po(\\lambda=3)\\). Calculemos \\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) con R:\n\ndpois(0,lambda = 3)\n\n[1] 0.04978707\n\ndpois(1,lambda = 3)\n\n[1] 0.1493612\n\n\nSi quisiéramos hallar la función de distribución en los mismos valores anteriores, \\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\), haríamos lo siguiente:\n\nppois(0,lambda = 3)\n\n[1] 0.04978707\n\nppois(1,lambda = 3)\n\n[1] 0.1991483\n\ndpois(0,lambda = 3)+dpois(1,lambda = 3) ## es igual a ppois(1,lambda=3)\n\n[1] 0.1991483\n\n\nA continuación, comprobemos que \\(F_X(10)=\\sum\\limits_{x=0}^{10} P_X(x)\\):\n\ndpois(0:10,3)\n\n [1] 0.0497870684 0.1493612051 0.2240418077 0.2240418077 0.1680313557\n [6] 0.1008188134 0.0504094067 0.0216040315 0.0081015118 0.0027005039\n[11] 0.0008101512\n\nsum(dpois(0:10,3))\n\n[1] 0.9997077\n\nppois(10,3)\n\n[1] 0.9997077\n\n\nSi quisiéramos generar una secuencia de \\(100\\) observaciones para una distribución de Poisson de parámetro \\(\\lambda=3\\), \\(Po(3)\\), tendríamos que hacer:\n\nrpois(n=100,lambda = 3)\n\n  [1] 2 5 3 3 2 2 5 2 4 4 2 3 2 2 2 2 2 3 3 5 3 3 2 4 2 3 2 1 1 3 4 6 2 5 3\n [36] 4 1 1 6 3 4 1 4 3 4 3 0 2 1 4 3 0 2 4 2 3 5 2 1 3 3 4 2 5 0 3 1 1 4 6\n [71] 4 5 0 4 0 3 3 3 4 1 2 6 2 2 2 2 1 2 5 2 5 3 7 3 5 2 3 2 1 3\n\n\n\n\nEjemplo: Trampa para insectos (continuación)\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con R a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nRecordemos que la probabilidad pedida es \\(P(X=21)\\):\n\ndpois(21,lambda=20)# P(X=21)\n\n[1] 0.08460506\n\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nRecordemos que la probabilidad pedida es \\(P(X\\geq 6)=1-P(X&lt;6)=1-P(X\\leq 5)\\):\n\nppois(5,lambda=20)\n\n[1] 7.190884e-05\n\n1-ppois(5,lambda=20) # es 1-P(X&lt;=5)=P(X&gt;=6)\n\n[1] 0.9999281\n\nppois(5,lambda=20,lower.tail =FALSE ) # acumula hacia arriba \n\n[1] 0.9999281\n\n# P(X&gt;5)=P(X&gt;=6)=P(X=6)+P(X=7)+...\n\n\n\nlambda=20; par(mfrow=c(1,2)); n=qpois(0.99,lambda=lambda)\naux=rep(0,(n+1)*2); aux[seq(2,(n+1)*2,2)]=dpois(c(0:n),lambda=lambda)\nymax=max(ppois(0:n,lambda=lambda)) \nplot(x=c(0:n),y=dpois(c(0:n),lambda=lambda),\n     ylim=c(0,ymax),xlim=c(-1,n+1),xlab=\"x\", ylab=\"Función de probabilidad\",\n     main=paste0(c(\"Función de probabilidad\\n  Po(lambda=\",lambda,\")\")\n                 collapse = \"\"))\nlines(x=rep(0:n,each=2),y=aux,pch=21, type = \"h\", lty = 2,col=\"blue\")\ncurve(ppois(x,lambda=lambda),\n      xlim=c(-1,n+1),col=\"blue\",ylab=\"Función de Distribución\",\n      main=paste0(c(\"Función de distribución \\n Po(lambda=\",lambda,\")\"),\n                  collapse = \"\"))\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n\n4.6.6 Cálculos Poisson con python\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(P_X(0)=P(X=0), P_X(1)=P(X=1)\\) en este orden son\n\nfrom scipy.stats import poisson\npoisson.pmf(0,mu = 3)\n\n0.049787068367863944\n\npoisson.pmf(1,mu = 3)\n\n0.14936120510359185\n\n\nSea \\(X\\) un una v.a. \\(Po(\\lambda=3)\\). Entonces\n\\(F_X(0)=P(X\\leq 0), F_X(1)=P(X\\leq 1)\\) en este orden son\n\npoisson.cdf(0,mu = 3)\n\n0.04978706836786395\n\npoisson.cdf(1,mu = 3)\n\n0.1991482734714558\n\npoisson.pmf(0,mu = 3)+poisson.pmf(1,mu= 3) \n\n0.1991482734714558\n\n## es igual a poisson.cdf(1,lambda=3)\n\nPor ejemplo podemos comprobar que \\(F_X(10)=\\displaystyle\\sum_{0}^{10} P_X(x)\\)\n\n\npoisson.pmf(range(0,10),mu=3)\n\narray([0.04978707, 0.14936121, 0.22404181, 0.22404181, 0.16803136,\n       0.10081881, 0.05040941, 0.02160403, 0.00810151, 0.0027005 ])\n\nsum(poisson.pmf(range(0,10),mu=3))\n\n0.9988975118698846\n\npoisson.cdf(10,mu=3)\n\n0.9997076630493527\n\n\n\nEn el ejercicio de la trampa para insectos teníamos que \\(X\\) es una \\(Po(20)\\). Responded con python a la preguntas 3 y 4 de este ejercicio\nPregunta 3. Calculad la probabilidad de que en una hora caigan en la trampa exactamente 21 insectos.\nLa respuesta a la pregunta 3 es calcular \\(P(X=21)\\)\n\npoisson.pmf(21,mu=20)\n\n0.08460506418293791\n\n# P(X=21)\n\nPregunta 4. Calculad la probabilidad de que en una hora caigan en la trampa al menos 6 insectos.\nLa pregunta 4 nos pide calcular \\(P(X\\geq 6)=1-P(X\\leq 5)\\)\n\n1-poisson.cdf(5,mu=20) \n\n0.9999280911594716\n\n# es 1-P(X&lt;=5)=P(X&gt;=6)\n\nComo ya hemos visto con scipy.stats podemos pedir los momentos de una variable aleatoria \\(Po(3)\\)\n\npoisson.stats(mu=3, moments='mv')\n\n(array(3.), array(3.))\n\n\nY también generar secuencias de observaciones aleatorias de una población \\(Po(3)\\)\n\npoisson.rvs(mu=3,size=40)\n\narray([1, 4, 1, 5, 3, 3, 0, 0, 3, 5, 7, 3, 2, 2, 4, 3, 3, 2, 2, 2, 2, 3,\n       5, 2, 2, 6, 5, 3, 1, 1, 2, 4, 3, 5, 3, 4, 2, 2, 2, 6], dtype=int64)\n\n\n\n\nfrom scipy.stats import poisson\nmu = 10 # mu = lambda\nx = np.arange(poisson.ppf(0.01, mu),poisson.ppf(0.99, mu))\nfig =plt.figure(figsize=(5, 2.7))\nax = fig.add_subplot(1,2,1)\nax.plot(x, poisson.pmf(x, mu), 'bo', ms=5, label='poisson pmf')\nax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks(): \n  tick.label.set_fontsize(5) \n\n\n\n\nax = fig.add_subplot(1,2,2)\nax.plot(x, poisson.cdf(x, mu), 'bo', ms=5, label='poisson cdf')\nax.vlines(x, 0, poisson.cdf(x, mu), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  tick.label.set_fontsize(5)\nfig.suptitle('Distribucion de Poisson')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nEjemplo:Número de impactos de insectos en la visera de un casco\n\n\nUn colega de trabajo, al que llamaremos JG, es muy aficionado a los grandes premios de velocidad tanto en coches como en motos.\nComo es tan aficionado está obsesionado con muchas de las más extravagantes estadísticas de estos deportes. En particular le propusimos que estudiara el número de insectos que chocan contra la visera de un casco de un motorista GP o de un conductor de fórmula 1 .\nLa idea es que el número de insectos está igualmente repartido por todo el circuito y de promedio impactan \\(\\lambda&gt;0\\) insectos por minuto. También es razonable suponer que:\n\npodemos dividir la superficie de la visera en cuadrados suficientemente pequeños de forma que la probabilidad de que caigan dos insectos en la misma zona es prácticamente 0.\nla probabilidad de que un insecto impacte en un cuadrado cualquiera de la visera es independiente de cualquier otro cuadrado.\nsi hemos dividido la visera en \\(n\\) cuadrados la probabilidad \\(p_n\\) de impacto de un cuadrado vale \\(p_n=\\frac{\\lambda}{n}\\).\n\nBajo estas condiciones, si denotamos por \\(X_t\\) como el número de insectos que ha impactado en la visera en el intervalo \\((0,t]\\) (en \\(t\\) minutos), podemos afirmar que \\(X_t\\) es un proceso de Poisson \\(Po(\\lambda\\cdot t)\\).\nSupongamos que nos dicen que \\(\\lambda=3\\) insectos por minuto. Entonces el proceso de poisson \\(X_t\\) seguirá un ley \\(Po(3\\cdot t).\\)\nAhora estamos en condiciones de preguntar al proceso de Poisson.\n¿Cuál es la probabilidad de que en 10 minutos impacten más de 25 insectos?\nEn este caso \\(t=10\\) \\(X_{10}\\)= número de insectos que impactan en 10 minutos, el intervalo \\([0,10)\\) que sigue una \\(P(3\\cdot 10=30)\\). Por lo tanto\n\\[P(X&gt;25)=1-P(X\\leq 25)\\]\nlo resolvemos con R\n\n1-ppois(25,lambda=30)\n\n[1] 0.7916426\n\n\nOtra pregunta interesante es que tengamos que esperar más de 2 minutos para observar el primer impacto\n\\[P(X_2=0)=\\frac{(3\\cdot 2)^0}{0!}\\cdot e^{-3\\cdot 2}= e^{-6}=0.002479.\\]\nCon R\n\n6^0/factorial(0)*exp(-6)\n\n[1] 0.002478752\n\nppois(0,lambda=3*2)\n\n[1] 0.002478752\n\n\n\n\n\n\n4.7 Distribución hipergeométrica\nSupongamos que disponemos de una urna de de sorteos que contiene \\(m\\) bolas blancas y \\(n\\) bolas rojas.\nEn total en esta urna hay \\(m+n\\) bolas, \\(m\\) blancas y \\(n\\) rojas. Si extraemos dos bolas de la urna lo podemos hacer de dos formas:\n\nExtraer una anotar su color y reponerla. Sacar otra y anotar su color. Hemos extraído la bola con reposición.\nExtraer simultáneamente dos bolas (sin reposición) y contar el número de bolas blancas.\n\nSea \\(X\\) es la v.a. que cuenta el número de bolas blancas extraídas.\n\nEn el primer caso, \\(X\\) es una \\(B(n=2,p=\\frac{m}{m+n})\\) ya que consiste en repetir dos veces el mismo experimento de Bernoulli.\nEn el segundo caso, \\(X\\) sigue una distribución hipergeométrica que estudiaremos en esta sección.\n\n\n\nDistribución hipergeométrica\n\n\nSean \\(n\\), \\(m\\) y \\(k\\) tres número enteros positivos y tales que \\(k&lt;m+n\\).\nConsideremos una urna que contiene \\(m+n\\) bolas de las que \\(m\\) son blancas y las restantes \\(n\\) no (son no blancas).\nEl número total de bolas es \\(m+n\\). Extraemos de forma aleatoria \\(k\\) bolas de la urna sin reemplazarlas.\nSea \\(X\\) la v.a. que cuenta el número de bolas blancas extraídas. Diremos que la distribución de \\(X\\) es hipergeométrica de parámetros \\(m\\), \\(n\\) y \\(k\\) y la denotaremos por \\(H(m,n,k)\\).\nSu dominio es\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\nFijemos ideas de cómo varía el dominio según los paraámetros \\(m\\), \\(n\\) y \\(k\\).\nPara explicarlo, veamos varios ejemplos:\n\n\\(H(m=5,n=2,k=3)\\). Tenemos \\(m=5\\) bolas blancas, \\(n=2\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas extraídas es \\(1=k-n=3-2\\), ya que sólo hay dos no blancas.\nEn cambio, el máximo si es \\(k=3\\), ya que tenemos bolas blancas de “sobra”.\n\n\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\\(H(m=2,n=5,k=3)\\). Tenemos \\(m=2\\) bolas blancas, \\(n=5\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas es \\(0\\) ya que puedo sacar 3 no blancas.\nEn cambio, el máximo si es \\(m=2\\), ya que aunque saquemos \\(k=3\\) bolas, al llegar a 2 ya hemos extraído todas las bolas blancas de la urna.\n\n\\(H(m=10,n=10,k=3)\\). Tenemos \\(m=10\\) bolas blancas, \\(n=10\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso podemos obtener desde \\(0\\) blancas hasta \\(k=3\\) blancas.\n\n\nSu función de probabilidad es: \\[\nP_{X}(x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}}, & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\}, \\mbox { para  } x\\in \\mathbf{N},\\\\\n0,  & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nObservación: otras parametrizaciones\nEn ocasiones se parametriza una v.a. hipergeométrica mediante \\(N=m+n\\), número total de bolas, \\(k\\), número de extracciones y \\(p\\), probabilidad de extraer una bola blanca.\nAsí podemos parametrizar alternativamente la distribución hipergeométrica así\n\\[H(N,k,p)\\mbox{ donde } p=\\frac{m}{N}.\\]\n\n4.7.1 Resumen distribución Hipergeométrica \\(H(m,n,k)\\).\n\n\nEjemplo clásico urna \\(m=15\\) blancas, \\(n=10\\) rojas y \\(k=3\\) extracciones sin reposición.\n\n\nTenemos una urna con 15 bolas blancas y 10 bolas rojas. Extraemos al azar tres bolas de la urna sin reposición. Sea \\(X\\) el número de bolas blancas extraídas. Bajo esta condiciones, la v.a. \\(X\\) sigue una ley de distribución \\(H(m=15,n=10,k=3)\\).\nSu función de probabilidad es\n\\[\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}} & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\} \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.,\n\\]\n\\[\\mbox{sustituyendo }\\scriptsize{\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{15}{x}\\cdot \\binom{10}{3-x}}{\\binom{25}{3}} & \\mbox{ si }\n0\\leq x \\leq 3 \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.\n}\\]\nLa probabilidad de sacar 2 blancas será\n\\[\nP(X=2)=\\frac{\\binom{15}{2}\\cdot \\binom{10}{3-2}}{\\binom{25}{3}}\n\\]\n\nc(choose(15,2), choose(10,1), choose(25,3))\n\n[1]  105   10 2300\n\n\n\\(P(X=2)=\\frac{105\\cdot10 }{2300}=0.4565217.\\)\nLa probabilidad de que saquemos más de 1 bola blanca es\n\\[\n\\begin{array}{rl}\nP(X&gt; 1)&= 1-P(X\\leq 1)=1-(P(X=0)+P(X=1))\\\\\n&=\n1-\\left(\\frac{\\binom{15}{0}\\cdot \\binom{10}{3}}{\\binom{25}{3}}+\n\\frac{\\binom{15}{1}\\cdot \\binom{10}{2}}{\\binom{25}{3}}\\right)\\\\\n&=\n1-\\left(\n\\frac{1\\cdot120 }{2300}+\\frac{15\\cdot45 }{2300}\n\\right)=1-\\frac{120+15\\cdot 45}{2300}=0.6543478.\n\\end{array}\n\\]\nEl número esperado de bolas blancas extraídas para una v.a. \\(X\\) \\(H(m=15,n=10,k=3)\\) es\n\\[E(X)=\\frac{k\\cdot m}{m+n}=\\frac{3\\cdot 15}{15+10}=\\frac{45}{35}=1.285714.\\]\nLa varianza vale: \\[\n\\begin{array}{rl}\nVar(X)&=k\\cdot\\frac{m}{m+n}\\cdot\\left(1-\\frac{m}{m+n}\\right) \\cdot\\frac{m+n-k}{m+n-1}\\\\\n&=3\\cdot\\frac{15}{15+10}\\cdot\\left(1-\\frac{15}{15+10}\\right) \\cdot\\frac{15+10-3}{15+10-1}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\left(1-\\frac{15}{25}\\right) \\cdot\\frac{22}{24}=\n3\\cdot\\frac{15}{25}\\cdot\\frac{25-15}{25} \\cdot\\frac{22}{24}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\frac{10}{25}\\cdot\\frac{22}{24}=0.66.\n\\end{array}\n\\]\nY por lo tanto su desviación típica es \\(+\\sqrt{Var(X)}=+\\sqrt{0.66}=0.812404.\\)\n\n\n\n\n4.7.2 Cálculos y gráficos distribución hipergeométrica con R\nSea \\(X\\) una v.a. \\(H(m,n,k)\\). La función de R para calcular la función de probabilidad en un valor \\(x\\), \\(P(X=x)\\), es dhyper(x,m,n,k) y para calcular la función de distribución en un valor \\(q\\), \\(P(X\\leq q)\\), es phyper(q,m,n,k). Para generar una muestra de valores que siga la distribución \\(H(m,n,k)\\), hay que usar la función rhyper(nn,m,n,k) donde nn es el número de observaciones aleatorias deseado de la muestra.\nPor ejemplo, si \\(X\\) es una \\(H(m=15,n=10,k=3)\\), los valores de \\(P(X=2)\\) y que \\(P(X&gt;1)=1-P(X\\leq 1)\\) son:\n\ndhyper(x=2,m=15,10,k=3)\n\n[1] 0.4565217\n\nphyper(q=1,m=15,n=10,k=3)# sí, le han puesto q ya veremos el porqué\n\n[1] 0.3456522\n\n1-phyper(q=1,m=15,n=10,k=3)\n\n[1] 0.6543478\n\n\nUna muestra aleatoria de este experimento de tamaño 200 sería:\n\nrhyper(nn=200,m=15,n=10,k=3)\n\n  [1] 2 3 1 3 1 2 2 3 2 2 1 2 1 2 2 3 3 1 1 1 1 0 2 3 2 1 3 2 2 2 2 3 2 3 3\n [36] 2 0 1 2 1 3 2 2 3 2 3 2 2 3 2 3 1 2 2 2 2 3 2 2 1 3 2 2 3 1 2 2 2 2 2\n [71] 3 0 2 0 3 2 2 2 1 2 2 3 1 1 1 2 2 2 2 1 1 3 2 2 3 2 2 1 1 1 3 3 2 2 2\n[106] 1 3 2 2 2 1 1 2 3 2 2 1 2 2 2 2 2 2 3 1 2 3 3 1 1 2 2 1 1 3 2 1 1 2 2\n[141] 3 1 1 1 2 1 1 3 1 2 2 3 3 2 3 1 2 1 2 2 2 1 2 3 1 3 3 3 2 2 1 3 3 1 1\n[176] 2 2 2 2 2 3 2 1 2 1 1 1 1 2 1 1 2 2 2 2 3 3 1 0 2\n\n\n\n\n\n\n\n\n\n\n\nSea \\(X\\) una \\(H(m,n,k)\\), las funciones de scipy.stats cambian los parámetros\n\n\\(M\\) es el número total de bolas. Con nuestra parametrización \\(M=m+n\\).\n\\(n\\) es el número de bolas blancas. Con nuestra parametrización \\(n=m\\).\n\\(N\\) es el número de extracciones. Con nuestra parametrización \\(N=k\\).\n\n\nfrom scipy.stats import hypergeom\n\n\n\n4.7.3 Cálculos y gráficos distribución hipergeométrica con python\n\nhypergeom.pmf(1,M=15+10,n=15,N=3)\n\n0.2934782608695652\n\nhypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.3456521739130434\n\n1-hypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.6543478260869566\n\n\nUna muestra aleatoria de este experimento sería…\n\nhypergeom.rvs(M=15+10,n=15,N=3,size=100)\n\narray([1, 2, 1, 3, 1, 3, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 3, 0, 2,\n       1, 1, 3, 2, 1, 2, 3, 3, 0, 2, 3, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 1,\n       2, 1, 2, 2, 1, 3, 1, 2, 1, 2, 0, 3, 1, 2, 3, 2, 2, 2, 2, 3, 2, 1,\n       3, 2, 2, 3, 2, 1, 1, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2,\n       2, 3, 1, 2, 0, 0, 2, 1, 2, 2, 2, 1], dtype=int64)\n\n\n\n\nfrom scipy.stats import hypergeom\n[M, n, N] = [20, 7, 12] ##20 elementos, 7 del tipo, extraemos 12\nx = np.arange(max(0, N-M+n),min(n, N))\nfig =plt.figure(figsize=(5, 2.7))\n =ax = fig.add_subplot(1,2,1)\n =ax.plot(x, hypergeom.pmf(x, M, n, N), 'bo', ms=5, label='hypergeom pmf')\n =ax.vlines(x, 0, hypergeom.pmf(x, M, n, N), colors='b', lw=2, alpha=0.5)\n =ax.set_ylim([0, max(hypergeom.pmf(x, M, n, N))*1.1])\n\n\n\n\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  =tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\n =ax.plot(x, hypergeom.cdf(x, M, n, N), 'bo', ms=5, label='hypergeom cdf')\n =ax.vlines(x, 0, hypergeom.cdf(x, M, n, N), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\n =fig.suptitle('Distribucion Hipergeometrica')\n =plt.show()",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuciones Notables I</span>"
    ]
  },
  {
    "objectID": "p01_04_1_Notables_discretas.html#distribución-hipergeométrica",
    "href": "p01_04_1_Notables_discretas.html#distribución-hipergeométrica",
    "title": "4  Distribuciones Notables I",
    "section": "4.7 Distribución hipergeométrica",
    "text": "4.7 Distribución hipergeométrica\nSupongamos que disponemos de una urna de de sorteos que contiene \\(m\\) bolas blancas y \\(n\\) bolas rojas.\nEn total en esta urna hay \\(m+n\\) bolas, \\(m\\) blancas y \\(n\\) rojas. Si extraemos dos bolas de la urna lo podemos hacer de dos formas:\n\nExtraer una anotar su color y reponerla. Sacar otra y anotar su color. Hemos extraído la bola con reposición.\nExtraer simultáneamente dos bolas (sin reposición) y contar el número de bolas blancas.\n\nSea \\(X\\) es la v.a. que cuenta el número de bolas blancas extraídas.\n\nEn el primer caso, \\(X\\) es una \\(B(n=2,p=\\frac{m}{m+n})\\) ya que consiste en repetir dos veces el mismo experimento de Bernoulli.\nEn el segundo caso, \\(X\\) sigue una distribución hipergeométrica que estudiaremos en esta sección.\n\n\n\nDistribución hipergeométrica\n\n\nSean \\(n\\), \\(m\\) y \\(k\\) tres número enteros positivos y tales que \\(k&lt;m+n\\).\nConsideremos una urna que contiene \\(m+n\\) bolas de las que \\(m\\) son blancas y las restantes \\(n\\) no (son no blancas).\nEl número total de bolas es \\(m+n\\). Extraemos de forma aleatoria \\(k\\) bolas de la urna sin reemplazarlas.\nSea \\(X\\) la v.a. que cuenta el número de bolas blancas extraídas. Diremos que la distribución de \\(X\\) es hipergeométrica de parámetros \\(m\\), \\(n\\) y \\(k\\) y la denotaremos por \\(H(m,n,k)\\).\nSu dominio es\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\nFijemos ideas de cómo varía el dominio según los paraámetros \\(m\\), \\(n\\) y \\(k\\).\nPara explicarlo, veamos varios ejemplos:\n\n\\(H(m=5,n=2,k=3)\\). Tenemos \\(m=5\\) bolas blancas, \\(n=2\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas extraídas es \\(1=k-n=3-2\\), ya que sólo hay dos no blancas.\nEn cambio, el máximo si es \\(k=3\\), ya que tenemos bolas blancas de “sobra”.\n\n\n\\[D_X=\\left\\{x\\in\\mathbf{N}\\mid \\max\\{0,k-n\\}\\leq  x \\leq \\min\\{m,k\\}\\right\\}\\]\n\n\\(H(m=2,n=5,k=3)\\). Tenemos \\(m=2\\) bolas blancas, \\(n=5\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso el mínimo de bolas blancas es \\(0\\) ya que puedo sacar 3 no blancas.\nEn cambio, el máximo si es \\(m=2\\), ya que aunque saquemos \\(k=3\\) bolas, al llegar a 2 ya hemos extraído todas las bolas blancas de la urna.\n\n\\(H(m=10,n=10,k=3)\\). Tenemos \\(m=10\\) bolas blancas, \\(n=10\\) no blancas y sacamos \\(k=3\\) bolas sin reposición.\n\nEn este caso podemos obtener desde \\(0\\) blancas hasta \\(k=3\\) blancas.\n\n\nSu función de probabilidad es: \\[\nP_{X}(x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}}, & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\}, \\mbox { para  } x\\in \\mathbf{N},\\\\\n0,  & \\mbox{en otro caso.}\\end{array}\\right.\n\\]\nObservación: otras parametrizaciones\nEn ocasiones se parametriza una v.a. hipergeométrica mediante \\(N=m+n\\), número total de bolas, \\(k\\), número de extracciones y \\(p\\), probabilidad de extraer una bola blanca.\nAsí podemos parametrizar alternativamente la distribución hipergeométrica así\n\\[H(N,k,p)\\mbox{ donde } p=\\frac{m}{N}.\\]\n\n4.7.1 Resumen distribución Hipergeométrica \\(H(m,n,k)\\).\n\n\nEjemplo clásico urna \\(m=15\\) blancas, \\(n=10\\) rojas y \\(k=3\\) extracciones sin reposición.\n\n\nTenemos una urna con 15 bolas blancas y 10 bolas rojas. Extraemos al azar tres bolas de la urna sin reposición. Sea \\(X\\) el número de bolas blancas extraídas. Bajo esta condiciones, la v.a. \\(X\\) sigue una ley de distribución \\(H(m=15,n=10,k=3)\\).\nSu función de probabilidad es\n\\[\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{m}{x}\\cdot \\binom{n}{k-x}}{\\binom{m+n}{k}} & \\mbox{ si }\n\\max\\{0,k-n\\}\\leq x \\leq \\min\\{m,k\\} \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.,\n\\]\n\\[\\mbox{sustituyendo }\\scriptsize{\nP_X(x)=P(X=x)=\\left\\{\n\\begin{array}{ll}\n\\frac{\\binom{15}{x}\\cdot \\binom{10}{3-x}}{\\binom{25}{3}} & \\mbox{ si }\n0\\leq x \\leq 3 \\mbox { para  } x\\in \\mathbf{N}\\\\\n0  & \\mbox{en otro caso}\\end{array}\\right.\n}\\]\nLa probabilidad de sacar 2 blancas será\n\\[\nP(X=2)=\\frac{\\binom{15}{2}\\cdot \\binom{10}{3-2}}{\\binom{25}{3}}\n\\]\n\nc(choose(15,2), choose(10,1), choose(25,3))\n\n[1]  105   10 2300\n\n\n\\(P(X=2)=\\frac{105\\cdot10 }{2300}=0.4565217.\\)\nLa probabilidad de que saquemos más de 1 bola blanca es\n\\[\n\\begin{array}{rl}\nP(X&gt; 1)&= 1-P(X\\leq 1)=1-(P(X=0)+P(X=1))\\\\\n&=\n1-\\left(\\frac{\\binom{15}{0}\\cdot \\binom{10}{3}}{\\binom{25}{3}}+\n\\frac{\\binom{15}{1}\\cdot \\binom{10}{2}}{\\binom{25}{3}}\\right)\\\\\n&=\n1-\\left(\n\\frac{1\\cdot120 }{2300}+\\frac{15\\cdot45 }{2300}\n\\right)=1-\\frac{120+15\\cdot 45}{2300}=0.6543478.\n\\end{array}\n\\]\nEl número esperado de bolas blancas extraídas para una v.a. \\(X\\) \\(H(m=15,n=10,k=3)\\) es\n\\[E(X)=\\frac{k\\cdot m}{m+n}=\\frac{3\\cdot 15}{15+10}=\\frac{45}{35}=1.285714.\\]\nLa varianza vale: \\[\n\\begin{array}{rl}\nVar(X)&=k\\cdot\\frac{m}{m+n}\\cdot\\left(1-\\frac{m}{m+n}\\right) \\cdot\\frac{m+n-k}{m+n-1}\\\\\n&=3\\cdot\\frac{15}{15+10}\\cdot\\left(1-\\frac{15}{15+10}\\right) \\cdot\\frac{15+10-3}{15+10-1}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\left(1-\\frac{15}{25}\\right) \\cdot\\frac{22}{24}=\n3\\cdot\\frac{15}{25}\\cdot\\frac{25-15}{25} \\cdot\\frac{22}{24}\\\\\n&=\n3\\cdot\\frac{15}{25}\\cdot\\frac{10}{25}\\cdot\\frac{22}{24}=0.66.\n\\end{array}\n\\]\nY por lo tanto su desviación típica es \\(+\\sqrt{Var(X)}=+\\sqrt{0.66}=0.812404.\\)\n\n\n\n\n4.7.2 Cálculos y gráficos distribución hipergeométrica con R\nSea \\(X\\) una v.a. \\(H(m,n,k)\\). La función de R para calcular la función de probabilidad en un valor \\(x\\), \\(P(X=x)\\), es dhyper(x,m,n,k) y para calcular la función de distribución en un valor \\(q\\), \\(P(X\\leq q)\\), es phyper(q,m,n,k). Para generar una muestra de valores que siga la distribución \\(H(m,n,k)\\), hay que usar la función rhyper(nn,m,n,k) donde nn es el número de observaciones aleatorias deseado de la muestra.\nPor ejemplo, si \\(X\\) es una \\(H(m=15,n=10,k=3)\\), los valores de \\(P(X=2)\\) y que \\(P(X&gt;1)=1-P(X\\leq 1)\\) son:\n\ndhyper(x=2,m=15,10,k=3)\n\n[1] 0.4565217\n\nphyper(q=1,m=15,n=10,k=3)# sí, le han puesto q ya veremos el porqué\n\n[1] 0.3456522\n\n1-phyper(q=1,m=15,n=10,k=3)\n\n[1] 0.6543478\n\n\nUna muestra aleatoria de este experimento de tamaño 200 sería:\n\nrhyper(nn=200,m=15,n=10,k=3)\n\n  [1] 2 3 1 3 1 2 2 3 2 2 1 2 1 2 2 3 3 1 1 1 1 0 2 3 2 1 3 2 2 2 2 3 2 3 3\n [36] 2 0 1 2 1 3 2 2 3 2 3 2 2 3 2 3 1 2 2 2 2 3 2 2 1 3 2 2 3 1 2 2 2 2 2\n [71] 3 0 2 0 3 2 2 2 1 2 2 3 1 1 1 2 2 2 2 1 1 3 2 2 3 2 2 1 1 1 3 3 2 2 2\n[106] 1 3 2 2 2 1 1 2 3 2 2 1 2 2 2 2 2 2 3 1 2 3 3 1 1 2 2 1 1 3 2 1 1 2 2\n[141] 3 1 1 1 2 1 1 3 1 2 2 3 3 2 3 1 2 1 2 2 2 1 2 3 1 3 3 3 2 2 1 3 3 1 1\n[176] 2 2 2 2 2 3 2 1 2 1 1 1 1 2 1 1 2 2 2 2 3 3 1 0 2\n\n\n\n\n\n\n\n\n\n\n\nSea \\(X\\) una \\(H(m,n,k)\\), las funciones de scipy.stats cambian los parámetros\n\n\\(M\\) es el número total de bolas. Con nuestra parametrización \\(M=m+n\\).\n\\(n\\) es el número de bolas blancas. Con nuestra parametrización \\(n=m\\).\n\\(N\\) es el número de extracciones. Con nuestra parametrización \\(N=k\\).\n\n\nfrom scipy.stats import hypergeom\n\n\n\n4.7.3 Cálculos y gráficos distribución hipergeométrica con python\n\nhypergeom.pmf(1,M=15+10,n=15,N=3)\n\n0.2934782608695652\n\nhypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.3456521739130434\n\n1-hypergeom.cdf(1,M=15+10,n=15,N=3)\n\n0.6543478260869566\n\n\nUna muestra aleatoria de este experimento sería…\n\nhypergeom.rvs(M=15+10,n=15,N=3,size=100)\n\narray([1, 2, 1, 3, 1, 3, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 3, 1, 3, 0, 2,\n       1, 1, 3, 2, 1, 2, 3, 3, 0, 2, 3, 1, 2, 2, 2, 2, 3, 3, 1, 2, 2, 1,\n       2, 1, 2, 2, 1, 3, 1, 2, 1, 2, 0, 3, 1, 2, 3, 2, 2, 2, 2, 3, 2, 1,\n       3, 2, 2, 3, 2, 1, 1, 1, 3, 2, 3, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2,\n       2, 3, 1, 2, 0, 0, 2, 1, 2, 2, 2, 1], dtype=int64)\n\n\n\n\nfrom scipy.stats import hypergeom\n[M, n, N] = [20, 7, 12] ##20 elementos, 7 del tipo, extraemos 12\nx = np.arange(max(0, N-M+n),min(n, N))\nfig =plt.figure(figsize=(5, 2.7))\n =ax = fig.add_subplot(1,2,1)\n =ax.plot(x, hypergeom.pmf(x, M, n, N), 'bo', ms=5, label='hypergeom pmf')\n =ax.vlines(x, 0, hypergeom.pmf(x, M, n, N), colors='b', lw=2, alpha=0.5)\n =ax.set_ylim([0, max(hypergeom.pmf(x, M, n, N))*1.1])\n\n\n\n\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n  =tick.label.set_fontsize(5) \nax = fig.add_subplot(1,2,2)\n =ax.plot(x, hypergeom.cdf(x, M, n, N), 'bo', ms=5, label='hypergeom cdf')\n =ax.vlines(x, 0, hypergeom.cdf(x, M, n, N), colors='b', lw=2, alpha=0.5)\nfor tick in ax.xaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\nfor tick in ax.yaxis.get_major_ticks():\n   =tick.label.set_fontsize(5)\n =fig.suptitle('Distribucion Hipergeometrica')\n =plt.show()",
    "crumbs": [
      "Antes de empezar",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Distribuciones Notables I</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Antes de empezar",
      "References"
    ]
  }
]